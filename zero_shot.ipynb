{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e162b33b984bcd903882333705163d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b814d36d78be4c12bfd47a03de322f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_tex_file(tex_file_path):\n",
    "    try:\n",
    "        with open(tex_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            tex_content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(tex_file_path, 'r', encoding=\"latin1\") as file:\n",
    "            tex_content = file.read()\n",
    "\n",
    "    citation_sentences = re.findall(r'(.*?\\\\cite\\{.*?\\}.*?[\\.!?])', tex_content)\n",
    "    citation_sentences = [' '.join(sentence.split()) for sentence in citation_sentences]  # Remove excess whitespace\n",
    "    return citation_sentences\n",
    "\n",
    "\n",
    "def parse_bbl_file(bbl_file_path):\n",
    "    try:\n",
    "        with open(bbl_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            bbl_content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(bbl_file_path, 'r', encoding=\"latin1\") as file:\n",
    "            bbl_content = file.read()\n",
    "\n",
    "    bib_entries = re.findall(r'\\\\bibitem\\{(.*)\\}\\n(.*?)(?=(\\\\bibitem|$))', bbl_content, re.DOTALL)\n",
    "    bib_entries_dict = {key: ' '.join(text.split()) for key, _, text in bib_entries}  # Remove excess whitespace\n",
    "    return bib_entries_dict\n",
    "\n",
    "\n",
    "def load_data_from_files(directory):\n",
    "    tex_file_paths = glob(os.path.join(directory, '**/*.tex'), recursive=True)\n",
    "    bbl_file_paths = glob(os.path.join(directory, '**/*.bbl'), recursive=True)\n",
    "\n",
    "    corpus = []\n",
    "    for tex_file_path in tqdm(tex_file_paths, desc='Parsing tex files'):\n",
    "        corpus.extend(parse_tex_file(tex_file_path))\n",
    "\n",
    "    bib_entries = {}\n",
    "    for bbl_file_path in tqdm(bbl_file_paths, desc='Parsing bbl files'):\n",
    "        bib_entries.update(parse_bbl_file(bbl_file_path))\n",
    "\n",
    "    return corpus, bib_entries\n",
    "\n",
    "def create_corpus_citation_masked(corpus):\n",
    "    corpus_citation_masked = [re.sub(r'\\\\cite\\{.*?\\}', '<CITATION>', sentence) for sentence in tqdm(corpus)]\n",
    "    return corpus_citation_masked\n",
    "\n",
    "def create_padded_bib_entries(bib_entries):\n",
    "    padded_bib_entries = {key: value + ' <BIB>' for key, value in tqdm(bib_entries.items())}\n",
    "    return padded_bib_entries\n",
    "\n",
    "# Example usage\n",
    "directory_path = './sources/'\n",
    "# corpus, bib_entries = load_data_from_files(directory_path)\n",
    "\n",
    "corpus_citation_masked = create_corpus_citation_masked(corpus)\n",
    "padded_bib_entries = create_padded_bib_entries(bib_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In all experiments, we used $\\\\delta=0.01$ and a heuristic threshold $\\\\beta_{t,\\\\delta} = \\\\log(1/\\\\delta) + \\\\log(1+t)$ for all elimination rules and LLR stopping. This is slightly larger than the heuristic threshold proposed by <CITATION> and adopted in many recent works.',\n",
       " 'We report the results on unstructured bandit instances (generated according to the procedure of Appendix \\\\ref{app:instances}) in Table \\\\ref{tab:uns_all}. The algorithm k-Learner is the unstructured variant of LinGame proposed by <CITATION>.',\n",
       " 'We combined adaptive algorithms which are natively based on LLR stopping with our elimination stopping rules and, whenever possible, we extended their sampling rule to use elimination. The selected baselines are the following. For linear BAI, LinGapE \\\\citep{xu2018fully}, LinGame \\\\citep{degenne2020gamification}, Frank-Wolfe Sampling (FWS) \\\\citep{wang2021fast}, Lazy Track-and-Stop (TaS) \\\\citep{jedra2020optimal}, XY-Adaptive \\\\citep{soare2014best}, and RAGE \\\\citep{fiez2019sequential} (the latter two are natively elimination based). For linear Top-m, m-LinGapE \\\\citep{reda2021top}, MisLid \\\\citep{reda2021dealing}, FWS, Lazy TaS\\\\footnote{Lazy TaS, while analyzed only for BAI, can be applied to any problem since it is a variant of Track-and-Stop.}, and LinGIFA \\\\citep{reda2021top}. For linear OSI, LinGapE\\\\footnote{LinGapE was originally proposed only for BAI in <CITATION>, but its extension to OSI is trivial.',\n",
       " '%\\\\item XY-Adaptive <CITATION> (and/or XY-Static?',\n",
       " '%\\\\item ALBA <CITATION> (seems to outperform all the XYs, should we just try this one?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_citation_masked[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define the special tokens\n",
    "special_tokens_dict = {'additional_special_tokens': ['<CITATION>', '<BIB>']}\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize the token embeddings\n",
    "\n",
    "# Citation Dataset\n",
    "class CitationDataset(Dataset):\n",
    "    def __init__(self, corpus, bib_entries):\n",
    "        self.corpus = corpus\n",
    "        self.bib_entries = list(bib_entries.values())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx], self.bib_entries[idx]\n",
    "\n",
    "# Create datasets\n",
    "corpus_train, corpus_val, bib_train, bib_val = train_test_split(corpus_citation_masked, padded_bib_entries, test_size=0.2)\n",
    "train_dataset = CitationDataset(corpus_train, bib_train)\n",
    "val_dataset = CitationDataset(corpus_val, bib_val)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8  # adjust according to your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "cos = CosineSimilarity(dim=-1)\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        corpus_batch, bib_batch = batch\n",
    "        corpus_encoded = tokenize_and_encode(corpus_batch)\n",
    "        bib_encoded = tokenize_and_encode(bib_batch)\n",
    "\n",
    "        loss = calculate_loss(corpus_encoded, bib_encoded)\n",
    "\n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            corpus_batch, bib_batch = batch\n",
    "            corpus_encoded = tokenize_and_encode(corpus_batch)\n",
    "            bib_encoded = tokenize_and_encode(bib_batch)\n",
    "\n",
    "            loss = calculate_loss(corpus_encoded, bib_encoded)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Define the training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_model()\n",
    "    val_loss = evaluate_model()\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
