Adam_eps: 1.0e-08
batch_size: 4024
val_batch_size: 8000
micro_batch_size: 32
train_ratio: 0.5
checkpoint_every: 50
cite_token: '[[CITE]]'
ref_token: '[[REF]]'
cite_token_id: null
collate_sample_size: 40000
device: cuda
initial_logit_scale: 3
k_values:
- 1
- 5
- 10
- 50
- 100
- 1000
learning_rate: 0.00015
logits_learning_rate: 0
max_grad_norm: 1.0
max_length: 512
max_targets: 5
model_name: bert-base-uncased
num_epochs: 100
overlap: 0.5
project_name: citation-matching
ref_token_id: null
resume_from: null
root_dir: /mnt/HDD/amir/paperGPT
run_name: null
seed: 42
source_len: 512
target_len: 128
vocab_size: null
warmup_steps: 0
weight_decay: 0.01
