# Model settings
model_name: "bert-base-uncased"
initial_logit_scale: 3.0

# Random seed
seed: 42

# Special tokens
cite_token: "[[CITE]]"
ref_token: "[[REF]]"

# Text processing
max_length: 512
source_len: 512
target_len: 128
max_targets: 5
overlap: 0.5

# Training batch sizes
batch_size: 512
val_batch_size: 512
retrieval_batch_size: 1024
micro_batch_size: 32

# Training parameters
num_epochs: 100
learning_rate: 1.5e-4
logits_learning_rate: 0.0
max_grad_norm: 1.0
Adam_eps: 1.0e-8
weight_decay: 0.01
warmup_steps: 0
train_ratio: 0.9
collate_sample_size: null

# Evaluation settings
k_values: [1, 5, 10, 50, 100, 1000]

# Project structure
root_dir: "."
project_name: "citation-matching"

# Checkpointing
checkpoint_every: null
run_name: null
resume_from: null

# Hardware
device: null