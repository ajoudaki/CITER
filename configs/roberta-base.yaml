Adam_eps: 1.0e-08
batch_size: 2048
micro_batch_size: 32
checkpoint_every: 20
cite_token: '[[CITE]]'
cite_token_id: null
collate_sample_size: null
device: 'cuda:0'
initial_logit_scale: 3
k_values:
- 1
- 5
- 10
- 50
- 100
- 1000
learning_rate: 2.0e-05
logits_learning_rate: 0
max_grad_norm: 0.5
max_length: 512
max_targets: 5
model_name: FacebookAI/roberta-base
num_epochs: 100
overlap: 0.5
project_name: citation-matching
ref_token: '[[REF]]'
ref_token_id: null
resume_from: null
root_dir: /mnt/HDD/amir/paperGPT
run_name: null
seed: 42
source_len: 512
target_len: 512
train_ratio: 1.0
vocab_size: null
warmup_steps: 0
weight_decay: 0.01
