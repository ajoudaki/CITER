global_batch_size: 1024  # Main batch size after warmup
micro_batch_size: 8
stream_chunk_size: 256
tau: 0.07
lr: 1e-3
weight_decay: 0.01  # L2 regularization for AdamW
num_epochs: 10
max_length: 256
output_dim: 2048
drop_last: true # drop last batch if its sizee is < global_batch_size
use_amp: true  # Enable automatic mixed precision for memory efficiency
warmup_steps: 100
validation_interval: 0  # Run validation every N training steps (set to 0 to validate only at epoch end)

# LoRA configuration
lora:
  enabled: true
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
