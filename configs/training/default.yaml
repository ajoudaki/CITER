# Parameters controlling the training loop and execution

# Batching & Data
global_batch_size: 1024
micro_batch_size: 8
stream_chunk_size: 256
max_length: 256

# Training schedule
num_epochs: 10

# Model & Loss
tau: 0.07 # Temperature for contrastive loss

# Performance & Memory
use_amp: true               # Mixed precision
gradient_checkpointing: false # Set to true to trade compute for memory

# Validation
validation_interval: 0  # 0 means validate only at epoch end

# Scheduler Configuration
scheduler:
  _target_: theorem_contrastive_training.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
  # num_training_steps is passed programmatically
