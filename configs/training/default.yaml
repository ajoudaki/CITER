# Default training configuration
global_batch_size: [64, 128, 256, 512, 1024, 2048]  # Gradual increase, for optimal flop/loss trade off  
micro_batch_size: 6
stream_chunk_size: 256
tau: 0.07
lr: 2e-4
num_epochs: 20
max_length: 256
output_dim: 1024
drop_last: true

# LoRA configuration
lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05