Adam_eps: 1.0e-08
batch_size: 1024
micro_batch_size: 128
checkpoint_every: 1000
cite_token: '[[CITE]]'
ref_token: '[[REF]]'
cite_token_id: null
collate_sample_size: 20000
device: cuda
initial_logit_scale: 3
k_values:
- 1
- 5
- 10
- 50
- 100
- 1000
learning_rate: 0.00015
logits_learning_rate: 0
max_grad_norm: 0.5
max_length: 512
max_targets: 5
model_name: bert-base-uncased
num_epochs: 100
overlap: 0.5
project_name: citation-matching
ref_token_id: null
resume_from: null
root_dir: /mnt/HDD/amir/paperGPT
run_name: null
seed: 42
source_len: 512
target_len: 128
train_ratio: 0.5
vocab_size: null
warmup_steps: 0
weight_decay: 0.01
