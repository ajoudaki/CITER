# Addon to enable 4-bit quantization.
# Usage: python train.py model=qwen-1.5b model/addons=quantization optimizer=adamw_8bit

quantization_config:
  _target_: transformers.BitsAndBytesConfig
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  bnb_4bit_compute_dtype: "bfloat16"
