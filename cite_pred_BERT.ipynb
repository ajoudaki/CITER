{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c30ca1-6c0b-40eb-b5c2-2a662dc4b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1571cd8a39134335b980712a44b96dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ee73caaff043e69fed6d62b6ec428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents before preprocessing: 3236\n",
      "Number of documents after preprocessing: 474\n"
     ]
    }
   ],
   "source": [
    "import re, os\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "def preprocess_latex_corpus(corpus):\n",
    "    citation_id = 1\n",
    "    citation_map = {}  # To keep track of the citation to id mapping\n",
    "\n",
    "    processed_corpus = []\n",
    "\n",
    "    # Define the regex patterns\n",
    "    citation_pattern = re.compile(r\"\\\\cite[t,p]?{(.*?)}\")\n",
    "    command_pattern = re.compile(r\"\\\\[a-zA-Z]+\")\n",
    "    comment_pattern = re.compile(r\"%.*?$\", re.MULTILINE)\n",
    "    environment_pattern = re.compile(r\"\\\\begin{(figure|table|equation).*?\\\\end{\\1}\", re.DOTALL)\n",
    "\n",
    "    for document in tqdm(corpus):\n",
    "        # Remove comments\n",
    "        document = re.sub(comment_pattern, \"\", document)\n",
    "\n",
    "        # Remove non-informative environments\n",
    "        # document = re.sub(environment_pattern, \"\", document)\n",
    "\n",
    "        # Replace citation commands\n",
    "        matches = citation_pattern.findall(document)\n",
    "        for match in matches:\n",
    "            citations = match.split(\",\")  # Handle multiple citations within one command\n",
    "            for citation in citations:\n",
    "                citation = citation.strip()\n",
    "                \n",
    "                # Assign a unique id to each citation if not already done & if not empty\n",
    "                if citation not in citation_map and citation != \"\":\n",
    "                    citation_map[citation] = f\"<CITATION_{citation_id}>\"\n",
    "                    citation_id += 1\n",
    "                \n",
    "                    # Replace the citation with the special token\n",
    "                    document = document.replace(citation, citation_map[citation])\n",
    "\n",
    "        # Remove other commands\n",
    "        # document = re.sub(command_pattern, \"\", document)\n",
    "        document = document.replace('{<','<').replace('>}','>')\n",
    "        if r'\\begin{document}' not in document:\n",
    "            continue\n",
    "        document = document.split(r'\\begin{document}')[1] # get the body\n",
    "        processed_corpus.append(document)\n",
    "\n",
    "    return processed_corpus, citation_map\n",
    "\n",
    "\n",
    "def read_tex_files(directory):\n",
    "    corpus = []\n",
    "    for root, dirs, files in tqdm(os.walk(directory)):\n",
    "        try:\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.tex'):\n",
    "                    # print(file_name)\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        corpus.append(content)\n",
    "        except:\n",
    "            pass\n",
    "    return corpus \n",
    "\n",
    "corpus = read_tex_files('./sources/')\n",
    "processed_corpus, citation_map = preprocess_latex_corpus(corpus)\n",
    "\n",
    "# pint number of corpus before & after preprocessing\n",
    "print(f'Number of documents before preprocessing: {len(corpus)}')\n",
    "print(f'Number of documents after preprocessing: {len(processed_corpus)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95c41a3-3b29-4deb-ae55-215f19fc1feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75f329d6da64249b95ead5d06a7487d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41012 > 512). Running this sequence through the model will result in indexing errors\n",
      "<ipython-input-17-94cbeeac6231>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inputs.append(torch.tensor(masked_sequence.clone().detach()))\n",
      "<ipython-input-17-94cbeeac6231>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.targets.append(torch.tensor(target_sequence))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b071ae5811417c81b61365438ae531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import random\n",
    "\n",
    "class MaskedCitationDataset(Dataset):\n",
    "    def __init__(self, text_list, tokenizer, seq_len, mask_prob=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in tqdm(text_list,total=len(text_list)):\n",
    "            self.process_text(text)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        tokenized_text = self.tokenizer.encode(text)\n",
    "        for i in range(0, len(tokenized_text)-self.seq_len, self.seq_len):\n",
    "            if i+self.seq_len+1 >= len(tokenized_text):\n",
    "                break\n",
    "            sequence = tokenized_text[i:i+self.seq_len]\n",
    "            masked_sequence, target_sequence = self.mask_sequence(sequence)\n",
    "            if len(target_sequence[target_sequence!=-100]) == 0:\n",
    "                continue    \n",
    "            self.inputs.append(torch.tensor(masked_sequence.clone().detach()))\n",
    "            self.targets.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def mask_sequence(self, sequence):\n",
    "        sequence = np.array(sequence)\n",
    "        target_sequence = np.full(sequence.shape, -100)  # -100 is the default ignore index for CrossEntropyLoss\n",
    "        citation_indices = np.where(sequence >= self.tokenizer.additional_special_tokens_ids[0])[0]\n",
    "        # num_to_mask = max(1, int(len(citation_indices) * self.mask_prob))  # At least mask 1 token\n",
    "        # num_to_mask = min(num_to_mask, len(citation_indices))  # Ensure we do not attempt to mask more citations than exist\n",
    "        # mask_indices = np.random.choice(citation_indices, num_to_mask, replace=False)\n",
    "        target_sequence[citation_indices] = sequence[citation_indices]\n",
    "        sequence[citation_indices] = self.tokenizer.mask_token_id\n",
    "        return torch.tensor(sequence.tolist()), torch.tensor(target_sequence.tolist())\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': list(citation_map.values())}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "\n",
    "# Split the processed_corpus into training and validation sets\n",
    "train_size = int(len(processed_corpus) * 0.8)\n",
    "random.shuffle(processed_corpus)\n",
    "train_texts = processed_corpus[:train_size]\n",
    "val_texts = processed_corpus[train_size:]\n",
    "\n",
    "# Create the datasets\n",
    "seq_len = 512\n",
    "\n",
    "train_dataset = MaskedCitationDataset(train_texts, tokenizer, seq_len=seq_len)\n",
    "val_dataset = MaskedCitationDataset(val_texts, tokenizer, seq_len=seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0440248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8908a1d-4eb1-444c-ba54-0eb88e908b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ea6878ac9c40c7b5959df6f02540d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1a3e030f1f4df58fe175b3125619ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.8685675048828125\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e367dc82954bfd8f41c5d9131e4b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac172d774e994a04906c593fc15f403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.067061805725098\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a798f10cd9544caae2182ddc7279cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f291be722e8498e8e3e9986fb95f72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.024843314034598\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edec8df487fb40a2bfaf0fd9e6525e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80cec457930d40118944b775f236c6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.1696972601754325\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75febacfedc04ed4b8ca346033551d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113ee9e5ba424daea32949460eea130e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.302638953072684\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c51bc4d49244c69cde8b70be8ffd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d7069403f94fb9b2604cb1d08528f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.784555151803153\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb4a1c39ca447398ed55ea3689063ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675cb328002a4673addd7f0ef33e060c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.456542854309082\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e39e087b86c4360b3a3c93ce365a70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcbfe0bea4b479783da2cb979919ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.655230772835868\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6cf18c8bcf4dcf97f915142c9213f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07089235a4d54b87a4d0d5ad6fbaf739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.717359913417271\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89e781190e0453d90ae35a0261f1bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c891b432e444f4eac7e2cba6dc8f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.982607830592564\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb0af080a8d43e0a584cd89ddf34ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faca59b03d54271b6efc9f0a9eb6d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.659653783525739\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fc3bad2a6f43bca41d394ed5264d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff865b2e3ea4096a9299533b647de0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.227774423871722\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d539b04e84bbc9d8ae81dde865ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f3912c909c4d5db272043f272a06dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.234947547912598\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19b40e1e93645d1aa2e9df089b27a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd1a4373fdc4fa49fe1612d3791b9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.103667308262416\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62a298791974390984f14b77344f70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5601e97ec4b94f5abedb7c725200f7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.546057444981166\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48849c438a14958b8dcd378b424a521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959f43c08a5b47f688f01ca18590bf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.291487334115165\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae2b7f34e09401fbef27fc495868f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3105ef8391648f3aec477ea1f6415eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.802298981802804\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e698dd03e008430aafeb59e07f7ebab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a67fa8c9b514aa88ab6a5255e509afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.350942328316825\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94849a528254f488f1242342e1a5e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d87923080c4001be991a6cca8dfeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.7918954195295065\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa21ef3de6f43ebb40305d189c16192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6ad8d59cdd48bf826c8045579ed725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.653470371791295\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00633b0074d04893835156c2fc6d05e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits,targets_flattened)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m running_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred_BERT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:476\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    473\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    474\u001b[0m     step_size \u001b[39m=\u001b[39m step_size \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2) \u001b[39m/\u001b[39m bias_correction1\n\u001b[0;32m--> 476\u001b[0m p\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[1;32m    478\u001b[0m \u001b[39m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast, AdamW\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',)\n",
    "\n",
    "# resise vocab size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    model.train()\n",
    "    bar = tqdm(enumerate(train_loader),total=len(train_loader),)\n",
    "    running_loss = []\n",
    "    for i, (inputs, targets) in bar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs,)\n",
    "        logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        targets_flattened = targets.view(-1)\n",
    "        loss = criterion(logits,targets_flattened)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        if (i+1) % 20 == 0:\n",
    "            # print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n",
    "            avg_loss = sum(running_loss) / len(running_loss)\n",
    "            bar.set_postfix(iteration=i, loss=f\"{avg_loss:.5f}\")\n",
    "            running_loss = []\n",
    "        if i > 100:\n",
    "            break\n",
    "            \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        total_citations = 0\n",
    "        bar = tqdm(val_loader, total=len(val_loader),)\n",
    "        running_loss = []\n",
    "        for i, (inputs, targets) in enumerate(bar):\n",
    "            if i > 100:\n",
    "                break\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            targets_flattened = targets.view(-1)\n",
    "\n",
    "            loss = criterion(logits, targets_flattened)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i+1) % 20 == 0:\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                bar.set_postfix(iteration=i, loss=f\"{avg_loss:.5f}\")\n",
    "                running_loss = []\n",
    "\n",
    "            # Compute the citation accuracy\n",
    "            citation_predictions = torch.argmax(logits, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "            correct_predictions = (citation_predictions == targets_flattened).sum().item()\n",
    "            total_citations += targets.numel()\n",
    "            total_acc += correct_predictions\n",
    "            # print('corr pred = ', correct_predictions, ' cites = ', citation_targets.numel(), ' total acc = ', total_acc, ' total cites = ', total_citations)\n",
    "\n",
    "        print(f\"Validation Loss: {total_loss / len(val_loader)}\")\n",
    "        print(f\"Citation Accuracy: {total_acc / total_citations if total_citations > 0 else 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d66b56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0', dtype=torch.int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_flattened[targets_flattened>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eae778d-46bf-41d8-abb7-417526d644c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CITATION_5825>\n",
      "Citation for, Probability: 0.029667695984244347\n",
      "<CITATION_2002>\n",
      "Citation generalization, Probability: 0.013682132586836815\n",
      "<CITATION_9241>\n",
      "Citation BERT, Probability: 0.013249002397060394\n",
      "<CITATION_2280>\n",
      "Citation transformer, Probability: 0.01189996674656868\n",
      "<CITATION_1563>\n",
      "Citation supervised, Probability: 0.011230929754674435\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the sample text\n",
    "sample_text = \"Batch normalization improves training efficiency of neural networks <CITATION_1>\"\n",
    "\n",
    "# # Preprocess the text by replacing the <cite_1> token with a <mask> token\n",
    "# mask_token = tokenizer.mask_token\n",
    "# masked_text = sample_text.replace(\"<cite_1>\", mask_token)\n",
    "\n",
    "# Tokenize the text and convert it to a tensor\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "# Send the tensor to the device\n",
    "inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the logits and compute the softmax to get probabilities\n",
    "logits = outputs.logits\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the position of the <mask> token\n",
    "mask_position = torch.where(inputs[\"input_ids\"] == tokenizer.encode('<CITATION_1>')[0])[1]\n",
    "\n",
    "# Get the probabilities of the tokens at the <mask> position\n",
    "mask_probs = probs[0, mask_position, :]\n",
    "\n",
    "# Get the top k probabilities and their indices\n",
    "top_k_probs, top_k_indices = torch.topk(mask_probs, k=5)  # Get the top 5 predictions\n",
    "\n",
    "# Print the top k citations and their probabilities\n",
    "for i in range(top_k_probs.shape[-1]):\n",
    "    try:\n",
    "        citation_id = top_k_indices[0, i].item()\n",
    "        prob = top_k_probs[0, i].item()\n",
    "        citation_key = tokenizer.decode(citation_id)\n",
    "        print(citation_key)\n",
    "        # citation_val = citation_map[citation_key]\n",
    "        citation_val = [k for k,v in citation_map.items() if v == citation_key][0]\n",
    "        print(f\"Citation {citation_val}, Probability: {prob}\")\n",
    "        # if citation_id in tokenizer.get_added_vocab():\n",
    "        #     citation_key = tokenizer.get_added_vocab()[citation_id]\n",
    "        #     print(f\"Citation: {citation_map[citation_key]}, Probability: {prob}\")\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d581996-3389-4d94-a766-544aa478b9c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'infinitewly wide neural networks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/paperGPT/cite_pred.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m citation_map[\u001b[39m'\u001b[39;49m\u001b[39minfinitewly wide neural networks\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'infinitewly wide neural networks'"
     ]
    }
   ],
   "source": [
    "citation_map['infi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bee06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e27925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
