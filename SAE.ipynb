{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59736dfb-20b3-4097-a6e1-305a09b49ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset wikitext/wikitext-103-raw-v1...\n",
      "Loading model meta-llama/Llama-3.2-1B...\n",
      "Computing and caching activations in memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3000/3000 [01:02<00:00, 48.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2400, Val size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 65.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 metrics:\n",
      "Train: {'recon_loss': '0.0027', 'l1_loss': '0.0256', 'kl_loss': '0.0121', 'dict_norm_loss': '0.0010', 'total_loss': '3.7662', 'sparsity_ratio': '0.4017', 'max_activation': '0.4348', 'dead_features': '0.0000', 'feature_usage_std': '0.1807'}\n",
      "Val: {'recon_loss': '0.0017', 'l1_loss': '0.0248', 'kl_loss': '0.0117', 'dict_norm_loss': '0.0026', 'total_loss': '3.6483', 'sparsity_ratio': '0.3789', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '1014.8158', 'max_activation': '0.4849', 'mean_activation': '0.0248', 'std_activation': '0.0443', 'feature_usage_std': '0.1629', 'feature_usage_entropy': '1654.7842', 'mean_correlation': '0.0654', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0254', 'kl_loss': '0.0114', 'dict_norm_loss': '0.0044', 'total_loss': '3.6814', 'sparsity_ratio': '0.3533', 'max_activation': '0.5857', 'dead_features': '0.0000', 'feature_usage_std': '0.1613'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0243', 'kl_loss': '0.0121', 'dict_norm_loss': '0.0060', 'total_loss': '3.6390', 'sparsity_ratio': '0.3112', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '495.2105', 'max_activation': '0.6668', 'mean_activation': '0.0243', 'std_activation': '0.0539', 'feature_usage_std': '0.1400', 'feature_usage_entropy': '1662.1155', 'mean_correlation': '0.0768', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0255', 'kl_loss': '0.0110', 'dict_norm_loss': '0.0074', 'total_loss': '3.6528', 'sparsity_ratio': '0.2931', 'max_activation': '0.7499', 'dead_features': '0.0000', 'feature_usage_std': '0.1341'}\n",
      "Val: {'recon_loss': '0.0011', 'l1_loss': '0.0255', 'kl_loss': '0.0106', 'dict_norm_loss': '0.0086', 'total_loss': '3.6135', 'sparsity_ratio': '0.2776', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '401.1842', 'max_activation': '0.8354', 'mean_activation': '0.0255', 'std_activation': '0.0655', 'feature_usage_std': '0.1319', 'feature_usage_entropy': '1630.9165', 'mean_correlation': '0.0843', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 metrics:\n",
      "Train: {'recon_loss': '0.0010', 'l1_loss': '0.0251', 'kl_loss': '0.0112', 'dict_norm_loss': '0.0097', 'total_loss': '3.6320', 'sparsity_ratio': '0.2538', 'max_activation': '0.9204', 'dead_features': '0.0000', 'feature_usage_std': '0.1318'}\n",
      "Val: {'recon_loss': '0.0011', 'l1_loss': '0.0268', 'kl_loss': '0.0094', 'dict_norm_loss': '0.0107', 'total_loss': '3.6137', 'sparsity_ratio': '0.2540', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '351.0526', 'max_activation': '1.0099', 'mean_activation': '0.0268', 'std_activation': '0.0769', 'feature_usage_std': '0.1297', 'feature_usage_entropy': '1588.9095', 'mean_correlation': '0.1120', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0110', 'dict_norm_loss': '0.0128', 'total_loss': '3.6170', 'sparsity_ratio': '0.2271', 'max_activation': '1.0791', 'dead_features': '0.0000', 'feature_usage_std': '0.1210'}\n",
      "Val: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0109', 'dict_norm_loss': '0.0141', 'total_loss': '3.6086', 'sparsity_ratio': '0.2163', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '181.5789', 'max_activation': '1.1666', 'mean_activation': '0.0252', 'std_activation': '0.0831', 'feature_usage_std': '0.1085', 'feature_usage_entropy': '1532.3412', 'mean_correlation': '0.1553', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0253', 'kl_loss': '0.0107', 'dict_norm_loss': '0.0161', 'total_loss': '3.6017', 'sparsity_ratio': '0.1981', 'max_activation': '1.2436', 'dead_features': '0.0000', 'feature_usage_std': '0.1014'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0237', 'kl_loss': '0.0123', 'dict_norm_loss': '0.0184', 'total_loss': '3.6065', 'sparsity_ratio': '0.1837', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '96.8684', 'max_activation': '1.3056', 'mean_activation': '0.0237', 'std_activation': '0.0862', 'feature_usage_std': '0.0910', 'feature_usage_entropy': '1455.1582', 'mean_correlation': '0.2358', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 metrics:\n",
      "Train: {'recon_loss': '0.0010', 'l1_loss': '0.0250', 'kl_loss': '0.0109', 'dict_norm_loss': '0.0225', 'total_loss': '3.5966', 'sparsity_ratio': '0.1812', 'max_activation': '1.3900', 'dead_features': '0.0000', 'feature_usage_std': '0.0859'}\n",
      "Val: {'recon_loss': '0.0010', 'l1_loss': '0.0257', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0262', 'total_loss': '3.5947', 'sparsity_ratio': '0.1837', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '73.3158', 'max_activation': '1.4584', 'mean_activation': '0.0257', 'std_activation': '0.0946', 'feature_usage_std': '0.0850', 'feature_usage_entropy': '1461.8641', 'mean_correlation': '0.2477', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 metrics:\n",
      "Train: {'recon_loss': '0.0010', 'l1_loss': '0.0253', 'kl_loss': '0.0107', 'dict_norm_loss': '0.0288', 'total_loss': '3.6007', 'sparsity_ratio': '0.1684', 'max_activation': '1.5360', 'dead_features': '0.0000', 'feature_usage_std': '0.0774'}\n",
      "Val: {'recon_loss': '0.0010', 'l1_loss': '0.0251', 'kl_loss': '0.0107', 'dict_norm_loss': '0.0297', 'total_loss': '3.5877', 'sparsity_ratio': '0.1666', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '40.7105', 'max_activation': '1.6190', 'mean_activation': '0.0251', 'std_activation': '0.0999', 'feature_usage_std': '0.0728', 'feature_usage_entropy': '1414.9563', 'mean_correlation': '0.2948', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|█████████████████████████████| 150/150 [00:02<00:00, 68.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0108', 'dict_norm_loss': '0.0338', 'total_loss': '3.5939', 'sparsity_ratio': '0.1547', 'max_activation': '1.7143', 'dead_features': '0.0000', 'feature_usage_std': '0.0712'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0252', 'kl_loss': '0.0106', 'dict_norm_loss': '0.0354', 'total_loss': '3.5853', 'sparsity_ratio': '0.1523', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '35.2632', 'max_activation': '1.7907', 'mean_activation': '0.0252', 'std_activation': '0.1076', 'feature_usage_std': '0.0686', 'feature_usage_entropy': '1360.5371', 'mean_correlation': '0.3003', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 metrics:\n",
      "Train: {'recon_loss': '0.0010', 'l1_loss': '0.0251', 'kl_loss': '0.0107', 'dict_norm_loss': '0.0340', 'total_loss': '3.5803', 'sparsity_ratio': '0.1421', 'max_activation': '1.8685', 'dead_features': '0.0000', 'feature_usage_std': '0.0687'}\n",
      "Val: {'recon_loss': '0.0011', 'l1_loss': '0.0261', 'kl_loss': '0.0098', 'dict_norm_loss': '0.0353', 'total_loss': '3.5906', 'sparsity_ratio': '0.1477', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '34.6579', 'max_activation': '1.9451', 'mean_activation': '0.0261', 'std_activation': '0.1152', 'feature_usage_std': '0.0718', 'feature_usage_entropy': '1332.9890', 'mean_correlation': '0.3463', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 metrics:\n",
      "Train: {'recon_loss': '0.0010', 'l1_loss': '0.0253', 'kl_loss': '0.0104', 'dict_norm_loss': '0.0376', 'total_loss': '3.5768', 'sparsity_ratio': '0.1350', 'max_activation': '2.0043', 'dead_features': '0.0000', 'feature_usage_std': '0.0656'}\n",
      "Val: {'recon_loss': '0.0010', 'l1_loss': '0.0252', 'kl_loss': '0.0106', 'dict_norm_loss': '0.0393', 'total_loss': '3.5815', 'sparsity_ratio': '0.1349', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '31.4211', 'max_activation': '2.0690', 'mean_activation': '0.0252', 'std_activation': '0.1193', 'feature_usage_std': '0.0655', 'feature_usage_entropy': '1280.5245', 'mean_correlation': '0.3448', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0104', 'dict_norm_loss': '0.0412', 'total_loss': '3.5602', 'sparsity_ratio': '0.1287', 'max_activation': '2.1346', 'dead_features': '0.0000', 'feature_usage_std': '0.0637'}\n",
      "Val: {'recon_loss': '0.0010', 'l1_loss': '0.0241', 'kl_loss': '0.0118', 'dict_norm_loss': '0.0414', 'total_loss': '3.5876', 'sparsity_ratio': '0.1245', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '31.1053', 'max_activation': '2.1877', 'mean_activation': '0.0241', 'std_activation': '0.1223', 'feature_usage_std': '0.0631', 'feature_usage_entropy': '1230.2170', 'mean_correlation': '0.3520', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0104', 'dict_norm_loss': '0.0411', 'total_loss': '3.5668', 'sparsity_ratio': '0.1221', 'max_activation': '2.2457', 'dead_features': '0.0000', 'feature_usage_std': '0.0622'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0257', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0438', 'total_loss': '3.5771', 'sparsity_ratio': '0.1241', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '29.0000', 'max_activation': '2.3080', 'mean_activation': '0.0257', 'std_activation': '0.1302', 'feature_usage_std': '0.0630', 'feature_usage_entropy': '1226.5381', 'mean_correlation': '0.3701', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0105', 'dict_norm_loss': '0.0447', 'total_loss': '3.5743', 'sparsity_ratio': '0.1150', 'max_activation': '2.3621', 'dead_features': '0.0000', 'feature_usage_std': '0.0595'}\n",
      "Val: {'recon_loss': '0.0011', 'l1_loss': '0.0250', 'kl_loss': '0.0107', 'dict_norm_loss': '0.0445', 'total_loss': '3.5711', 'sparsity_ratio': '0.1144', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '27.8158', 'max_activation': '2.4237', 'mean_activation': '0.0250', 'std_activation': '0.1352', 'feature_usage_std': '0.0583', 'feature_usage_entropy': '1179.8540', 'mean_correlation': '0.3910', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 metrics:\n",
      "Train: {'recon_loss': '0.0013', 'l1_loss': '0.0252', 'kl_loss': '0.0105', 'dict_norm_loss': '0.0456', 'total_loss': '3.5776', 'sparsity_ratio': '0.1074', 'max_activation': '2.4988', 'dead_features': '0.0000', 'feature_usage_std': '0.0557'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0247', 'kl_loss': '0.0111', 'dict_norm_loss': '0.0532', 'total_loss': '3.5748', 'sparsity_ratio': '0.1040', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '19.2105', 'max_activation': '2.5517', 'mean_activation': '0.0247', 'std_activation': '0.1426', 'feature_usage_std': '0.0513', 'feature_usage_entropy': '1124.9290', 'mean_correlation': '0.4264', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0105', 'dict_norm_loss': '0.0546', 'total_loss': '3.5804', 'sparsity_ratio': '0.1026', 'max_activation': '2.6203', 'dead_features': '0.0000', 'feature_usage_std': '0.0527'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0242', 'kl_loss': '0.0115', 'dict_norm_loss': '0.0593', 'total_loss': '3.5706', 'sparsity_ratio': '0.0986', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '19.0526', 'max_activation': '2.6736', 'mean_activation': '0.0242', 'std_activation': '0.1482', 'feature_usage_std': '0.0506', 'feature_usage_entropy': '1089.3126', 'mean_correlation': '0.4208', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 metrics:\n",
      "Train: {'recon_loss': '0.0011', 'l1_loss': '0.0252', 'kl_loss': '0.0104', 'dict_norm_loss': '0.0612', 'total_loss': '3.5533', 'sparsity_ratio': '0.0961', 'max_activation': '2.7241', 'dead_features': '0.0000', 'feature_usage_std': '0.0511'}\n",
      "Val: {'recon_loss': '0.0011', 'l1_loss': '0.0242', 'kl_loss': '0.0115', 'dict_norm_loss': '0.0631', 'total_loss': '3.5705', 'sparsity_ratio': '0.0910', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '17.8421', 'max_activation': '2.7543', 'mean_activation': '0.0242', 'std_activation': '0.1541', 'feature_usage_std': '0.0491', 'feature_usage_entropy': '1040.1262', 'mean_correlation': '0.4230', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 metrics:\n",
      "Train: {'recon_loss': '0.0012', 'l1_loss': '0.0251', 'kl_loss': '0.0103', 'dict_norm_loss': '0.0648', 'total_loss': '3.5494', 'sparsity_ratio': '0.0916', 'max_activation': '2.7880', 'dead_features': '0.0000', 'feature_usage_std': '0.0492'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0268', 'kl_loss': '0.0089', 'dict_norm_loss': '0.0679', 'total_loss': '3.5716', 'sparsity_ratio': '0.0989', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.9211', 'max_activation': '2.8285', 'mean_activation': '0.0268', 'std_activation': '0.1630', 'feature_usage_std': '0.0517', 'feature_usage_entropy': '1086.0735', 'mean_correlation': '0.4537', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 metrics:\n",
      "Train: {'recon_loss': '0.0012', 'l1_loss': '0.0254', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0702', 'total_loss': '3.5435', 'sparsity_ratio': '0.0887', 'max_activation': '2.8518', 'dead_features': '0.0000', 'feature_usage_std': '0.0484'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0251', 'kl_loss': '0.0105', 'dict_norm_loss': '0.0706', 'total_loss': '3.5601', 'sparsity_ratio': '0.0871', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '15.1053', 'max_activation': '2.8807', 'mean_activation': '0.0251', 'std_activation': '0.1652', 'feature_usage_std': '0.0478', 'feature_usage_entropy': '1010.9999', 'mean_correlation': '0.4409', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 metrics:\n",
      "Train: {'recon_loss': '0.0013', 'l1_loss': '0.0253', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0716', 'total_loss': '3.5533', 'sparsity_ratio': '0.0846', 'max_activation': '2.9213', 'dead_features': '0.0000', 'feature_usage_std': '0.0482'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0240', 'kl_loss': '0.0117', 'dict_norm_loss': '0.0704', 'total_loss': '3.5680', 'sparsity_ratio': '0.0807', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '15.3158', 'max_activation': '2.9446', 'mean_activation': '0.0240', 'std_activation': '0.1685', 'feature_usage_std': '0.0463', 'feature_usage_entropy': '966.7092', 'mean_correlation': '0.4361', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 metrics:\n",
      "Train: {'recon_loss': '0.0013', 'l1_loss': '0.0252', 'kl_loss': '0.0103', 'dict_norm_loss': '0.0718', 'total_loss': '3.5513', 'sparsity_ratio': '0.0813', 'max_activation': '2.9731', 'dead_features': '0.0000', 'feature_usage_std': '0.0476'}\n",
      "Val: {'recon_loss': '0.0026', 'l1_loss': '0.0255', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0698', 'total_loss': '3.5550', 'sparsity_ratio': '0.0821', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '15.0000', 'max_activation': '3.0048', 'mean_activation': '0.0255', 'std_activation': '0.1766', 'feature_usage_std': '0.0470', 'feature_usage_entropy': '973.8613', 'mean_correlation': '0.4626', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 metrics:\n",
      "Train: {'recon_loss': '0.0013', 'l1_loss': '0.0252', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0729', 'total_loss': '3.5422', 'sparsity_ratio': '0.0776', 'max_activation': '3.0200', 'dead_features': '0.0000', 'feature_usage_std': '0.0468'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0256', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0756', 'total_loss': '3.5513', 'sparsity_ratio': '0.0792', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.9211', 'max_activation': '3.0380', 'mean_activation': '0.0256', 'std_activation': '0.1816', 'feature_usage_std': '0.0473', 'feature_usage_entropy': '950.2690', 'mean_correlation': '0.4757', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 metrics:\n",
      "Train: {'recon_loss': '0.0012', 'l1_loss': '0.0251', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0763', 'total_loss': '3.5385', 'sparsity_ratio': '0.0752', 'max_activation': '3.0532', 'dead_features': '0.0000', 'feature_usage_std': '0.0467'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0260', 'kl_loss': '0.0095', 'dict_norm_loss': '0.0774', 'total_loss': '3.5519', 'sparsity_ratio': '0.0793', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.8421', 'max_activation': '3.0749', 'mean_activation': '0.0260', 'std_activation': '0.1864', 'feature_usage_std': '0.0496', 'feature_usage_entropy': '945.1851', 'mean_correlation': '0.4742', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0254', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0787', 'total_loss': '3.5483', 'sparsity_ratio': '0.0728', 'max_activation': '3.0833', 'dead_features': '0.0000', 'feature_usage_std': '0.0471'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0248', 'kl_loss': '0.0106', 'dict_norm_loss': '0.0770', 'total_loss': '3.5499', 'sparsity_ratio': '0.0699', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '15.0000', 'max_activation': '3.0886', 'mean_activation': '0.0248', 'std_activation': '0.1896', 'feature_usage_std': '0.0460', 'feature_usage_entropy': '876.5588', 'mean_correlation': '0.4617', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 metrics:\n",
      "Train: {'recon_loss': '0.0013', 'l1_loss': '0.0252', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0758', 'total_loss': '3.5377', 'sparsity_ratio': '0.0686', 'max_activation': '3.1058', 'dead_features': '0.0000', 'feature_usage_std': '0.0460'}\n",
      "Val: {'recon_loss': '0.0016', 'l1_loss': '0.0260', 'kl_loss': '0.0095', 'dict_norm_loss': '0.0800', 'total_loss': '3.5494', 'sparsity_ratio': '0.0707', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.9211', 'max_activation': '3.1259', 'mean_activation': '0.0260', 'std_activation': '0.1967', 'feature_usage_std': '0.0475', 'feature_usage_entropy': '878.4620', 'mean_correlation': '0.4968', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0802', 'total_loss': '3.5538', 'sparsity_ratio': '0.0649', 'max_activation': '3.1325', 'dead_features': '0.0000', 'feature_usage_std': '0.0460'}\n",
      "Val: {'recon_loss': '0.0017', 'l1_loss': '0.0253', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0810', 'total_loss': '3.5468', 'sparsity_ratio': '0.0636', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '15.1842', 'max_activation': '3.1510', 'mean_activation': '0.0253', 'std_activation': '0.2019', 'feature_usage_std': '0.0458', 'feature_usage_entropy': '821.6993', 'mean_correlation': '0.4857', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0252', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0789', 'total_loss': '3.5343', 'sparsity_ratio': '0.0612', 'max_activation': '3.1506', 'dead_features': '0.0000', 'feature_usage_std': '0.0448'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0260', 'kl_loss': '0.0095', 'dict_norm_loss': '0.0800', 'total_loss': '3.5457', 'sparsity_ratio': '0.0633', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.7632', 'max_activation': '3.1604', 'mean_activation': '0.0260', 'std_activation': '0.2070', 'feature_usage_std': '0.0463', 'feature_usage_entropy': '815.1338', 'mean_correlation': '0.5103', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0766', 'total_loss': '3.5320', 'sparsity_ratio': '0.0597', 'max_activation': '3.1618', 'dead_features': '0.0000', 'feature_usage_std': '0.0449'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0257', 'kl_loss': '0.0097', 'dict_norm_loss': '0.0702', 'total_loss': '3.5427', 'sparsity_ratio': '0.0614', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.5263', 'max_activation': '3.1630', 'mean_activation': '0.0257', 'std_activation': '0.2098', 'feature_usage_std': '0.0464', 'feature_usage_entropy': '795.2557', 'mean_correlation': '0.5446', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0769', 'total_loss': '3.5297', 'sparsity_ratio': '0.0583', 'max_activation': '3.1709', 'dead_features': '0.0000', 'feature_usage_std': '0.0456'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0249', 'kl_loss': '0.0105', 'dict_norm_loss': '0.0830', 'total_loss': '3.5428', 'sparsity_ratio': '0.0568', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '14.4474', 'max_activation': '3.1725', 'mean_activation': '0.0249', 'std_activation': '0.2121', 'feature_usage_std': '0.0447', 'feature_usage_entropy': '756.6307', 'mean_correlation': '0.5322', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0817', 'total_loss': '3.5294', 'sparsity_ratio': '0.0559', 'max_activation': '3.1795', 'dead_features': '0.0000', 'feature_usage_std': '0.0448'}\n",
      "Val: {'recon_loss': '0.0017', 'l1_loss': '0.0253', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0836', 'total_loss': '3.5410', 'sparsity_ratio': '0.0554', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '13.6579', 'max_activation': '3.1836', 'mean_activation': '0.0253', 'std_activation': '0.2161', 'feature_usage_std': '0.0442', 'feature_usage_entropy': '742.7288', 'mean_correlation': '0.5573', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0251', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0864', 'total_loss': '3.5330', 'sparsity_ratio': '0.0527', 'max_activation': '3.1861', 'dead_features': '0.0000', 'feature_usage_std': '0.0433'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0261', 'kl_loss': '0.0094', 'dict_norm_loss': '0.0863', 'total_loss': '3.5466', 'sparsity_ratio': '0.0552', 'dead_features': '0.0000', 'rarely_active': '0.0000', 'hyperactive': '13.1053', 'max_activation': '3.1897', 'mean_activation': '0.0261', 'std_activation': '0.2206', 'feature_usage_std': '0.0455', 'feature_usage_entropy': '734.9283', 'mean_correlation': '0.5949', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0254', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0888', 'total_loss': '3.5317', 'sparsity_ratio': '0.0511', 'max_activation': '3.1928', 'dead_features': '0.0000', 'feature_usage_std': '0.0430'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0889', 'total_loss': '3.5405', 'sparsity_ratio': '0.0509', 'dead_features': '0.0000', 'rarely_active': '0.1316', 'hyperactive': '13.3158', 'max_activation': '3.1952', 'mean_activation': '0.0253', 'std_activation': '0.2228', 'feature_usage_std': '0.0433', 'feature_usage_entropy': '698.2536', 'mean_correlation': '0.5868', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0896', 'total_loss': '3.5275', 'sparsity_ratio': '0.0492', 'max_activation': '3.1979', 'dead_features': '0.0000', 'feature_usage_std': '0.0428'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0252', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0894', 'total_loss': '3.5399', 'sparsity_ratio': '0.0485', 'dead_features': '0.0000', 'rarely_active': '0.2895', 'hyperactive': '12.9474', 'max_activation': '3.1967', 'mean_activation': '0.0252', 'std_activation': '0.2251', 'feature_usage_std': '0.0425', 'feature_usage_entropy': '675.6532', 'mean_correlation': '0.5964', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0253', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0914', 'total_loss': '3.5358', 'sparsity_ratio': '0.0470', 'max_activation': '3.2023', 'dead_features': '0.0000', 'feature_usage_std': '0.0423'}\n",
      "Val: {'recon_loss': '0.0016', 'l1_loss': '0.0248', 'kl_loss': '0.0107', 'dict_norm_loss': '0.0892', 'total_loss': '3.5439', 'sparsity_ratio': '0.0449', 'dead_features': '0.0000', 'rarely_active': '0.6579', 'hyperactive': '12.8158', 'max_activation': '3.2052', 'mean_activation': '0.0248', 'std_activation': '0.2275', 'feature_usage_std': '0.0408', 'feature_usage_entropy': '641.9308', 'mean_correlation': '0.5905', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0884', 'total_loss': '3.5332', 'sparsity_ratio': '0.0446', 'max_activation': '3.2065', 'dead_features': '0.0000', 'feature_usage_std': '0.0413'}\n",
      "Val: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0101', 'dict_norm_loss': '0.0883', 'total_loss': '3.5388', 'sparsity_ratio': '0.0440', 'dead_features': '0.0000', 'rarely_active': '1.6053', 'hyperactive': '12.4474', 'max_activation': '3.2062', 'mean_activation': '0.0253', 'std_activation': '0.2309', 'feature_usage_std': '0.0410', 'feature_usage_entropy': '630.8351', 'mean_correlation': '0.5933', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0254', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0880', 'total_loss': '3.5271', 'sparsity_ratio': '0.0430', 'max_activation': '3.2110', 'dead_features': '0.0000', 'feature_usage_std': '0.0410'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0254', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0915', 'total_loss': '3.5356', 'sparsity_ratio': '0.0426', 'dead_features': '0.0000', 'rarely_active': '4.4211', 'hyperactive': '12.4474', 'max_activation': '3.2053', 'mean_activation': '0.0254', 'std_activation': '0.2330', 'feature_usage_std': '0.0412', 'feature_usage_entropy': '614.5599', 'mean_correlation': '0.6231', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 metrics:\n",
      "Train: {'recon_loss': '0.0014', 'l1_loss': '0.0252', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0941', 'total_loss': '3.5268', 'sparsity_ratio': '0.0410', 'max_activation': '3.2145', 'dead_features': '0.0000', 'feature_usage_std': '0.0409'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0257', 'kl_loss': '0.0097', 'dict_norm_loss': '0.0953', 'total_loss': '3.5377', 'sparsity_ratio': '0.0413', 'dead_features': '0.0000', 'rarely_active': '6.8684', 'hyperactive': '12.5000', 'max_activation': '3.2167', 'mean_activation': '0.0257', 'std_activation': '0.2361', 'feature_usage_std': '0.0414', 'feature_usage_entropy': '599.9575', 'mean_correlation': '0.6263', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0928', 'total_loss': '3.5275', 'sparsity_ratio': '0.0396', 'max_activation': '3.2161', 'dead_features': '0.0000', 'feature_usage_std': '0.0405'}\n",
      "Val: {'recon_loss': '0.0021', 'l1_loss': '0.0256', 'kl_loss': '0.0097', 'dict_norm_loss': '0.0916', 'total_loss': '3.5348', 'sparsity_ratio': '0.0398', 'dead_features': '0.0000', 'rarely_active': '11.7368', 'hyperactive': '11.8421', 'max_activation': '3.2255', 'mean_activation': '0.0256', 'std_activation': '0.2384', 'feature_usage_std': '0.0406', 'feature_usage_entropy': '583.2651', 'mean_correlation': '0.6468', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 metrics:\n",
      "Train: {'recon_loss': '0.0016', 'l1_loss': '0.0253', 'kl_loss': '0.0100', 'dict_norm_loss': '0.0908', 'total_loss': '3.5312', 'sparsity_ratio': '0.0381', 'max_activation': '3.2188', 'dead_features': '0.0000', 'feature_usage_std': '0.0404'}\n",
      "Val: {'recon_loss': '0.0015', 'l1_loss': '0.0254', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0927', 'total_loss': '3.5304', 'sparsity_ratio': '0.0378', 'dead_features': '0.0000', 'rarely_active': '18.6316', 'hyperactive': '11.8684', 'max_activation': '3.2239', 'mean_activation': '0.0254', 'std_activation': '0.2401', 'feature_usage_std': '0.0403', 'feature_usage_entropy': '561.2244', 'mean_correlation': '0.6603', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 metrics:\n",
      "Train: {'recon_loss': '0.0016', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0916', 'total_loss': '3.5225', 'sparsity_ratio': '0.0368', 'max_activation': '3.2206', 'dead_features': '0.0000', 'feature_usage_std': '0.0401'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0250', 'kl_loss': '0.0103', 'dict_norm_loss': '0.0957', 'total_loss': '3.5288', 'sparsity_ratio': '0.0358', 'dead_features': '0.0000', 'rarely_active': '30.8158', 'hyperactive': '11.6316', 'max_activation': '3.2213', 'mean_activation': '0.0250', 'std_activation': '0.2413', 'feature_usage_std': '0.0391', 'feature_usage_entropy': '541.0594', 'mean_correlation': '0.6565', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0987', 'total_loss': '3.5221', 'sparsity_ratio': '0.0357', 'max_activation': '3.2231', 'dead_features': '0.0000', 'feature_usage_std': '0.0394'}\n",
      "Val: {'recon_loss': '0.0019', 'l1_loss': '0.0251', 'kl_loss': '0.0102', 'dict_norm_loss': '0.0987', 'total_loss': '3.5281', 'sparsity_ratio': '0.0346', 'dead_features': '0.0000', 'rarely_active': '43.7895', 'hyperactive': '11.0263', 'max_activation': '3.2230', 'mean_activation': '0.0251', 'std_activation': '0.2440', 'feature_usage_std': '0.0383', 'feature_usage_entropy': '527.8495', 'mean_correlation': '0.6582', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 metrics:\n",
      "Train: {'recon_loss': '0.0016', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0968', 'total_loss': '3.5240', 'sparsity_ratio': '0.0342', 'max_activation': '3.2247', 'dead_features': '0.0000', 'feature_usage_std': '0.0391'}\n",
      "Val: {'recon_loss': '0.0021', 'l1_loss': '0.0257', 'kl_loss': '0.0096', 'dict_norm_loss': '0.0952', 'total_loss': '3.5278', 'sparsity_ratio': '0.0342', 'dead_features': '0.0000', 'rarely_active': '60.1842', 'hyperactive': '11.3684', 'max_activation': '3.2273', 'mean_activation': '0.0257', 'std_activation': '0.2471', 'feature_usage_std': '0.0396', 'feature_usage_entropy': '519.4700', 'mean_correlation': '0.6766', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.0986', 'total_loss': '3.5198', 'sparsity_ratio': '0.0327', 'max_activation': '3.2262', 'dead_features': '0.0000', 'feature_usage_std': '0.0383'}\n",
      "Val: {'recon_loss': '0.0014', 'l1_loss': '0.0251', 'kl_loss': '0.0101', 'dict_norm_loss': '0.1002', 'total_loss': '3.5246', 'sparsity_ratio': '0.0320', 'dead_features': '0.0000', 'rarely_active': '84.4474', 'hyperactive': '10.9211', 'max_activation': '3.2219', 'mean_activation': '0.0251', 'std_activation': '0.2478', 'feature_usage_std': '0.0379', 'feature_usage_entropy': '496.9270', 'mean_correlation': '0.6742', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.1017', 'total_loss': '3.5235', 'sparsity_ratio': '0.0318', 'max_activation': '3.2279', 'dead_features': '0.0000', 'feature_usage_std': '0.0386'}\n",
      "Val: {'recon_loss': '0.0013', 'l1_loss': '0.0251', 'kl_loss': '0.0101', 'dict_norm_loss': '0.1028', 'total_loss': '3.5242', 'sparsity_ratio': '0.0312', 'dead_features': '0.0000', 'rarely_active': '105.5000', 'hyperactive': '10.9737', 'max_activation': '3.2312', 'mean_activation': '0.0251', 'std_activation': '0.2500', 'feature_usage_std': '0.0380', 'feature_usage_entropy': '486.3195', 'mean_correlation': '0.6832', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 69.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 metrics:\n",
      "Train: {'recon_loss': '0.0016', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.1022', 'total_loss': '3.5213', 'sparsity_ratio': '0.0306', 'max_activation': '3.2294', 'dead_features': '0.0000', 'feature_usage_std': '0.0380'}\n",
      "Val: {'recon_loss': '0.0015', 'l1_loss': '0.0256', 'kl_loss': '0.0096', 'dict_norm_loss': '0.1036', 'total_loss': '3.5245', 'sparsity_ratio': '0.0309', 'dead_features': '0.0000', 'rarely_active': '143.6316', 'hyperactive': '10.6579', 'max_activation': '3.2317', 'mean_activation': '0.0256', 'std_activation': '0.2528', 'feature_usage_std': '0.0383', 'feature_usage_entropy': '479.2269', 'mean_correlation': '0.6961', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 metrics:\n",
      "Train: {'recon_loss': '0.0015', 'l1_loss': '0.0253', 'kl_loss': '0.0098', 'dict_norm_loss': '0.1039', 'total_loss': '3.5153', 'sparsity_ratio': '0.0297', 'max_activation': '3.2303', 'dead_features': '0.0000', 'feature_usage_std': '0.0378'}\n",
      "Val: {'recon_loss': '0.0015', 'l1_loss': '0.0255', 'kl_loss': '0.0097', 'dict_norm_loss': '0.1012', 'total_loss': '3.5245', 'sparsity_ratio': '0.0298', 'dead_features': '0.0000', 'rarely_active': '183.1316', 'hyperactive': '10.9737', 'max_activation': '3.2300', 'mean_activation': '0.0255', 'std_activation': '0.2537', 'feature_usage_std': '0.0384', 'feature_usage_entropy': '466.7622', 'mean_correlation': '0.6864', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 metrics:\n",
      "Train: {'recon_loss': '0.0016', 'l1_loss': '0.0253', 'kl_loss': '0.0098', 'dict_norm_loss': '0.1030', 'total_loss': '3.5169', 'sparsity_ratio': '0.0289', 'max_activation': '3.2314', 'dead_features': '0.0000', 'feature_usage_std': '0.0378'}\n",
      "Val: {'recon_loss': '0.0015', 'l1_loss': '0.0257', 'kl_loss': '0.0096', 'dict_norm_loss': '0.1063', 'total_loss': '3.5243', 'sparsity_ratio': '0.0294', 'dead_features': '0.0000', 'rarely_active': '210.6316', 'hyperactive': '10.8158', 'max_activation': '3.2293', 'mean_activation': '0.0257', 'std_activation': '0.2552', 'feature_usage_std': '0.0383', 'feature_usage_entropy': '461.0586', 'mean_correlation': '0.6964', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 metrics:\n",
      "Train: {'recon_loss': '0.0012', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.1071', 'total_loss': '3.5186', 'sparsity_ratio': '0.0283', 'max_activation': '3.2308', 'dead_features': '0.0000', 'feature_usage_std': '0.0377'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0251', 'kl_loss': '0.0101', 'dict_norm_loss': '0.1078', 'total_loss': '3.5240', 'sparsity_ratio': '0.0277', 'dead_features': '0.0000', 'rarely_active': '233.5263', 'hyperactive': '10.8947', 'max_activation': '3.2315', 'mean_activation': '0.0251', 'std_activation': '0.2550', 'feature_usage_std': '0.0370', 'feature_usage_entropy': '444.8228', 'mean_correlation': '0.6871', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 metrics:\n",
      "Train: {'recon_loss': '0.0012', 'l1_loss': '0.0253', 'kl_loss': '0.0098', 'dict_norm_loss': '0.1092', 'total_loss': '3.5179', 'sparsity_ratio': '0.0277', 'max_activation': '3.2318', 'dead_features': '0.0000', 'feature_usage_std': '0.0375'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0252', 'kl_loss': '0.0100', 'dict_norm_loss': '0.1094', 'total_loss': '3.5227', 'sparsity_ratio': '0.0273', 'dead_features': '0.0000', 'rarely_active': '269.3947', 'hyperactive': '10.9211', 'max_activation': '3.2324', 'mean_activation': '0.0252', 'std_activation': '0.2559', 'feature_usage_std': '0.0370', 'feature_usage_entropy': '438.5318', 'mean_correlation': '0.6882', 'feature_importance_std': '0.0000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|████████████████████████████| 150/150 [00:02<00:00, 68.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 metrics:\n",
      "Train: {'recon_loss': '0.0012', 'l1_loss': '0.0253', 'kl_loss': '0.0099', 'dict_norm_loss': '0.1097', 'total_loss': '3.5208', 'sparsity_ratio': '0.0272', 'max_activation': '3.2320', 'dead_features': '0.0000', 'feature_usage_std': '0.0374'}\n",
      "Val: {'recon_loss': '0.0012', 'l1_loss': '0.0255', 'kl_loss': '0.0097', 'dict_norm_loss': '0.1095', 'total_loss': '3.5222', 'sparsity_ratio': '0.0276', 'dead_features': '0.0000', 'rarely_active': '308.9211', 'hyperactive': '10.8158', 'max_activation': '3.2313', 'mean_activation': '0.0255', 'std_activation': '0.2569', 'feature_usage_std': '0.0377', 'feature_usage_entropy': '439.4990', 'mean_correlation': '0.6909', 'feature_importance_std': '0.0000'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MemoryCachedTransformerDataset(Dataset):\n",
    "    \"\"\"Dataset for extracting and caching transformer layer activations in memory\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str = \"wikitext\",\n",
    "        dataset_config: str = \"wikitext-103-raw-v1\",\n",
    "        split: str = \"train\",\n",
    "        model_name: str = \"gpt2\",\n",
    "        layer_idx: int = 6,\n",
    "        max_length: int = 128,\n",
    "        max_samples: Optional[int] = None,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        print(f\"Loading dataset {dataset_name}/{dataset_config}...\")\n",
    "        self.dataset = load_dataset(dataset_name, dataset_config, split=split)\n",
    "        if max_samples:\n",
    "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
    "            \n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.layer_idx = layer_idx\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        \n",
    "        # Compute and store all activations in memory\n",
    "        print(\"Computing and caching activations in memory...\")\n",
    "        self.activations = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.dataset))):\n",
    "            tokens = self.tokenizer(\n",
    "                self.dataset[i]['text'],\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move tokens to device\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            \n",
    "            # Extract activations\n",
    "            with torch.no_grad():\n",
    "                activations = self._extract_activations(tokens)\n",
    "            \n",
    "            self.activations.append(activations)\n",
    "        \n",
    "        # Convert to single tensor for more efficient storage and indexing\n",
    "        self.activations = torch.stack(self.activations)\n",
    "        \n",
    "        # Clean up the model and tokenizer since we don't need them anymore\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def _extract_activations(self, tokens: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Extract activations from specified layer\"\"\"\n",
    "        activations = []\n",
    "        \n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                act = output[0]\n",
    "            else:\n",
    "                act = output\n",
    "                \n",
    "            if isinstance(act, torch.Tensor):\n",
    "                activations.append(act.detach())\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected activation type: {type(act)}\")\n",
    "        \n",
    "        # Get the appropriate layer\n",
    "        if hasattr(self.model, 'encoder'):\n",
    "            layer = self.model.encoder.layer[self.layer_idx]\n",
    "        elif hasattr(self.model, 'h'):\n",
    "            layer = self.model.h[self.layer_idx]\n",
    "        elif hasattr(self.model, 'layers'):\n",
    "            layer = self.model.layers[self.layer_idx]\n",
    "        elif hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
    "            layer = self.model.model.layers[self.layer_idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model architecture\")\n",
    "        \n",
    "        # For LLaMA models\n",
    "        if 'llama' in self.model.config.model_type.lower():\n",
    "            if hasattr(layer, 'mlp'):\n",
    "                layer = layer.mlp\n",
    "            elif hasattr(layer, 'feed_forward'):\n",
    "                layer = layer.feed_forward\n",
    "        \n",
    "        handle = layer.register_forward_hook(hook)\n",
    "        \n",
    "        try:\n",
    "            self.model(**tokens)\n",
    "            if not activations:\n",
    "                raise ValueError(\"No activations captured by hook\")\n",
    "            return activations[0].squeeze(0)  # [seq_len, hidden_size]\n",
    "        finally:\n",
    "            handle.remove()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.activations)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.activations[idx]\n",
    "        \n",
    "\n",
    "class SparseAutoencoder(torch.nn.Module):\n",
    "    \"\"\"Sparse autoencoder with enhanced sparsity mechanisms\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        sparsity_lambda: float = 0.1,  # Increased from 1e-3\n",
    "        dict_norm_lambda: float = 1e-3,\n",
    "        target_sparsity: float = 0.05,  # Target activation rate (5%)\n",
    "        activation_threshold: float = 1e-6,  # Increased threshold\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim, bias=False),\n",
    "            torch.nn.ReLU(),  # ReLU for positive sparse activations\n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        with torch.no_grad():\n",
    "            # Initialize encoder weights\n",
    "            encoder_layer = self.encoder[0]  # Get the linear layer\n",
    "            encoder_init = torch.randn(hidden_dim, input_dim)\n",
    "            encoder_init = torch.nn.functional.normalize(encoder_init, dim=1)\n",
    "            encoder_layer.weight.data = encoder_init\n",
    "            \n",
    "            # Initialize decoder weights\n",
    "            decoder_init = torch.randn(input_dim, hidden_dim)\n",
    "            decoder_init = torch.nn.functional.normalize(decoder_init, dim=0)\n",
    "            self.decoder.weight.data = decoder_init\n",
    "            \n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "        self.dict_norm_lambda = dict_norm_lambda\n",
    "        self.target_sparsity = target_sparsity\n",
    "        self.activation_threshold = activation_threshold\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        h = self.encoder(x)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, h\n",
    "        \n",
    "    def loss(self, x: torch.Tensor) -> tuple[torch.Tensor, dict]:\n",
    "        x_recon, h = self.forward(x)\n",
    "        \n",
    "        # Basic losses\n",
    "        recon_loss = torch.nn.functional.mse_loss(x_recon, x)\n",
    "        \n",
    "        # L1 sparsity\n",
    "        l1_loss = torch.mean(torch.abs(h))\n",
    "        \n",
    "        # KL divergence sparsity penalty\n",
    "        rho_hat = torch.mean(h, dim=0)  # Average activation of each hidden unit\n",
    "        kl_loss = torch.mean(self.kl_divergence_sparsity(rho_hat))\n",
    "        \n",
    "        # Dictionary element norm loss\n",
    "        dict_norm_loss = torch.mean((torch.norm(self.decoder.weight, dim=0) - 1)**2)\n",
    "        \n",
    "        # Combined loss with both L1 and KL sparsity penalties\n",
    "        total_loss = (\n",
    "            recon_loss + \n",
    "            self.sparsity_lambda * (l1_loss + kl_loss) +\n",
    "            self.dict_norm_lambda * dict_norm_loss\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        sparsity_ratio = torch.mean((torch.abs(h) > self.activation_threshold).float())\n",
    "        max_activation = torch.max(torch.abs(h))\n",
    "        feature_usage = torch.mean((torch.abs(h) > self.activation_threshold).float(), dim=0)\n",
    "        dead_features = torch.sum(feature_usage == 0).item()\n",
    "        \n",
    "        metrics = {\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'l1_loss': l1_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'dict_norm_loss': dict_norm_loss.item(),\n",
    "            'total_loss': total_loss.item(),\n",
    "            'sparsity_ratio': sparsity_ratio.item(),\n",
    "            'max_activation': max_activation.item(),\n",
    "            'dead_features': dead_features,\n",
    "            'feature_usage_std': torch.std(feature_usage).item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, metrics\n",
    "\n",
    "    def compute_metrics(self, x: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Compute detailed metrics for sparse autoencoder analysis\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing various metrics:\n",
    "            - Basic loss terms (reconstruction, L1, KL divergence)\n",
    "            - Sparsity measurements\n",
    "            - Feature activation statistics\n",
    "            - Neuron specialization metrics\n",
    "        \"\"\"\n",
    "        x_recon, h = self.forward(x)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. Basic Losses\n",
    "        recon_loss = torch.nn.functional.mse_loss(x_recon, x)\n",
    "        l1_loss = torch.mean(torch.abs(h))\n",
    "        \n",
    "        # 2. KL Divergence Loss for Sparsity\n",
    "        rho_hat = torch.mean(h, dim=0)  # Average activation of each hidden unit\n",
    "        kl_loss = torch.mean(self.kl_divergence_sparsity(rho_hat))\n",
    "        \n",
    "        # 3. Dictionary Element Norm Loss\n",
    "        dict_norm_loss = torch.mean((torch.norm(self.decoder.weight, dim=0) - 1)**2)\n",
    "        \n",
    "        # 4. Total Loss\n",
    "        total_loss = (\n",
    "            recon_loss + \n",
    "            self.sparsity_lambda * (l1_loss + kl_loss) +\n",
    "            self.dict_norm_lambda * dict_norm_loss\n",
    "        )\n",
    "        \n",
    "        # 5. Sparsity Metrics\n",
    "        # Count activations above threshold\n",
    "        active_neurons = (torch.abs(h) > self.activation_threshold).float()\n",
    "        sparsity_ratio = torch.mean(active_neurons)\n",
    "        \n",
    "        # Per-neuron activation frequencies\n",
    "        neuron_activity = torch.mean(active_neurons, dim=0)  # [hidden_dim]\n",
    "        dead_features = torch.sum(neuron_activity == 0).item()\n",
    "        rarely_active = torch.sum(neuron_activity < 0.01).item()  # <1% activation rate\n",
    "        hyperactive = torch.sum(neuron_activity > 0.5).item()  # >50% activation rate\n",
    "        \n",
    "        # 6. Activation Statistics\n",
    "        max_activation = torch.max(torch.abs(h))\n",
    "        mean_activation = torch.mean(torch.abs(h))\n",
    "        std_activation = torch.std(torch.abs(h))\n",
    "        \n",
    "        # 7. Feature Usage Distribution\n",
    "        feature_usage_std = torch.std(neuron_activity).item()\n",
    "        feature_usage_entropy = -torch.sum(\n",
    "            neuron_activity * torch.log(neuron_activity + 1e-10)\n",
    "        ).item()\n",
    "        \n",
    "        # 8. Neuron Correlation Analysis\n",
    "        # Compute pairwise correlations between most active neurons\n",
    "        top_k = min(100, h.size(1))  # Use top 100 neurons or all if less\n",
    "        _, top_indices = torch.topk(neuron_activity, top_k)\n",
    "        top_activations = h[:, top_indices]\n",
    "        correlations = torch.corrcoef(top_activations.T)\n",
    "        mean_correlation = torch.mean(torch.abs(correlations - torch.eye(top_k, device=correlations.device))).item()\n",
    "        \n",
    "        # 9. Reconstruction Quality per Feature\n",
    "        # Compute how much each feature contributes to reconstruction\n",
    "        with torch.no_grad():\n",
    "            feature_importance = []\n",
    "            for i in range(min(100, h.size(1))):  # Sample 100 features for efficiency\n",
    "                h_zeroed = h.clone()\n",
    "                h_zeroed[:, i] = 0\n",
    "                x_recon_zeroed = self.decoder(h_zeroed)\n",
    "                feature_importance.append(\n",
    "                    torch.nn.functional.mse_loss(x_recon_zeroed, x).item()\n",
    "                )\n",
    "            feature_importance = torch.tensor(feature_importance)\n",
    "            feature_importance_std = torch.std(feature_importance).item()\n",
    "        \n",
    "        metrics = {\n",
    "            # Loss components\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'l1_loss': l1_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'dict_norm_loss': dict_norm_loss.item(),\n",
    "            'total_loss': total_loss.item(),\n",
    "            \n",
    "            # Sparsity metrics\n",
    "            'sparsity_ratio': sparsity_ratio.item(),\n",
    "            'dead_features': dead_features,\n",
    "            'rarely_active': rarely_active,\n",
    "            'hyperactive': hyperactive,\n",
    "            \n",
    "            # Activation statistics\n",
    "            'max_activation': max_activation.item(),\n",
    "            'mean_activation': mean_activation.item(),\n",
    "            'std_activation': std_activation.item(),\n",
    "            \n",
    "            # Feature usage metrics\n",
    "            'feature_usage_std': feature_usage_std,\n",
    "            'feature_usage_entropy': feature_usage_entropy,\n",
    "            'mean_correlation': mean_correlation,\n",
    "            'feature_importance_std': feature_importance_std\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def kl_divergence_sparsity(self, rho_hat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute KL divergence between average activation and target sparsity\n",
    "        \n",
    "        Args:\n",
    "            rho_hat: Average activation of hidden units [hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            KL divergence loss measuring deviation from target sparsity\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10  # Small constant for numerical stability\n",
    "        rho = self.target_sparsity\n",
    "        \n",
    "        # Clip values to prevent log(0)\n",
    "        rho_hat = torch.clamp(rho_hat, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_div = rho * torch.log((rho + epsilon) / (rho_hat + epsilon)) + \\\n",
    "                 (1 - rho) * torch.log((1 - rho + epsilon) / (1 - rho_hat + epsilon))\n",
    "        \n",
    "        return kl_div\n",
    "    \n",
    "    def analyze_feature(self, feature_idx: int, dataloader: torch.utils.data.DataLoader) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze a specific feature's behavior across the dataset\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of the feature to analyze\n",
    "            dataloader: DataLoader containing samples to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing feature analysis\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        activations = []\n",
    "        coactivations = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = batch.reshape(-1, batch.size(-1)).to(self.device)\n",
    "                _, h = self.forward(batch)\n",
    "                \n",
    "                # Get activations for this feature\n",
    "                feature_acts = h[:, feature_idx]\n",
    "                activations.append(feature_acts)\n",
    "                \n",
    "                # Get co-activated features\n",
    "                active_mask = torch.abs(h) > self.activation_threshold\n",
    "                coactive = active_mask & (active_mask[:, feature_idx].unsqueeze(1))\n",
    "                coactivations.append(coactive)\n",
    "        \n",
    "        activations = torch.cat(activations)\n",
    "        coactivations = torch.cat(coactivations)\n",
    "        \n",
    "        analysis = {\n",
    "            'mean_activation': torch.mean(activations).item(),\n",
    "            'std_activation': torch.std(activations).item(),\n",
    "            'activation_rate': torch.mean((torch.abs(activations) > self.activation_threshold).float()).item(),\n",
    "            'max_activation': torch.max(torch.abs(activations)).item(),\n",
    "            'top_coactivated_features': torch.topk(torch.sum(coactivations, dim=0), k=5)[1].tolist()\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "def evaluate_model(\n",
    "    model: SparseAutoencoder,\n",
    "    dataloader: DataLoader,\n",
    "    device: str\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate model on given dataloader\"\"\"\n",
    "    model.eval()\n",
    "    all_metrics = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.reshape(-1, batch.size(-1)).to(device)\n",
    "            metrics = model.compute_metrics(batch)\n",
    "            all_metrics.append(metrics)\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_metrics = {\n",
    "        k: np.mean([m[k] for m in all_metrics])\n",
    "        for k in all_metrics[0].keys()\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "def plot_training_history(history: dict):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    metrics = ['total_loss', 'recon_loss', 'sparsity_ratio', 'dead_features']\n",
    "    \n",
    "    for ax, metric in zip(axes.flat, metrics):\n",
    "        ax.plot(history[f'train_{metric}'], label='Train')\n",
    "        ax.plot(history[f'val_{metric}'], label='Validation')\n",
    "        ax.set_title(metric)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "def train_autoencoder(\n",
    "    dataset_name: str = \"wikitext\",\n",
    "    dataset_config: str = \"wikitext-103-raw-v1\",\n",
    "    model_name: str = \"gpt2\",\n",
    "    layer_idx: int = 6,\n",
    "    hidden_dim: int = 4096,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 50,\n",
    "    max_samples: Optional[int] = 10000,\n",
    "    lr: float = 5e-4,  # Reduced learning rate\n",
    "    sparsity_lambda: float = 0.1,  # Increased sparsity penalty\n",
    "    target_sparsity: float = 0.05,  # Target 5% activation\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    val_split: float = 0.1\n",
    ") -> tuple[SparseAutoencoder, dict]:\n",
    "    \"\"\"Train sparse autoencoder with validation\"\"\"\n",
    "    # Create full dataset\n",
    "    full_dataset = MemoryCachedTransformerDataset(\n",
    "        dataset_name=dataset_name,\n",
    "        dataset_config=dataset_config,\n",
    "        model_name=model_name,\n",
    "        layer_idx=layer_idx,\n",
    "        max_samples=max_samples,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Split into train/val\n",
    "    val_size = int(len(full_dataset) * val_split)\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Initialize model with new parameters\n",
    "    sae = SparseAutoencoder(\n",
    "        input_dim=full_dataset.hidden_size,\n",
    "        hidden_dim=hidden_dim,\n",
    "        sparsity_lambda=sparsity_lambda,\n",
    "        target_sparsity=target_sparsity,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_total_loss': [], 'val_total_loss': [],\n",
    "        'train_recon_loss': [], 'val_recon_loss': [],\n",
    "        'train_sparsity_ratio': [], 'val_sparsity_ratio': [],\n",
    "        'train_dead_features': [], 'val_dead_features': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        sae.train()\n",
    "        epoch_metrics = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = batch.reshape(-1, full_dataset.hidden_size).to(device)\n",
    "            \n",
    "            loss, metrics = sae.loss(batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(sae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_metrics.append(metrics)\n",
    "        \n",
    "        train_metrics = {\n",
    "            k: np.mean([m[k] for m in epoch_metrics])\n",
    "            for k in epoch_metrics[0].keys()\n",
    "        }\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_model(sae, val_loader, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_total_loss'].append(train_metrics['total_loss'])\n",
    "        history['val_total_loss'].append(val_metrics['total_loss'])\n",
    "        history['train_recon_loss'].append(train_metrics['recon_loss'])\n",
    "        history['val_recon_loss'].append(val_metrics['recon_loss'])\n",
    "        history['train_sparsity_ratio'].append(train_metrics['sparsity_ratio'])\n",
    "        history['val_sparsity_ratio'].append(val_metrics['sparsity_ratio'])\n",
    "        history['train_dead_features'].append(train_metrics['dead_features'])\n",
    "        history['val_dead_features'].append(val_metrics['dead_features'])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nEpoch {epoch+1} metrics:\")\n",
    "        print(\"Train:\", {k: f\"{v:.4f}\" for k, v in train_metrics.items()})\n",
    "        print(\"Val:\", {k: f\"{v:.4f}\" for k, v in val_metrics.items()})\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_metrics['total_loss'])\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['total_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['total_loss']\n",
    "            torch.save(sae.state_dict(), 'best_sae.pt')\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_training_history(history)\n",
    "    \n",
    "    return sae, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sae, history = train_autoencoder(\n",
    "        dataset_name=\"wikitext\",\n",
    "        dataset_config=\"wikitext-103-raw-v1\",\n",
    "        model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "        max_samples=3000,\n",
    "        num_epochs=50,\n",
    "        batch_size=16,\n",
    "        hidden_dim=5000,\n",
    "        val_split=0.2,\n",
    "        sparsity_lambda=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1163f00e-ab16-45ee-ac86-1227c4af81a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0081698-26aa-4c6c-9f61-9bd99d3453ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
