{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be4d772-3002-479d-8c81-fdfd262776ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Structure:\n",
      "└─ document: root\n",
      "  └─ Text: twoside,11pt\n",
      "  └─ Text: article\n",
      "  └─ Text: algorithm\n",
      "  └─ Text: abbrvbib,nohyperref,preprint\n",
      "  └─ Text: jmlr2e\n",
      "  └─ Text: ref,caption\n",
      "  └─ Text: leaf\n",
      "  └─ Text: listings\n",
      "  └─ Text: xcolor\n",
      "  └─ Text: codegreen\n",
      "  └─ Text: rgb\n",
      "  └─ Text: 0,0.6,0\n",
      "  └─ Text: codegray\n",
      "  └─ Text: rgb\n",
      "  └─ Text: 0.5,0.5,0.5\n",
      "  └─ Text: codepurple\n",
      "  └─ Text: rgb\n",
      "  └─ Text: 0.58,0,0.82\n",
      "  └─ Text: backcolour\n",
      "  └─ Text: rgb\n",
      "  └─ Text: 0.95,0.95,0.92\n",
      "  └─ Text: dark-blue\n",
      "  └─ Text: rgb\n",
      "  └─ Text: 0.15,0.15,0.4\n",
      "  └─ Text: medium-blue\n",
      "  └─ Text: rgb\n",
      "  └─ Text: 0,0,0.5\n",
      "  └─ Text: mystyle\n",
      "  └─ Text: \n",
      "    backgroundcolor=\n",
      "  └─ Text: backcolour\n",
      "  └─ Text: ,   \n",
      "    commentstyle=\n",
      "  └─ Text: codegreen\n",
      "  └─ Text: ,\n",
      "    keywordstyle=\n",
      "  └─ Text: magenta\n",
      "  └─ Text: ,\n",
      "    numberstyle=\n",
      "  └─ Text: codegray\n",
      "  └─ Text: ,\n",
      "    stringstyle=\n",
      "  └─ Text: codepurple\n",
      "  └─ Text: ,\n",
      "    basicstyle=\n",
      "  └─ Text: ,\n",
      "    breakatwhitespace=false,         \n",
      "    breakl...\n",
      "  └─ Text: \n",
      "  colorlinks, linkcolor=\n",
      "  └─ Text: dark-blue\n",
      "  └─ Text: ,\n",
      "  citecolor=\n",
      "  └─ Text: dark-blue\n",
      "  └─ Text: , urlcolor=\n",
      "  └─ Text: medium-blue\n",
      "  └─ Text: style=mystyle\n",
      "  └─ Text: %% Layout\n",
      "  └─ Text: 0em\n",
      "  └─ Text: 1em\n",
      "  └─ Text: 4\n",
      "  └─ Text: #1\n",
      "  └─ Text: [\n",
      "  └─ Text: ^\n",
      "  └─ Text: #2\n",
      "  └─ Text: _\n",
      "  └─ Text: #3\n",
      "  └─ Text: #4\n",
      "  └─ Text: ]\n",
      "  └─ Text: 1\n",
      "  └─ Text: blue\n",
      "  └─ Text: S\n",
      "  └─ Text: K\n",
      "  └─ Text: #1\n",
      "  └─ Text: 1\n",
      "  └─ Text: green\n",
      "  └─ Text: W\n",
      "  └─ Text: M\n",
      "  └─ Text: #1\n",
      "  └─ Text: 1\n",
      "  └─ Text: cyan\n",
      "  └─ Text: P\n",
      "  └─ Text: I\n",
      "  └─ Text: #1\n",
      "  └─ Text: 1\n",
      "  └─ Text: red\n",
      "  └─ Text: A\n",
      "  └─ Text: W\n",
      "  └─ Text: #1\n",
      "  └─ Text: green\n",
      "  └─ Text:  Done\n",
      "  └─ Text: %% Notations\n",
      "  └─ Text: On Uncertainty, Tempering, and Data Augmentation i...\n",
      "  └─ Text: On Uncertainty, Tempering, and Data Augmentation i...\n",
      "  └─ Text:  Sanyam Kapoor\n",
      "  └─ Text: Equal contribution.\n",
      "  └─ Text: , \n",
      "  └─ Text:  New York University  \n",
      "  └─ Text: \\\\\n",
      "  └─ Text:  Wesley J. Maddox\n",
      "  └─ Text: ^*\n",
      "  └─ Text: , \n",
      "  └─ Text:  New York University \n",
      "  └─ Text: \\\\\n",
      "  └─ Text:  Pavel Izmailov\n",
      "  └─ Text: ^*\n",
      "  └─ Text: , \n",
      "  └─ Text:  New York University \n",
      "  └─ Text: \\\\\n",
      "  └─ Text:  Andrew Gordon Wilson, \n",
      "  └─ Text:  New York University\n",
      "  └─ Text: \n",
      "Aleatoric uncertainty captures the inherent rando...\n",
      "  └─ section: Introduction\n",
      "    └─ Text: sec:intro\n",
      "    └─ Text: \n",
      "Uncertainty is often compartmentalized into \n",
      "    └─ Text: epistemic uncertainty\n",
      "    └─ Text:  and \n",
      "    └─ Text: aleatoric uncertainty\n",
      "    └─ Text: Hora1996AleatoryAE,Kendall2017WhatUD,Malinin2018Pr...\n",
      "    └─ Text: . Epistemic uncertainty, sometimes called \n",
      "    └─ Text: model uncertainty\n",
      "    └─ Text: , is the reducible uncertainty over which solution...\n",
      "    └─ Text: e.g.\n",
      "    └─ Text: beyer2020we\n",
      "    └─ Text: . Although measurement noise can be reduced, for e...\n",
      "    └─ Text: Senge2014ReliableCL\n",
      "    └─ Text: .\n",
      "\n",
      "In particular, our assumptions about aleatoric ...\n",
      "    └─ Text: accuracy\n",
      "    └─ Text: , not only predictive uncertainty.\n",
      "    └─ Text: Epistemic uncertainty also has a significant effec...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text: .\n",
      "    └─ Text:  In \n",
      "    └─ Text: fig:conceptual\n",
      "    └─ Text: (a,b) we show the predictive distributions of two ...\n",
      "    └─ Text: rasmussen2006gaussian\n",
      "    └─ Text:  trained on the same data and using the same RBF k...\n",
      "    └─ Text: fig:conceptual\n",
      "    └─ Text: (a) assumes a high observation noise (each point i...\n",
      "    └─ Text: N\n",
      "    └─ Text: (0,\n",
      "    └─ Text: ^2 )\n",
      "    └─ Text: , with \n",
      "    └─ Text: ^2 = 1\n",
      "    └─ Text: ).\n",
      "Consequently, this model explains many of the o...\n",
      "    └─ Text: fig:conceptual\n",
      "    └─ Text: (b) assumes a low observation noise (\n",
      "    └─ Text: ^2 = 10^\n",
      "    └─ Text: -2\n",
      "    └─ Text: ), and consequently the predictive mean runs throu...\n",
      "    └─ Text: !t\n",
      "    └─ Text: c cc cc\n",
      "    └─ Text: -0.4cm\n",
      "    └─ Text: height=.277\n",
      "    └─ Text: figures/intro_gp_highsigma.pdf\n",
      "    └─ Text: &&\n",
      "\n",
      "    └─ Text: -.7cm\n",
      "    └─ Text: height=.277\n",
      "    └─ Text: figures/intro_gp_lowsigma.pdf\n",
      "    └─ Text: &\n",
      "    └─ Text: &\n",
      "\n",
      "    └─ Text: height=.27\n",
      "    └─ Text: figures/conceptual_aleatoric.pdf\n",
      "    └─ Text: \\\\\n",
      "    └─ Text: [\n",
      "    └─ Text: 1mm\n",
      "    └─ Text: ]\n",
      "    └─ Text: (a) GP regression, \n",
      "    └─ Text: ^2 = 1\n",
      "    └─ Text: &&\n",
      "\n",
      "    └─ Text: -.4cm\n",
      "    └─ Text: (b) GP regression, \n",
      "    └─ Text: ^2 = 10^\n",
      "    └─ Text: -2\n",
      "    └─ Text: &&\n",
      "(c) Classification\n",
      "\n",
      "    └─ Text: Effect of aleatoric uncertainty.\n",
      "    └─ Text: \n",
      "    In regression problems, we can express our as...\n",
      "    └─ Text: ^2\n",
      "    └─ Text: .\n",
      "    \n",
      "    └─ Text: (a)\n",
      "    └─ Text: : Gaussian process regression with high observatio...\n",
      "    └─ Text: ^2\n",
      "    └─ Text:  explains many of the observations with noise.\n",
      "   ...\n",
      "    └─ Text: (b)\n",
      "    └─ Text: : Gaussian process regression on the same data, bu...\n",
      "    └─ Text: ^2\n",
      "    └─ Text:  fits the training data nearly perfectly.\n",
      "    \n",
      "    └─ Text: (c)\n",
      "    └─ Text: : In classification problems, we do not have a dir...\n",
      "    └─ Text: fig:conceptual\n",
      "    └─ Text: \n",
      "Our assumptions about the aleatoric uncertainty a...\n",
      "    └─ Text: fig:conceptual\n",
      "    └─ Text: (c), where we fix a dataset and consider two possi...\n",
      "    └─ Text: Scenario A\n",
      "    └─ Text: : suppose we know that the data contains label noi...\n",
      "    └─ Text: fig:conceptual\n",
      "    └─ Text: (c) are highly likely to be incorrectly labeled, a...\n",
      "    └─ Text: Scenario B\n",
      "    └─ Text: : now, suppose that we know that the data are corr...\n",
      "    └─ Text: sec:aleatoric_uncertainty_understanding\n",
      "    └─ Text:  we show that the standard softmax likelihood is e...\n",
      "    └─ Text: 1 / T\n",
      "    └─ Text:  with \n",
      "    └─ Text: T < 1\n",
      "    └─ Text: , corresponds to increasing the concentration para...\n",
      "    └─ Text: noisy Dirichlet model\n",
      "    └─ Text: , which can be viewed as an input dependent prior,...\n",
      "    └─ Text: sec:data_aug_effects\n",
      "    └─ Text: , we show theoretically how data augmentation coun...\n",
      "    └─ Text: izmailov2021bayesian\n",
      "    └─ Text:  and \n",
      "    └─ Text: fortuin2021bayesian\n",
      "    └─ Text: , and show that tempering in fact serves to \n",
      "    └─ Text: correct\n",
      "    └─ Text:  for the effects of data augmentation on represent...\n",
      "    └─ Text: sec:experiments\n",
      "    └─ Text:  we exemplify several of the conceptual findings i...\n",
      "    └─ Text: T=1\n",
      "    └─ Text: , as data augmentation softens the likelihood, and...\n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text:  note that cold posteriors can provide good perfor...\n",
      "  └─ section: Related Work\n",
      "    └─ Text: sec:related\n",
      "    └─ Text: \n",
      "Early work on Bayesian neural networks (BNNs) foc...\n",
      "    └─ Text: e.g.\n",
      "    └─ Text: Mackay1992APB, hinton1993keeping, Neal1995Bayesian...\n",
      "    └─ Text: . More recently, \n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text:  show how Bayesian model averaging is especially c...\n",
      "    └─ Text: modern\n",
      "    └─ Text:  deep networks --- covering deep ensembles \n",
      "    └─ Text: lakshminarayanan2017simple\n",
      "    └─ Text:  as approximate Bayesian inference, induced priors...\n",
      "    └─ Text: zhang2021understanding\n",
      "    └─ Text: , posterior tempering \n",
      "    └─ Text: grunwald2012safe, wenzel2020good\n",
      "    └─ Text: , and connections with loss surface structure such...\n",
      "    └─ Text: garipov2018loss\n",
      "    └─ Text: . A number of recent works have focused on making ...\n",
      "    └─ Text: e.g.,\n",
      "    └─ Text: wilson2016deep, maddox2019simple, osawa2019practic...\n",
      "    └─ Text: , often with better results than classical trainin...\n",
      "    └─ Text: cranmer2021bayesian\n",
      "    └─ Text: , to click-through rate prediction \n",
      "    └─ Text: liu2017pbodl\n",
      "    └─ Text: , to diagnosis of diabetic retinopathy \n",
      "    └─ Text: filos2019systematic\n",
      "    └─ Text: , to fluid dynamics \n",
      "    └─ Text: geneva2020modeling\n",
      "    └─ Text: . \n",
      "\n",
      "\n",
      "\n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text:  demonstrated with several examples that raising t...\n",
      "    └─ Text: 1/T\n",
      "    └─ Text: , with \n",
      "    └─ Text: T<1\n",
      "    └─ Text: , in conjunction with SGLD inference, improves per...\n",
      "    └─ Text: T=1\n",
      "    └─ Text:  can provide worse results than classical training...\n",
      "    └─ Text: cold posterior effect\n",
      "    └─ Text: . However, in a study using HMC inference \n",
      "    └─ Text: izmailov2021bayesian\n",
      "    └─ Text:  showed that for \n",
      "    └─ Text: all\n",
      "    └─ Text:  cases considered in \n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text:  there is no cold posterior effect when data augme...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text:  claims that data augmentation only explains the c...\n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text: , \n",
      "    └─ Text: izmailov2021bayesian\n",
      "    └─ Text:  in fact also shows no cold posterior effect on IM...\n",
      "    └─ Text: fortuin2021bayesian,Noci2021DisentanglingTR,nabarr...\n",
      "    └─ Text: . \n",
      "\n",
      "Several works have suggested that misspecified...\n",
      "    └─ Text: wenzel2020good,zeno2020cold,fortuin2021bayesian\n",
      "    └─ Text: . \n",
      "However, \n",
      "    └─ Text: fortuin2021bayesian\n",
      "    └─ Text:  find that the cold posterior effect cannot be all...\n",
      "    └─ Text: fortuin2021bayesian\n",
      "    └─ Text:  claim a cold posterior effect on Fashion MNIST fo...\n",
      "    └─ Text: 0.25\n",
      "    └─ Text: \\%\n",
      "    └─ Text:  on test accuracy on Fashion MNIST) that are gener...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text:  show that the experiments in \n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text:  suggesting a poor prior are easily resolved\n",
      "by tu...\n",
      "    └─ Text: izmailov2021bayesian\n",
      "    └─ Text:  also show that \n",
      "standard Gaussian priors perform ...\n",
      "    └─ Text: izmailov2021dangers\n",
      "    └─ Text:  explain how standard priors for Bayesian neural n...\n",
      "    └─ Text: EmpCov\n",
      "    └─ Text:  priors, which helps remedy this issue.\n",
      "\n",
      "\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text:  argue that many different factors --- likelihoods...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text: , which argues that tempering can partially addres...\n",
      "    └─ Text: T=1\n",
      "    └─ Text:  in general provided the best performance. In othe...\n",
      "    └─ Text: T=1\n",
      "    └─ Text:  to be suboptimal, and since in any realistic\n",
      "sett...\n",
      "    └─ Text: T=1\n",
      "    └─ Text:  or be particularly alarmed if it is not.\n",
      "\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text:  further suggest that Gaussian priors may be putti...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text:  on the other hand demonstrate how Gaussian priors...\n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text: . \n",
      "\n",
      "\n",
      "    └─ Text: aitchison2020statistical\n",
      "    └─ Text:  argue that BNN likelihoods are misspecified since...\n",
      "    └─ Text: p(y\n",
      "    └─ Text:  x)^H\n",
      "    └─ Text:  for \n",
      "    └─ Text: H\n",
      "    └─ Text:  human labelers, which connects to likelihood temp...\n",
      "    └─ Text: adlam2020cold\n",
      "    └─ Text:  also consider model misspecification, showing a c...\n",
      "    └─ Text: nabarro2021data\n",
      "    └─ Text:  modify the likelihood to accommodate data augment...\n",
      "    └─ Text: precise way in which data augmentation with SGLD l...\n",
      "    └─ Text: , a counterintuitive result which finally resolves...\n",
      "    └─ Text: T<1\n",
      "    └─ Text: , particularly with data augmentation, is a more h...\n",
      "    └─ Text: T=1\n",
      "    └─ Text: ; (5) we show how a lognormal approximation of the...\n",
      "    └─ Text: Milios2018DirichletbasedGP\n",
      "    └─ Text: , can naturally reflect our beliefs about aleatori...\n",
      "    └─ Text: https://github.com/activatedgeek/bayesian-classifi...\n",
      "    └─ Text: .\n",
      "\n",
      "\n",
      "  └─ section: Background\n",
      "    └─ Text: sec:background\n",
      "    └─ Text: Bayesian Model Averaging.\n",
      "    └─ Text:  With Bayesian inference, we aim to infer the\n",
      "post...\n",
      "    └─ Text:  = \n",
      "    └─ Text: \\{\n",
      "    └─ Text: (x_i,y_i)\n",
      "    └─ Text: \\}\n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N\n",
      "    └─ Text:  of input-output pairs, given by\n",
      "\n",
      "    └─ Text: p(\n",
      "    └─ Text: ) \n",
      "    └─ Text:  p(\n",
      "    └─ Text: )p(\n",
      "    └─ Text: )\n",
      "    └─ Text: \n",
      "for a given observation likelihood under the i.i....\n",
      "    └─ Text: p(\n",
      "    └─ Text: ) = \n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N p(y_i\n",
      "    └─ Text:  x_i, \n",
      "    └─ Text: )\n",
      "    └─ Text: , and prior over parameters \n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: . For any novel input \n",
      "    └─ Text: x_\n",
      "    └─ Text: , we estimate the posterior predictive distributio...\n",
      "    └─ Text: Bayesian model averaging\n",
      "    └─ Text: (BMA) given by\n",
      "\n",
      "    └─ Text: \n",
      "p(y_\n",
      "    └─ Text:  x_\n",
      "    └─ Text: )\t&= \n",
      "    └─ Text:  p(y_\n",
      "    └─ Text:  x_\n",
      "    └─ Text: ,\n",
      "    └─ Text: )p(\n",
      "    └─ Text: ) d\n",
      "    └─ Text:  . \n",
      "\n",
      "    └─ Text: eq:bma\n",
      "    └─ Text: \n",
      "This integral cannot be expressed in closed form ...\n",
      "    └─ Text: e.g.,\n",
      "    └─ Text: wilson2020bayesian, pml2Book\n",
      "    └─ Text: . \n",
      "\n",
      "\n",
      "    └─ Text: Cold Posteriors and Tempering.\n",
      "    └─ Text:  Let \n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text:  denote the likelihood function, \n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text:  \n",
      "the prior over parameters of the neural network,...\n",
      "    └─ Text: U(\n",
      "    └─ Text: )\n",
      "    └─ Text:  the \n",
      "    └─ Text: posterior energy function\n",
      "    └─ Text: . Following \n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text: , we then define a \n",
      "    └─ Text: cold \n",
      "posterior\n",
      "    └─ Text:  for \n",
      "    └─ Text: T<1\n",
      "    └─ Text:  as\n",
      "\n",
      "    └─ Text: \n",
      "p_\n",
      "    └─ Text: cold\n",
      "    └─ Text: (\n",
      "    └─ Text: ) \n",
      "    └─ Text:  -\n",
      "    └─ Text: 1\n",
      "    └─ Text: T\n",
      "    └─ Text:  -\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text:  - \n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: _\n",
      "    └─ Text: U(\n",
      "    └─ Text: )\n",
      "    └─ Text: , \n",
      "    └─ Text: eq:cold_posterior\n",
      "    └─ Text: \n",
      "which is effectively raises both the likelihood a...\n",
      "    └─ Text: 1/T\n",
      "    └─ Text: . \n",
      "    └─ Text: T=1\n",
      "    └─ Text:  recovers the standard Bayes' posterior. \n",
      "By compa...\n",
      "    └─ Text: tempered likelihood posterior\n",
      "    └─ Text: (e.g. \n",
      "    └─ Text: grunwald2012safe\n",
      "    └─ Text: ) only raises the likelihood term to a power \n",
      "    └─ Text: 1/T\n",
      "    └─ Text:  as\n",
      "\n",
      "    └─ Text: \n",
      "p_\n",
      "    └─ Text: temp\n",
      "    └─ Text: (\n",
      "    └─ Text: ) \n",
      "    └─ Text: - \n",
      "    └─ Text: -\n",
      "    └─ Text: 1\n",
      "    └─ Text: T\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text:  - \n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: . \n",
      "    └─ Text: eq:tempered_posterior\n",
      "    └─ Text: Stochastic Gradient Langevin Dynamics (SGLD).\n",
      "    └─ Text:  For large-scale neural \n",
      "networks, exact posterior...\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text:  over parameters of a neural network, we simulate ...\n",
      "    └─ Text: Srkk2019AppliedSD,welling2011bayesian\n",
      "    └─ Text: &= \n",
      "    └─ Text: M\n",
      "    └─ Text: ^\n",
      "    └─ Text: -1\n",
      "    └─ Text: m\n",
      "    └─ Text: , \n",
      "    └─ Text: \\\\\n",
      "    └─ Text: m\n",
      "    └─ Text: &= - \n",
      "    └─ Text: _\n",
      "    └─ Text: U\n",
      "    └─ Text: (\n",
      "    └─ Text: ) \n",
      "    └─ Text:  - \n",
      "    └─ Text: m\n",
      "    └─ Text:  + \n",
      "    └─ Text: 2\n",
      "    └─ Text:  T\n",
      "    └─ Text: , \n",
      "    └─ Text:  where \n",
      "    └─ Text: 0\n",
      "    └─ Text: , \n",
      "    └─ Text: M\n",
      "    └─ Text: ,\n",
      "\n",
      "    └─ Text: eq:langevin_sde\n",
      "    └─ Text: \n",
      "where \n",
      "    └─ Text: m\n",
      "    └─ Text:  are the auxiliary momentum variables, \n",
      "    └─ Text: M\n",
      "    └─ Text:  is\n",
      "the mass matrix which acts as a preconditioner...\n",
      "    └─ Text:  is the friction parameter, \n",
      "    └─ Text: T\n",
      "    └─ Text:  is the temperature, \n",
      "\n",
      "    └─ Text:  t = \n",
      "    └─ Text:  is the time discretization (step size),\n",
      "and \n",
      "    └─ Text: _\n",
      "    └─ Text: U\n",
      "    └─ Text: (\n",
      "    └─ Text: )\n",
      "    └─ Text:  is an unbiased estimator of the gradient \n",
      "    └─ Text: _\n",
      "    └─ Text:  U(\n",
      "    └─ Text: )\n",
      "    └─ Text:  using only a subset of the dataset \n",
      "    └─ Text:  for computational efficiency. \n",
      "For \n",
      "    └─ Text:  0\n",
      "    └─ Text:  in the limit of time \n",
      "    └─ Text: t \n",
      "    └─ Text: , simulating \n",
      "    └─ Text: eq:langevin_sde\n",
      "    └─ Text:  produces a trajectory \n",
      "distributed according to t...\n",
      "    └─ Text:  -U(\n",
      "    └─ Text: )/T \n",
      "    └─ Text: , which is exactly the posterior\n",
      "\n",
      "    └─ Text: p_\n",
      "    └─ Text: cold\n",
      "    └─ Text:  in \n",
      "    └─ Text: eq:cold_posterior\n",
      "    └─ Text:  we desire.\n",
      "When \n",
      "    └─ Text:  = 0\n",
      "    └─ Text: , \n",
      "    └─ Text: eq:langevin_sde\n",
      "    └─ Text:  describes the Stochastic Gradient Langevin Dynami...\n",
      "    └─ Text: welling2011bayesian\n",
      "    └─ Text: , and otherwise it describes the Stochastic Gradie...\n",
      "    └─ Text: 1-\n",
      "    └─ Text:  represents the momentum \n",
      "    └─ Text: cheni14\n",
      "    └─ Text: . \n",
      "Furthermore, we can sample from \n",
      "    └─ Text: eq:tempered_posterior\n",
      "    └─ Text:  by setting \n",
      "    └─ Text: T = 1\n",
      "    └─ Text:  in \n",
      "    └─ Text: eq:langevin_sde\n",
      "    └─ Text:  and raising only the likelihood to a power \n",
      "    └─ Text: T\n",
      "    └─ Text: . In addition, it is often beneficial to use a cyc...\n",
      "    └─ Text:  in \n",
      "    └─ Text: eq:langevin_sde\n",
      "    └─ Text: , as proposed by \n",
      "    └─ Text: zhang2019cyclical\n",
      "    └─ Text:  for cyclical-SGLD (cSGLD) when \n",
      "    └─ Text:  = 0\n",
      "    └─ Text: , and cyclical-SGHMC (cSGHMC) when \n",
      "    └─ Text:  > 0\n",
      "    └─ Text: .\n",
      "\n",
      "\n",
      "    └─ Text: Bayesian Classification.\n",
      "    └─ Text:  For a \n",
      "    └─ Text: C\n",
      "    └─ Text: -class classification problem, a standard choice o...\n",
      "    └─ Text: p(y \n",
      "    └─ Text:  x,\n",
      "    └─ Text: ) = \n",
      "    └─ Text: Cat\n",
      "    └─ Text: (\n",
      "    └─ Text: [\n",
      "    └─ Text: _1,\n",
      "    └─ Text: _2,\n",
      "    └─ Text: ,\n",
      "    └─ Text: _C\n",
      "    └─ Text: ]\n",
      "    └─ Text: )\n",
      "    └─ Text: , where each class probability is computed using a...\n",
      "    └─ Text: f(x;\n",
      "    └─ Text: ) \n",
      "    └─ Text: ^C\n",
      "    └─ Text:  as \n",
      "    └─ Text: _c \n",
      "    └─ Text: \\{\n",
      "    └─ Text: [\n",
      "    └─ Text: f(x; \n",
      "    └─ Text: )\n",
      "    └─ Text: ]\n",
      "    └─ Text: _c \n",
      "    └─ Text: \\}\n",
      "    └─ Text: , and hence called the \n",
      "    └─ Text: softmax likelihood\n",
      "    └─ Text: . The negative log of the softmax likelihood, i.e....\n",
      "    └─ Text: -\n",
      "    └─ Text: p(y \n",
      "    └─ Text:  x,\n",
      "    └─ Text: )\n",
      "    └─ Text:  is exactly the standard cross-entropy loss. The p...\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text:  is often chosen to be an isotropic Gaussian \n",
      "    └─ Text: 0, \n",
      "    └─ Text: ^2 \n",
      "    └─ Text: I\n",
      "    └─ Text:  with some fixed variance \n",
      "    └─ Text: ^2\n",
      "    └─ Text: . Approximate posterior samples are often obtained...\n",
      "    └─ Text: eq:bma\n",
      "    └─ Text:  using simple Monte Carlo.\n",
      "\n",
      "\n",
      "  └─ section: Aleatoric Uncertainty in Bayesian Classification\n",
      "    └─ Text: sec:aleatoric_uncertainty_understanding\n",
      "    └─ Text: \n",
      "We will now discuss how we represent aleatoric un...\n",
      "    └─ Text: sec: howaleatoric\n",
      "    └─ Text: ), how tempering reduces aleatoric uncertainty (\n",
      "    └─ Text: sec: liktemp\n",
      "    └─ Text: ), and how\n",
      "we can explicitly represent aleatoric u...\n",
      "    └─ Text: sec: ndm\n",
      "    └─ Text: ). In \n",
      "    └─ Text: sec:data_aug_effects\n",
      "    └─ Text:  we will use these \n",
      "foundations to show precisely ...\n",
      "    └─ subsection: How do we represent aleatoric uncertainty in classification?\n",
      "      └─ Text: sec: howaleatoric\n",
      "      └─ Text: \n",
      "Let us consider the Bayesian neural network poste...\n",
      "      └─ Text: w\n",
      "      └─ Text:  in a classification problem:\n",
      "\n",
      "      └─ Text: eq:posterior\n",
      "      └─ Text: \n",
      "    p(w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: \n",
      "    p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text:  f_y(x, w),\n",
      "\n",
      "      └─ Text: \n",
      "where we denote the output of the softmax layer o...\n",
      "      └─ Text: y\n",
      "      └─ Text:  on input \n",
      "      └─ Text: x\n",
      "      └─ Text:  as \n",
      "      └─ Text: f_y(x, w)\n",
      "      └─ Text: .\n",
      "We can think of the class probability vectors \n",
      "      └─ Text: f(x) = (f_1(x, w), \n",
      "      └─ Text: , f_C(x, w))\n",
      "      └─ Text:  as latent variables, where the prior distribution...\n",
      "      └─ Text: p(w)\n",
      "      └─ Text:  over\n",
      "the parameters of the network implies a join...\n",
      "      └─ Text: \\{\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: \\}\n",
      "      └─ Text: _\n",
      "      └─ Text: x \n",
      "      └─ Text:  D\n",
      "      └─ Text: .\n",
      "We will initially focus on the observation model...\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: :\n",
      "\n",
      "      └─ Text: eq:adding_uniform_prior\n",
      "      └─ Text: \n",
      "    p(y \n",
      "      └─ Text:  f(x)) = f_y(x) \n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (1, \n",
      "      └─ Text: , 1)(f(x)) \n",
      "      └─ Text:  f_y(x),\n",
      "\n",
      "      └─ Text: \n",
      "where Dir. denotes the Dirichlet distribution and...\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (1, \n",
      "      └─ Text: , 1)\n",
      "      └─ Text:  is a uniform distribution over the class probabil...\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: (with support on the unit \n",
      "      └─ Text: C\n",
      "      └─ Text: -simplex), and the proportionality is with respect...\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: .\n",
      "We can view the right hand side of Eq. \n",
      "      └─ Text: eq:adding_uniform_prior\n",
      "      └─ Text:  as the unnormalized posterior in a multinomial mo...\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: , and observe a single count of class \n",
      "      └─ Text: y\n",
      "      └─ Text: .\n",
      "Using the fact that the Dirichlet distribution i...\n",
      "      └─ Text: e.g.,\n",
      "      └─ Text: Ch. 2.2.1\n",
      "      └─ Text: bishop06\n",
      "      └─ Text: , we can rewrite the right hand side of \n",
      "      └─ Text: eq:adding_uniform_prior\n",
      "      └─ Text:  as follows:\n",
      "\n",
      "      └─ Text: eq:likelihood_dirichlet\n",
      "      └─ Text: \n",
      "    p(y \n",
      "      └─ Text:  f(x)) \n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (1, \n",
      "      └─ Text: , 1, \n",
      "      └─ Text: 2\n",
      "      └─ Text: _\n",
      "      └─ Text: position\n",
      "      └─ Text: ~y\n",
      "      └─ Text: , 1, \n",
      "      └─ Text: , 1)(f(x)).\n",
      "\n",
      "      └─ Text: \n",
      "Eq. \n",
      "      └─ Text: eq:likelihood_dirichlet\n",
      "      └─ Text:  describes the distribution induced by the observa...\n",
      "      └─ Text: y\n",
      "      └─ Text:  on the predicted class probabilities for the corr...\n",
      "      └─ Text: x\n",
      "      └─ Text: .\n",
      "The posterior over the parameters \n",
      "      └─ Text: w\n",
      "      └─ Text:  of the network can then be written as a product o...\n",
      "      └─ Text: eq:likelihood_dirichlet\n",
      "      └─ Text:  and the prior \n",
      "      └─ Text: p(w)\n",
      "      └─ Text: :\n",
      "\n",
      "      └─ Text: eq:posterior_dir\n",
      "      └─ Text: \n",
      "    p(w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: \n",
      "    p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (1, \n",
      "      └─ Text: , 1, \n",
      "      └─ Text: 2\n",
      "      └─ Text: _\n",
      "      └─ Text: position\n",
      "      └─ Text: ~y\n",
      "      └─ Text: , 1, \n",
      "      └─ Text: , 1)(f(x)).\n",
      "\n",
      "      └─ Text: eq:posterior_dir\n",
      "      └─ Text:  provides intuition for how Bayesian neural networ...\n",
      "      └─ Text: p(w)\n",
      "      └─ Text:  and assume that the implied prior over \n",
      "      └─ Text: f(x)\n",
      "      └─ Text:  is uniform, then for an observation \n",
      "      └─ Text: (x, y)\n",
      "      └─ Text:  the posterior over \n",
      "      └─ Text: f(x)\n",
      "      └─ Text:  is Dir.\n",
      "      └─ Text: (1, \n",
      "      └─ Text: , 2, \n",
      "      └─ Text: , 1)\n",
      "      └─ Text:  and the posterior mean for the probability of the...\n",
      "      └─ Text:  E f_y(x)\n",
      "      └─ Text:  is \n",
      "      └─ Text:  2 \n",
      "      └─ Text: C + 1\n",
      "      └─ Text: .\n",
      "For example, for a dataset with \n",
      "      └─ Text: 100\n",
      "      └─ Text:  classes (e.g. CIFAR-100), the model on average wi...\n",
      "      └─ Text: 2 / 101 \n",
      "      └─ Text:  2\n",
      "      └─ Text: \\%\n",
      "      └─ Text:  confident in the correct label on the \n",
      "      └─ Text: training data\n",
      "      └─ Text: !\n",
      "\n",
      "In practice, the prior \n",
      "      └─ Text: p(w)\n",
      "      └─ Text:  will imply a non-trivial joint prior distribution...\n",
      "      └─ Text: \\{\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: \\}\n",
      "      └─ Text: _\n",
      "      └─ Text: x \n",
      "      └─ Text:  D\n",
      "      └─ Text: , so the actual posterior may be more (or less) co...\n",
      "      └─ Text: see\n",
      "      └─ Text: for empirical analysis\n",
      "      └─ Text: wenzel2020good, wilson2020bayesian\n",
      "      └─ Text: .\n",
      "In particular, most practitioners use simple \n",
      "      └─ Text: p(w) = \n",
      "      └─ Text:  N(0, \n",
      "      └─ Text: ^2 I)\n",
      "      └─ Text:  priors, regardless of the amount of label noise i...\n",
      "      └─ Text: f(x)\n",
      "      └─ Text:  is challenging, but we will show how modifying th...\n",
      "      └─ Text: eq:posterior_dir\n",
      "      └─ Text:  suggests two natural ways of modifying the poster...\n",
      "    └─ subsection: Likelihood tempering reduces aleatoric uncertainty\n",
      "      └─ Text: sec: liktemp\n",
      "      └─ Text: \n",
      "The tempered likelihood posterior (see \n",
      "      └─ Text: sec:background\n",
      "      └─ Text: ) corresponding to the posterior in \n",
      "      └─ Text: eq:posterior\n",
      "      └─ Text:  in BNN classification can be written as\n",
      "\n",
      "      └─ Text: eq:temp_posterior\n",
      "      └─ Text: \n",
      "    p_\n",
      "      └─ Text: temp\n",
      "      └─ Text: (w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: \n",
      "    p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text:  f_y(x, w)^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text: ,\n",
      "\n",
      "      └─ Text: \n",
      "where \n",
      "      └─ Text: T\n",
      "      └─ Text:  is the temperature.\n",
      "Analogously to the derivation...\n",
      "      └─ Text: eq:temp_posterior_dirichlet\n",
      "      └─ Text: \n",
      "    p_\n",
      "      └─ Text: temp\n",
      "      └─ Text: (w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: \n",
      "    p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: 1, \n",
      "      └─ Text: , 1, \n",
      "      └─ Text: 1 + \n",
      "      └─ Text:  1 T\n",
      "      └─ Text: _\n",
      "      └─ Text: position\n",
      "      └─ Text: ~y\n",
      "      └─ Text: , 1, \n",
      "      └─ Text: , 1\n",
      "      └─ Text: (f(x)).\n",
      "\n",
      "      └─ Text: \n",
      "In other words, the tempered posterior correspond...\n",
      "      └─ Text: 1 / T\n",
      "      └─ Text:  counts of class \n",
      "      └─ Text: y\n",
      "      └─ Text:  for each input \n",
      "      └─ Text: x\n",
      "      └─ Text:  in the training data.\n",
      "In particular, assuming the...\n",
      "      └─ Text: p(w)\n",
      "      └─ Text:  implies a uniform distribution over \n",
      "      └─ Text: f(x)\n",
      "      └─ Text: , the confidence in the correct labels on the trai...\n",
      "      └─ Text:  E f_y(x)\n",
      "      └─ Text:  is \n",
      "      └─ Text: 1 + 1 / T\n",
      "      └─ Text: C + 1 / T\n",
      "      └─ Text:  = \n",
      "      └─ Text: T + 1\n",
      "      └─ Text: C T + 1\n",
      "      └─ Text: .\n",
      "For a dataset with \n",
      "      └─ Text: 100\n",
      "      └─ Text:  classes and at temperature \n",
      "      └─ Text: T = 10^\n",
      "      └─ Text: -2\n",
      "      └─ Text:  we get an average confidence of \n",
      "      └─ Text: 50.5\n",
      "      └─ Text: \\%\n",
      "      └─ Text: , much higher than the \n",
      "      └─ Text: 2\n",
      "      └─ Text: \\%\n",
      "      └─ Text:  for the standard observation model.\n",
      "\n",
      "\n",
      "      └─ Text: Tempering the Likelihood vs Tempering the Posterio...\n",
      "      └─ Text: \n",
      "Prior work has mostly considered tempering the fu...\n",
      "      └─ Text: eq:cold_posterior\n",
      "      └─ Text:  as opposed to just tempering the likelihood in \n",
      "      └─ Text: eq:tempered_posterior\n",
      "      └─ Text: . \n",
      "In \n",
      "      └─ Text: sec:cp_and_sharpness\n",
      "      └─ Text:  we show that tempering the full Bayesian posterio...\n",
      "      └─ Text: wenzel2020good\n",
      "      └─ Text:  as tempering the full posterior.\n",
      "\n",
      "\n",
      "      └─ Text: How should we think about tempering?\n",
      "      └─ Text: \n",
      "Prior work has asserted that posterior tempering ...\n",
      "      └─ Text: wenzel2020good,Noci2021DisentanglingTR,fortuin2021...\n",
      "      └─ Text: .\n",
      "We, on the other hand, argue that likelihood tem...\n",
      "      └─ Text: aitchison2020statistical\n",
      "      └─ Text:  argue that the cold posterior effect can be cause...\n",
      "      └─ Text: 10\n",
      "      └─ Text:  and ImageNet datasets have very relatively little...\n",
      "      └─ Text: e.g.\n",
      "      └─ Text: pleiss2020identifying\n",
      "      └─ Text: .\n",
      "Thus, we may\n",
      "expect that tempered likelihoods wi...\n",
      "      └─ Text: wilson2020bayesian\n",
      "      └─ Text:  also argue that posterior tempering can be viewed...\n",
      "      └─ Text: Is Tempered Likelihood a Valid Likelihood?\n",
      "      └─ Text: \n",
      "In classical Bayesian inference, the observation ...\n",
      "      └─ Text: p(y \n",
      "      └─ Text:  x)\n",
      "      └─ Text:  over the labels conditioned on the input.\n",
      "\n",
      "      └─ Text: wenzel2020good\n",
      "      └─ Text:  argue that the tempered softmax likelihood is in ...\n",
      "      └─ Text: 1\n",
      "      └─ Text:  over classes.\n",
      "However, \n",
      "      └─ Text: wenzel2020good\n",
      "      └─ Text:  show, the tempered softmax likelihood with \n",
      "      └─ Text: T < 1\n",
      "      └─ Text:  can be interpreted as a valid likelihood if we in...\n",
      "      └─ Text: eq:temp_posterior_dirichlet\n",
      "      └─ Text: , we can naturally interpret the tempered likeliho...\n",
      "      └─ Text: 1 / T\n",
      "      └─ Text:  counts of the label are observed for each of the ...\n",
      "    └─ subsection: Noisy Dirichlet model: changing the prior over class probabilities\n",
      "      └─ Text: sec: ndm\n",
      "      └─ Text: \n",
      "As we have seen in \n",
      "      └─ Text: eq:temp_posterior_dirichlet\n",
      "      └─ Text: , likelihood tempering increases the posterior con...\n",
      "      └─ Text: y\n",
      "      └─ Text:  from \n",
      "      └─ Text: 2\n",
      "      └─ Text:  to \n",
      "      └─ Text: 1 + 1/T\n",
      "      └─ Text: . \n",
      "We can achieve a similar effect by \n",
      "      └─ Text: decreasing the concentration of the unobserved cla...\n",
      "      └─ Text:  instead\n",
      "      └─ Text: \n",
      "While for concreteness we discuss increasing the ...\n",
      "      └─ Text: increasing\n",
      "      └─ Text:  the concentration parameters for the unobserved c...\n",
      "      └─ Text: !\n",
      "Indeed, consider the distribution\n",
      "\n",
      "      └─ Text: eq:posterior_dir_noisy\n",
      "      └─ Text: \n",
      "    p_\n",
      "      └─ Text: ND\n",
      "      └─ Text: (w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: \n",
      "    p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (\n",
      "      └─ Text: _\n",
      "      └─ Text: , \n",
      "      └─ Text: , \n",
      "      └─ Text: _\n",
      "      └─ Text: , \n",
      "      └─ Text: _\n",
      "      └─ Text: +1\n",
      "      └─ Text: _\n",
      "      └─ Text: position\n",
      "      └─ Text: ~y\n",
      "      └─ Text: , \n",
      "      └─ Text: _\n",
      "      └─ Text: , \n",
      "      └─ Text: , \n",
      "      └─ Text: _\n",
      "      └─ Text: )(f(x)),\n",
      "\n",
      "      └─ Text: \n",
      "where ND in \n",
      "      └─ Text: p_\n",
      "      └─ Text: ND\n",
      "      └─ Text:  stands for \n",
      "      └─ Text: Noisy Dirichlet\n",
      "      └─ Text:  and \n",
      "      └─ Text: _\n",
      "      └─ Text:  is a tunable parameter.\n",
      "The noisy Dirichlet model...\n",
      "      └─ Text: Milios2018DirichletbasedGP\n",
      "      └─ Text:  in the context of Gaussian process classification...\n",
      "      └─ Text: p(w)\n",
      "      └─ Text:  induces a uniform distribution over \n",
      "      └─ Text: f(x)\n",
      "      └─ Text: , for a problem with \n",
      "      └─ Text: 100\n",
      "      └─ Text:  classes and \n",
      "      └─ Text: _\n",
      "      └─ Text:  = 10^\n",
      "      └─ Text: -2\n",
      "      └─ Text:  we have expected confidence \n",
      "\n",
      "      └─ Text:  E f_y(x) = \n",
      "      └─ Text: _\n",
      "      └─ Text:  + 1\n",
      "      └─ Text: C\n",
      "      └─ Text: _\n",
      "      └─ Text:  + 1\n",
      "      └─ Text:  = 50.5\n",
      "      └─ Text: \\%\n",
      "      └─ Text: , which is the same as for the tempered likelihood...\n",
      "      └─ Text: T=10^\n",
      "      └─ Text: -2\n",
      "      └─ Text: .\n",
      "\n",
      "Now, again using the conjugacy of the Dirichlet...\n",
      "      └─ Text: p_\n",
      "      └─ Text: ND\n",
      "      └─ Text:  as follows:\n",
      "\n",
      "      └─ Text: eq:posterior_dir_noisy_final\n",
      "      └─ Text: \n",
      "    p_\n",
      "      └─ Text: ND\n",
      "      └─ Text: (w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (\n",
      "      └─ Text: _\n",
      "      └─ Text: , \n",
      "      └─ Text: , \n",
      "      └─ Text: _\n",
      "      └─ Text: )(f(x))\n",
      "      └─ Text: ^\n",
      "      └─ Text: q_\n",
      "      └─ Text: ND\n",
      "      └─ Text: (w)\n",
      "      └─ Text:  f_y(x) = q_\n",
      "      └─ Text: ND\n",
      "      └─ Text: (w) f_y(w).\n",
      "\n",
      "      └─ Text: \n",
      "We can interpret \n",
      "      └─ Text: q_\n",
      "      └─ Text: ND\n",
      "      └─ Text: (w) = p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x \n",
      "      └─ Text:  D\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (\n",
      "      └─ Text: _\n",
      "      └─ Text: , \n",
      "      └─ Text: , \n",
      "      └─ Text: _\n",
      "      └─ Text: )(f(x))\n",
      "      └─ Text:  as a prior over the parameters \n",
      "      └─ Text: w\n",
      "      └─ Text:  of the model.\n",
      "Indeed, \n",
      "      └─ Text: q_\n",
      "      └─ Text: ND\n",
      "      └─ Text: (w)\n",
      "      └─ Text:  does not depend on the labels \n",
      "      └─ Text: y\n",
      "      └─ Text: , and simply forces the predicted class probabilit...\n",
      "      └─ Text: EmpCov\n",
      "      └─ Text:  prior in \n",
      "      └─ Text: izmailov2021dangers\n",
      "      └─ Text:  for another example of a prior that depends on th...\n",
      "      └─ Text: valid likelihood\n",
      "      └─ Text:  --- the standard softmax likelihood --- with a pr...\n",
      "      └─ Text: sec:exp_bnn_images\n",
      "      └─ Text:  we will see that the noisy Dirichlet model remove...\n",
      "      └─ Text: Gaussian Approximation.\n",
      "      └─ Text: Milios2018DirichletbasedGP\n",
      "      └─ Text:  considered the distribution in \n",
      "      └─ Text: eq:posterior_dir_noisy_final\n",
      "      └─ Text:  in the context of Gaussian process classification...\n",
      "      └─ Text: Dir.\n",
      "      └─ Text: (f(x))\n",
      "      └─ Text:  over the class probabilities \n",
      "      └─ Text: f(x)\n",
      "      └─ Text:  with a product of independent Gaussian distributi...\n",
      "      └─ Text: z(x)\n",
      "      └─ Text: :\n",
      "\n",
      "      └─ Text: eq:posterior_dir_noisy_gaussian\n",
      "      └─ Text: \n",
      "    p_\n",
      "      └─ Text: NDG\n",
      "      └─ Text: (w \n",
      "      └─ Text:  D)\n",
      "    \n",
      "      └─ Text: \n",
      "    p(w) \n",
      "      └─ Text: _\n",
      "      └─ Text: x, y \n",
      "      └─ Text:  D\n",
      "      └─ Text: _\n",
      "      └─ Text: c=1\n",
      "      └─ Text: ^C \n",
      "      └─ Text:  N (z_c(x) \n",
      "      └─ Text: _c, \n",
      "      └─ Text: _c^2),~~\n",
      "      └─ Text: with\n",
      "      └─ Text: \\\\\n",
      "      └─ Text: _c = 1 + \n",
      "      └─ Text: _\n",
      "      └─ Text:  I\n",
      "      └─ Text: [\n",
      "      └─ Text: c = y\n",
      "      └─ Text: ]\n",
      "      └─ Text: ,~~\n",
      "    \n",
      "      └─ Text: _c^2 = \n",
      "      └─ Text: (1 / \n",
      "      └─ Text: _c + 1),~~\n",
      "    \n",
      "      └─ Text: _c = \n",
      "      └─ Text: (\n",
      "      └─ Text: _c) - \n",
      "      └─ Text: ^2_c\n",
      "      └─ Text: 2\n",
      "      └─ Text: ,\n",
      "\n",
      "      └─ Text: \n",
      "where \n",
      "      └─ Text: I\n",
      "      └─ Text: [\n",
      "      └─ Text: c = y\n",
      "      └─ Text: ]\n",
      "      └─ Text:  is the indicator function equal to \n",
      "      └─ Text: 1\n",
      "      └─ Text:  for \n",
      "      └─ Text: c = y\n",
      "      └─ Text:  and \n",
      "      └─ Text: 0\n",
      "      └─ Text:  for \n",
      "      └─ Text: c \n",
      "      └─ Text:  y\n",
      "      └─ Text: .\n",
      "Here \n",
      "      └─ Text: NDG\n",
      "      └─ Text:  stands for \n",
      "      └─ Text: Noisy Dirichlet Gaussian\n",
      "      └─ Text:  approximation.\n",
      "While theoretically we do not need...\n",
      "      └─ Text: eq:posterior_dir_noisy_gaussian\n",
      "      └─ Text:  for Bayesian neural networks and can directly use...\n",
      "      └─ Text: eq:posterior_dir_noisy_final\n",
      "      └─ Text: , we found that in practice the approximation is m...\n",
      "      └─ Text: eq:posterior_dir_noisy_gaussian\n",
      "      └─ Text:  amounts to solving a regression problem in the sp...\n",
      "      └─ Text: z(x)\n",
      "      └─ Text:  with a Gaussian observation model.\n",
      "In the experim...\n",
      "      └─ Text: p_\n",
      "      └─ Text: NDG\n",
      "      └─ Text:  model.\n",
      "\n",
      "\n",
      "      └─ Text: !t\n",
      "      └─ Text: width=0.8\n",
      "      └─ Text: figures/coinflip_comparison.pdf\n",
      "      └─ Text: Comparison of Bayesian classification models.\n",
      "      └─ Text: \n",
      "    The probability CDF of the posterior over the...\n",
      "      └─ Text: f_y\n",
      "      └─ Text:  for the correct class \n",
      "      └─ Text: y\n",
      "      └─ Text:  with the standard cross entropy likelihood,\n",
      "    t...\n",
      "      └─ Text: Left\n",
      "      └─ Text: : Tempering and noisy Dirichlet models both allow ...\n",
      "      └─ Text: Middle\n",
      "      └─ Text: : Lower likelihood temperatures lead to higher con...\n",
      "      └─ Text: Right\n",
      "      └─ Text: : In the noisy Dirichlet model, lower values of th...\n",
      "      └─ Text: _\n",
      "      └─ Text:  also lead to higher confidence.\n",
      "    \n",
      "      └─ Text: fig:coinflip\n",
      "      └─ Text: Visual comparison.\n",
      "      └─ Text: \n",
      "In \n",
      "      └─ Text: fig:coinflip\n",
      "      └─ Text: (left), we show the CDF of the posterior distribut...\n",
      "      └─ Text: f_y\n",
      "      └─ Text:  for the correct class \n",
      "      └─ Text: y\n",
      "      └─ Text:  with the standard softmax likelihood, tempered li...\n",
      "      └─ Text: \\{\n",
      "      └─ Text: f(x)\n",
      "      └─ Text: \\}\n",
      "      └─ Text: _\n",
      "      └─ Text: x \n",
      "      └─ Text:  D\n",
      "      └─ Text:  is uniform and independent across \n",
      "      └─ Text: x\n",
      "      └─ Text:  for the purpose of visualization.\n",
      "Both tempering ...\n",
      "      └─ Text: f_y\n",
      "      └─ Text:  to be much more confident in the correct class \n",
      "      └─ Text: y\n",
      "      └─ Text:  compared to the standard softmax likelihood.\n",
      "In \n",
      "      └─ Text: fig:coinflip\n",
      "      └─ Text: (middle) we show the effect of the value of temper...\n",
      "      └─ Text: fig:coinflip\n",
      "      └─ Text: (right) lower values of \n",
      "      └─ Text: _\n",
      "      └─ Text:  lead to more confident predictions in the noisy D...\n",
      "  └─ section: The Effect of Data Augmentation\n",
      "    └─ Text: sec:data_aug_effects\n",
      "    └─ Text: \n",
      "Data augmentation is a key ingredient of modern d...\n",
      "    └─ Text: izmailov2021bayesian,zeno2020cold,fortuin2021bayes...\n",
      "    └─ Text: .\n",
      "We show that naive data augmentation, as typical...\n",
      "    └─ Text: T > 1\n",
      "    └─ Text: .\n",
      "In this case, the cold posteriors counterbalance...\n",
      "    └─ Text: true\n",
      "    └─ Text:  Bayesian neural networks.\n",
      "\n",
      "Consider  the interact...\n",
      "    └─ Text: eq:langevin_sde\n",
      "    └─ Text: . \n",
      "In prior work, e.g. in \n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text: , \n",
      "the stochastic gradient is estimated using a ra...\n",
      "    └─ Text: _m = \n",
      "    └─ Text: \\{\n",
      "    └─ Text: x_i,y_i\n",
      "    └─ Text: \\}\n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^m \n",
      "    └─ Text: , and a finite set of augmentations \n",
      "    └─ Text: T\n",
      "    └─ Text:  = \n",
      "    └─ Text: \\{\n",
      "    └─ Text:  t_1,\n",
      "    └─ Text: ,t_K \n",
      "    └─ Text: \\}\n",
      "    └─ Text: , \n",
      "this stochastic gradient is given by\n",
      "\n",
      "    └─ Text: U\n",
      "    └─ Text: (\n",
      "    └─ Text: ) =&~ \n",
      "    └─ Text: N\n",
      "    └─ Text: m\n",
      "    └─ Text: _\n",
      "    └─ Text: (x_i,y_i) \n",
      "    └─ Text: _m\n",
      "    └─ Text: _\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_j(x_i))\n",
      "    └─ Text: \n",
      "+ \n",
      "    └─ Text: _\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: ,\n",
      "\n",
      "    └─ Text: eq:stochastic_gradient\n",
      "    └─ Text: \n",
      "where the transformations \n",
      "    └─ Text: t_j\n",
      "    └─ Text:  are sampled uniformly from \n",
      "    └─ Text: T\n",
      "    └─ Text: .\n",
      "\n",
      "There are two sources of randomness in \n",
      "    └─ Text: U\n",
      "    └─ Text: (\n",
      "    └─ Text: )\n",
      "    └─ Text:  --- the choice of the mini-batch \n",
      "    └─ Text: _m\n",
      "    └─ Text:  and the choice of augmentation \n",
      "    └─ Text: t_j\n",
      "    └─ Text:  used for each \n",
      "    └─ Text: x_i\n",
      "    └─ Text: . The limiting distribution of SGLD is determined ...\n",
      "    └─ Text: eq:stochastic_gradient\n",
      "    └─ Text: , which is given by\n",
      "\n",
      "    └─ Text: E\n",
      "    └─ Text: U\n",
      "    └─ Text: (\n",
      "    └─ Text: )\n",
      "    └─ Text: &= \n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N \n",
      "    └─ Text: E\n",
      "    └─ Text: _\n",
      "    └─ Text: t_j \n",
      "    └─ Text: T\n",
      "    └─ Text: [\n",
      "    └─ Text: _\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_j(x_i))\n",
      "    └─ Text: ]\n",
      "    └─ Text:  + \n",
      "    └─ Text: _\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: , \n",
      "    └─ Text: \\\\\n",
      "    └─ Text: &= \n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N \n",
      "    └─ Text: _\n",
      "    └─ Text: j=1\n",
      "    └─ Text: ^K \n",
      "    └─ Text: [\n",
      "    └─ Text: _\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_j(x_i))^\n",
      "    └─ Text: 1/K\n",
      "    └─ Text: ]\n",
      "    └─ Text:  + \n",
      "    └─ Text: _\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: ,\n",
      "\n",
      "    └─ Text: eq:aug_sgld_grad_expectation\n",
      "    └─ Text: \n",
      "where \n",
      "    └─ Text: t_j \n",
      "    └─ Text: T\n",
      "    └─ Text:  are augmentations sampled uniformly from \n",
      "    └─ Text: T\n",
      "    └─ Text: .\n",
      "Therefore, we conclude that the limiting distrib...\n",
      "    └─ Text: \n",
      "p_\n",
      "    └─ Text: aug\n",
      "    └─ Text: (\n",
      "    └─ Text: ) \n",
      "    └─ Text:  p(\n",
      "    └─ Text: )\n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N\n",
      "    └─ Text: _\n",
      "    └─ Text: j=1\n",
      "    └─ Text: ^K p(y_i \n",
      "    └─ Text:  t_j(x_i))^\n",
      "    └─ Text: 1/K\n",
      "    └─ Text: .\n",
      "\n",
      "    └─ Text: eq:implied_posterior\n",
      "    └─ Text: \n",
      "We can interpret the limiting distribution in \n",
      "    └─ Text: eq:implied_posterior\n",
      "    └─ Text:  as a \n",
      "    └─ Text: tempered likelihood posterior\n",
      "    └─ Text:  for a new dataset \n",
      "    └─ Text: ^\n",
      "    └─ Text:  = \n",
      "    └─ Text: \\{\n",
      "    └─ Text: (t_j(x_i), y_i)\n",
      "    └─ Text: \\}\n",
      "    └─ Text: _\n",
      "    └─ Text: i, j\n",
      "    └─ Text:  which contains all augmentations of every data po...\n",
      "    └─ Text: .\n",
      "Furthermore, the likelihood is tempered with a t...\n",
      "    └─ Text: K > 1\n",
      "    └─ Text: , corresponding to a \n",
      "    └─ Text: warm posterior\n",
      "    └─ Text: .\n",
      "In other words, the limiting distribution of SGL...\n",
      "    └─ Text: softened\n",
      "    └─ Text:  likelihood, leading to less confidence about the ...\n",
      "    └─ Text: T = 1 / K\n",
      "    └─ Text:  to the posterior in \n",
      "    └─ Text: eq:implied_posterior\n",
      "    └─ Text: , we can recover the standard Bayesian posterior o...\n",
      "    └─ Text: ^\n",
      "    └─ Text: .\n",
      "\n",
      "In \n",
      "    └─ Text: sec:exp_bnn_images\n",
      "    └─ Text: , we explore the effect of data augmentation on al...\n",
      "    └─ Text: What about Other Inference Procedures?\n",
      "    └─ Text: \n",
      "While the derivation in \n",
      "    └─ Text: eq:implied_posterior\n",
      "    └─ Text:  comes directly from studying the stochastic gradi...\n",
      "    └─ Text: app:alt_inferences\n",
      "    └─ Text: . \n",
      "\n",
      "\n",
      "    └─ Text: Should we always use temperature \n",
      "    └─ Text: T=1/K\n",
      "    └─ Text: ?\n",
      "    └─ Text: \n",
      "While using the temperature \n",
      "    └─ Text: T = 1 / K\n",
      "    └─ Text:  recovers the standard Bayesian posterior on the a...\n",
      "    └─ Text: '\n",
      "    └─ Text: , it is not necessarily a correct approach to mode...\n",
      "    └─ Text: p(\n",
      "    └─ Text: ') \n",
      "    └─ Text:  p(\n",
      "    └─ Text: )\n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N\n",
      "    └─ Text: _\n",
      "    └─ Text: j=1\n",
      "    └─ Text: ^K p(y_i \n",
      "    └─ Text:  t_j(x_i))\n",
      "    └─ Text:  assumes indepdendence across both augmentations a...\n",
      "    └─ Text: y_i\n",
      "    └─ Text:  across all the augmented versions of the image; t...\n",
      "    └─ Text: underestimating\n",
      "    └─ Text:  the aleatoric uncertainty.\n",
      "Indeed, consider the e...\n",
      "    └─ Text: t_j(x)\n",
      "    └─ Text:  simply return \n",
      "    └─ Text: x\n",
      "    └─ Text: .\n",
      "In this case, treating the observations \n",
      "    └─ Text: (t_j(x), y)\n",
      "    └─ Text:  as independent will simply raise the likelihood t...\n",
      "    └─ Text: K\n",
      "    └─ Text: , while in reality we have not received any additi...\n",
      "    └─ Text: 1/K\n",
      "    └─ Text:  is the correct approach only when the predictions...\n",
      "    └─ Text: t_j(x)\n",
      "    └─ Text:  are completely independent from each other (see \n",
      "    └─ Text: sec:data_aug_gp\n",
      "    └─ Text: ).\n",
      "See also \n",
      "    └─ Text: nabarro2021data\n",
      "    └─ Text:  for a related discussion.\n",
      "\n",
      "\n",
      "    └─ Text: A Proper Augmentation Likelihood.\n",
      "    └─ Text: \n",
      "As we have noted, simple tempering does not model...\n",
      "    └─ Text: sec:app_aug_lik\n",
      "    └─ Text: , we develop a proper likelihood that correctly ac...\n",
      "    └─ Text: nabarro2021data\n",
      "    └─ Text:  also derived a valid likelihood for data augmenta...\n",
      "  └─ section: Experiments\n",
      "    └─ Text: sec:experiments\n",
      "    └─ Text: \n",
      "In this section we provide empirical support for ...\n",
      "    └─ Text: sec:aleatoric_uncertainty_understanding,sec:data_a...\n",
      "    └─ Text: .\n",
      "First, in \n",
      "    └─ Text: sec:exp_synthetic\n",
      "    └─ Text:  we illustrate the effects of tempering, noisy Dir...\n",
      "    └─ Text: sec:data_aug_gp\n",
      "    └─ Text:  we visualize the effect of data augmentation on t...\n",
      "    └─ Text: sec:exp_bnn_images\n",
      "    └─ Text:  we report the results for BNNs on image classific...\n",
      "    └─ Text: !t\n",
      "    └─ Text: 0.24\n",
      "    └─ Text: font=small\n",
      "    └─ Text: height=1.1\n",
      "    └─ Text: figures/fig1_ce_noaug_fit.pdf\n",
      "    └─ Text: Softmax Lik. (SL)\n",
      "    └─ Text: fig:da_crossent\n",
      "    └─ Text: % \t\\hspace{0.1cm}\n",
      "    └─ Text: 0.24\n",
      "    └─ Text: font=small\n",
      "    └─ Text: height=1.1\n",
      "    └─ Text: figures/fig1_ce_aug_fit.pdf\n",
      "    └─ Text: SL+Data Aug.(DA)\n",
      "    └─ Text: fig:da_crossent_da\n",
      "    └─ Text: % \t\\hspace{0.1cm}\n",
      "    └─ Text: 0.24\n",
      "    └─ Text: font=small\n",
      "    └─ Text: height=1.1\n",
      "    └─ Text: figures/fig1_ce_aug_cold_fit.pdf\n",
      "    └─ Text: SL+DA+Tempering\n",
      "    └─ Text: fig:da_crossent_da_tempering\n",
      "    └─ Text: % \t\\hspace{0.1cm}\n",
      "    └─ Text: 0.24\n",
      "    └─ Text: font=small\n",
      "    └─ Text: height=1.1\n",
      "    └─ Text: figures/fig1_dirichlet_aug_fit.pdf\n",
      "    └─ Text: Noisy Dirichlet+DA\n",
      "    └─ Text: fig:da_dirichlet_da\n",
      "    └─ Text: Insufficient Sharpness of Softmax Likelihood.\n",
      "    └─ Text: \n",
      "\t\tDecision boundary of BNN classifiers on a synth...\n",
      "    └─ Text: (a)\n",
      "    └─ Text:  A standard softmax likelihood provides a reasonab...\n",
      "    └─ Text: (b)\n",
      "    └─ Text:  Adding data augmentation leads to a more diffuse ...\n",
      "    └─ Text: (c)\n",
      "    └─ Text:  Tempering with \n",
      "    └─ Text: T=0.1\n",
      "    └─ Text:  sharpens the softmax likelihood leading to a perf...\n",
      "    └─ Text: (d)\n",
      "    └─ Text:  A noisy Dirichlet model, provides a similar fit w...\n",
      "    └─ Text: fig:aleatoric_pitfalls\n",
      "    └─ subsection: Synthetic problem\n",
      "      └─ Text: sec:exp_synthetic\n",
      "      └─ Text: \n",
      "We generate the data from a modified version of t...\n",
      "      └─ Text: see e.g.\n",
      "      └─ Text: maddox2020rethinking\n",
      "      └─ Text: , where we restrict the data to the \n",
      "      └─ Text: x_1 <0, x_2 <0\n",
      "      └─ Text:  quadrant.\n",
      "The exact code used to generate the dat...\n",
      "      └─ Text: sec:app_exp_synthetic\n",
      "      └─ Text: .\n",
      "\n",
      "In \n",
      "      └─ Text: fig:aleatoric_pitfalls\n",
      "      └─ Text: (a) we visualize the data and the decision boundar...\n",
      "      └─ Text:  N(0, 0.3^2)\n",
      "      └─ Text:  over the parameters of the model;\n",
      "we use full bat...\n",
      "      └─ Text: izmailov2021bayesian\n",
      "      └─ Text: .\n",
      "With the chosen prior, the model is not able to ...\n",
      "      └─ Text: fig:aleatoric_pitfalls\n",
      "      └─ Text: (b) we consider the effect of data augmentation.\n",
      "S...\n",
      "      └─ Text: x_1\n",
      "      └─ Text:  and \n",
      "      └─ Text: x_2\n",
      "      └─ Text:  axes.\n",
      "In the figure, the augmented datapoints are...\n",
      "      └─ Text: eq:implied_posterior\n",
      "      └─ Text: .\n",
      "The Bayesian neural network provides a lower qua...\n",
      "      └─ Text: fig:aleatoric_pitfalls\n",
      "      └─ Text: (a).\n",
      "Indeed, according to our analysis in \n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text: , the observation model is softened by the data au...\n",
      "      └─ Text: sec:aleatoric_uncertainty_understanding\n",
      "      └─ Text: , we can sharpen the model leading to a much bette...\n",
      "      └─ Text: fig:aleatoric_pitfalls\n",
      "      └─ Text: (c) or the noisy Dirichlet observation model as in...\n",
      "      └─ Text: fig:aleatoric_pitfalls\n",
      "      └─ Text: (d).\n",
      "With both of these approaches we achieve a ne...\n",
      "      └─ Text: sec:app_exp_synthetic\n",
      "      └─ Text: .\n",
      "\n",
      "\n",
      "    └─ subsection: Visualizing the Effect of Data Augmentation\n",
      "      └─ Text: sec:data_aug_gp\n",
      "      └─ Text: !t\n",
      "      └─ Text: width=.9\n",
      "      └─ Text: figures/augmentation_gp_v2\n",
      "      └─ Text: Effect of data augmentation on GP regression.\n",
      "      └─ Text: \n",
      "    As we increase the number \n",
      "      └─ Text: K\n",
      "      └─ Text:  of augmentations of the dataset, a GP regression ...\n",
      "      └─ Text: t_j(x) = x + \n",
      "      └─ Text: _j\n",
      "      └─ Text:  that are obtained by shifting the inputs \n",
      "      └─ Text: x\n",
      "      └─ Text:  by a large constant \n",
      "      └─ Text: _j\n",
      "      └─ Text:  are not shown.\n",
      "    Note that both the predictive ...\n",
      "      └─ Text: K\n",
      "      └─ Text:  of augmentations.\n",
      "    \n",
      "      └─ Text: fig:augmentation_shift_gp\n",
      "      └─ Text: \n",
      "To illustrate our analysis in \n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text:  we visualize the posterior in \n",
      "      └─ Text: eq:implied_posterior\n",
      "      └─ Text:  under data augmentation in a Gaussian process reg...\n",
      "      └─ Text: GP\n",
      "      └─ Text: rasmussen2006gaussian\n",
      "      └─ Text:  with a truncated RBF kernel such that \n",
      "      └─ Text: k(x,x^\n",
      "      └─ Text: ) = 0\n",
      "      └─ Text:  when \n",
      "      └─ Text: x-x^\n",
      "      └─ Text:  > \n",
      "      └─ Text: , where \n",
      "      └─ Text:  is the lengthscale of the kernel.\n",
      "Consequently, t...\n",
      "      └─ Text:  are zero.\n",
      "We use an augmentation policy such that...\n",
      "      └─ Text: t_j(x) = x + \n",
      "      └─ Text: _j\n",
      "      └─ Text: , for some set of vectors \n",
      "      └─ Text: \\{\n",
      "      └─ Text: _j\n",
      "      └─ Text: \\}\n",
      "      └─ Text: _\n",
      "      └─ Text: j=1\n",
      "      └─ Text: ^K\n",
      "      └─ Text:  such that  \n",
      "      └─ Text: _j\n",
      "      └─ Text: .\n",
      "Therefore we assume that \n",
      "      └─ Text: k(x, t_j(x')) = 0\n",
      "      └─ Text:  for all datapoints \n",
      "      └─ Text: x, x'\n",
      "      └─ Text:  and all augmentations \n",
      "      └─ Text: t_j\n",
      "      └─ Text: .\n",
      "In other words, the outputs of the Gaussian proc...\n",
      "      └─ Text: eq:implied_posterior\n",
      "      └─ Text:  for \n",
      "      └─ Text: K=1, 4\n",
      "      └─ Text:  and \n",
      "      └─ Text: 10\n",
      "      └─ Text:  augmentations per datapoint in \n",
      "      └─ Text: fig:augmentation_shift_gp\n",
      "      └─ Text: .\n",
      "In the visualization, we show the posterior just...\n",
      "      └─ Text: eq:implied_posterior\n",
      "      └─ Text:  corresponds to tempering the posterior on the ori...\n",
      "      └─ Text: K\n",
      "      └─ Text:  of augmentations.\n",
      "As a result, increasing the num...\n",
      "    └─ subsection: Image Classification with Bayesian Neural Networks\n",
      "      └─ Text: sec:exp_bnn_images\n",
      "      └─ Text: \n",
      "In this section we experimentally verify the resu...\n",
      "      └─ Text: sec:aleatoric_uncertainty_understanding,sec:data_a...\n",
      "      └─ Text:  for Bayesian neural networks in image classificat...\n",
      "      └─ Text: he2016identity\n",
      "      └─ Text:  and the CIFAR-10 \n",
      "      └─ Text: Krizhevsky2009LearningML\n",
      "      └─ Text:  and Tiny Imagenet \n",
      "      └─ Text: Le2015TinyIV\n",
      "      └─ Text:  datasets.\n",
      "We use the SGLD sampler with a cyclical...\n",
      "      └─ Text: welling2011bayesian, zhang2019cyclical\n",
      "      └─ Text:  to sample from the posterior.\n",
      "We provide details ...\n",
      "      └─ Text: app:experimental_details\n",
      "      └─ Text: .\n",
      "\n",
      "\n",
      "\n",
      "      └─ Text: R\n",
      "      └─ Text: .4\n",
      "      └─ Text: -1em\n",
      "      └─ Text: width=.9\n",
      "      └─ Text: figures/reduced_cpe.pdf\n",
      "      └─ Text: \n",
      "    BMA test accuracy for the noisy Dirichlet mod...\n",
      "      └─ Text: _\n",
      "      └─ Text:  = 10^\n",
      "      └─ Text: -6\n",
      "      └─ Text:  and the softmax likelihood\n",
      "    as a function of p...\n",
      "      └─ Text: fig:no_cpe\n",
      "      └─ Text: No cold posterior effect in the noisy Dirichlet mo...\n",
      "      └─ Text: \n",
      "First, we explore the effect of posterior temperi...\n",
      "      └─ Text: _\n",
      "      └─ Text: =10^\n",
      "      └─ Text: -6\n",
      "      └─ Text: .\n",
      "In \n",
      "      └─ Text: fig:no_cpe\n",
      "      └─ Text:  we show the results on the CIFAR-10 dataset.\n",
      "As r...\n",
      "      └─ Text: wenzel2020good\n",
      "      └─ Text: , for the standard softmax likelihood, tempering i...\n",
      "      └─ Text: T=10^\n",
      "      └─ Text: -3\n",
      "      └─ Text:  providing the best results.\n",
      "For the noisy Dirichl...\n",
      "      └─ Text: 1\n",
      "      └─ Text: !\n",
      "These results agree with our analysis in \n",
      "      └─ Text: sec:aleatoric_uncertainty_understanding\n",
      "      └─ Text: : both tempering and the noisy Dirichlet observati...\n",
      "      └─ Text: !ht\n",
      "      └─ Text: cc\n",
      "      └─ Text: width=.47\n",
      "      └─ Text: figures/c10_label_noise_acc_nll\n",
      "      └─ Text: &\n",
      "    \n",
      "      └─ Text: width=.47\n",
      "      └─ Text: figures/ti_label_noise_acc_nll\n",
      "      └─ Text: \\\\\n",
      "      └─ Text: (a) CIFAR-10 & (b) Tiny Imagenet\n",
      "\n",
      "      └─ Text: Label noise in BNN image classification.\n",
      "      └─ Text: \n",
      "BMA test accuracy and negative log likelihood for...\n",
      "      └─ Text: (a)\n",
      "      └─ Text:  on CIFAR-10 and \n",
      "      └─ Text: (b)\n",
      "      └─ Text:  Tiny Imagenet.\n",
      "Accounting for the label noise via...\n",
      "      └─ Text: T^\n",
      "      └─ Text: *\n",
      "      └─ Text:  in the tempered softmax likelihood or noise \n",
      "      └─ Text: _\n",
      "      └─ Text: ^\n",
      "      └─ Text: *\n",
      "      └─ Text:  parameter in the noisy Dirichlet model, i.e.\n",
      "no o...\n",
      "      └─ Text: fig:noisy_dirichlet_perf_cifar_labelnoise\n",
      "      └─ Text: Modeling label noise.\n",
      "      └─ Text: \n",
      "In classification problems the aleatoric uncertai...\n",
      "      └─ Text: fig:noisy_dirichlet_perf_cifar_labelnoise\n",
      "      └─ Text: , we show the BMA test accuracy and negative log-l...\n",
      "      └─ Text: T \n",
      "      └─ Text: \\{\n",
      "      └─ Text: 10^\n",
      "      └─ Text: -5\n",
      "      └─ Text: ,10^\n",
      "      └─ Text: -4\n",
      "      └─ Text: ,10^\n",
      "      └─ Text: -3\n",
      "      └─ Text: ,10^\n",
      "      └─ Text: -2\n",
      "      └─ Text: ,10^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ,1,3,10\n",
      "      └─ Text: \\}\n",
      "      └─ Text:  for the standard softmax likelihood or noise \n",
      "      └─ Text: _\n",
      "      └─ Text: \\{\n",
      "      └─ Text:  10^\n",
      "      └─ Text: -6\n",
      "      └─ Text: , 10^\n",
      "      └─ Text: -5\n",
      "      └─ Text: , 10^\n",
      "      └─ Text: -4\n",
      "      └─ Text: , 10^\n",
      "      └─ Text: -3\n",
      "      └─ Text: , 10^\n",
      "      └─ Text: -2\n",
      "      └─ Text: , 10^\n",
      "      └─ Text: -1\n",
      "      └─ Text: \\}\n",
      "      └─ Text:  for the noisy Dirichlet model.\n",
      "Both on CIFAR-10 a...\n",
      "      └─ Text: _\n",
      "      └─ Text:  parameter.\n",
      "\n",
      "In \n",
      "      └─ Text: fig:noisy_dirichlet_perf_cifar_labelnoise\n",
      "      └─ Text: (b), for Tiny Imagenet the noisy Dirichlet model p...\n",
      "      └─ Text: Data augmentation leads to underfitting on train d...\n",
      "      └─ Text: \n",
      "Next, we verify the results from \n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text:  for Bayesian neural networks in image classificat...\n",
      "      └─ Text: fig:train_diffuse_aug_lik\n",
      "      └─ Text: (a,b) we show the BMA train negative log-likelihoo...\n",
      "      └─ Text: _\n",
      "      └─ Text:  parameters for the noisy Dirichlet model,\n",
      "adding ...\n",
      "      └─ Text: sec:data_aug_gp\n",
      "      └─ Text:  and agrees with our analysis in \n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text: : data augmentation softens the likelihood, leadin...\n",
      "      └─ Text: Complex augmentations require lower temperatures.\n",
      "      └─ Text: \n",
      "Finally, we explore the effect of the number \n",
      "      └─ Text: K\n",
      "      └─ Text:  of data augmentations applied to each datapoint.\n",
      "...\n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text:  we showed that under data augmentation the likeli...\n",
      "      └─ Text: K\n",
      "      └─ Text: , assuming the predictions on the augmentated data...\n",
      "      └─ Text: fig:train_diffuse_aug_lik\n",
      "      └─ Text: (c), we consider five separate types of augmentati...\n",
      "      └─ Text: 18\n",
      "      └─ Text:  on CIFAR-\n",
      "      └─ Text: 10\n",
      "      └─ Text:  at various posterior temperatures:\n",
      "horizontal and...\n",
      "      └─ Text: hendrycks2019augmix\n",
      "      └─ Text:  --- an augmentation policy employing a very diver...\n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text: , the optimal temperatures are different for diffe...\n",
      "      └─ Text: K = 2\n",
      "      └─ Text:  work best at warmer temperatures \n",
      "      └─ Text: T=1\n",
      "      └─ Text: , \n",
      "intermediate policies (crops and crops\n",
      "      └─ Text: +\n",
      "      └─ Text: flips) corresponding to \n",
      "      └─ Text: K \n",
      "      └─ Text:  100\n",
      "      └─ Text:  require lower temperatures \n",
      "      └─ Text: T \n",
      "      └─ Text: [\n",
      "      └─ Text: 10^\n",
      "      └─ Text: -2\n",
      "      └─ Text: , 10^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ]\n",
      "      └─ Text: ,\n",
      "and the most complex AugMix policy requires the ...\n",
      "      └─ Text: T \n",
      "      └─ Text:  10^\n",
      "      └─ Text: -4\n",
      "      └─ Text: .\n",
      "\n",
      "\n",
      "      └─ Text: !t\n",
      "      └─ Text: cc\n",
      "      └─ Text: width=0.48\n",
      "      └─ Text: figures/aug_noaug_train_lik\n",
      "      └─ Text: &\n",
      "    \n",
      "      └─ Text: width=0.49\n",
      "      └─ Text: figures/dataaug_counts\n",
      "      └─ Text: \\\\\n",
      "      └─ Text: (a) Softmax \n",
      "      └─ Text: \\&\n",
      "      └─ Text:  Noisy Dirichlet & (b) Augmentations vs Tempering\n",
      "\n",
      "      └─ Text: Effect of data augmentation on BNN image classific...\n",
      "      └─ Text: \n",
      "For all plots we use ResNet-18 on the CIFAR-10 da...\n",
      "      └─ Text: (a)\n",
      "      └─ Text: : The BMA negative log-likelihood on the train dat...\n",
      "      └─ Text: T\n",
      "      └─ Text:  and the noisy Dirichlet model across different va...\n",
      "      └─ Text: _\n",
      "      └─ Text: , both with and without data augmentation.\n",
      "As pred...\n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text: , data augmentation softens the likelihood, leadin...\n",
      "      └─ Text: _\n",
      "      └─ Text:  is needed to counteract the effect of data augmen...\n",
      "      └─ Text: (b)\n",
      "      └─ Text:  BMA test accuracy and NLL for various augmentatio...\n",
      "      └─ Text: sec:data_aug_effects\n",
      "      └─ Text: , more complex policies corresponding to a higher ...\n",
      "      └─ Text: K\n",
      "      └─ Text:  require lower temperatures for optimal performanc...\n",
      "      └─ Text: fig:train_diffuse_aug_lik\n",
      "  └─ section: Discussion\n",
      "    └─ Text: \n",
      "A correct representation of aletoric uncertainty ...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text: : Data augmentation is sufficient but not necessar...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text:  confirm the findings in \n",
      "    └─ Text: izmailov2021bayesian\n",
      "    └─ Text:  and \n",
      "    └─ Text: fortuin2021bayesian\n",
      "    └─ Text:  that data augmentation plays a significant role i...\n",
      "    └─ Text: izmailov2021bayesian\n",
      "    └─ Text:  show that removing data augmentation from the mod...\n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text:  alleviates the cold posterior effect in all of th...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text:  also show that it is sometimes possible to achiev...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text:  note that many types of misspecification could le...\n",
      "    └─ Text: largely\n",
      "    └─ Text:  responsible for the cold posterior effect, and we...\n",
      "    └─ Text: izmailov2021dangers\n",
      "    └─ Text:  show that BNNs have issues under covariate shift,...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text: . \n",
      "\n",
      "\n",
      "    └─ Text: nabarro2021data\n",
      "    └─ Text: : the lack of a CPE without DA is an artifact of u...\n",
      "    └─ Text: \n",
      "Similar to our discussion in \n",
      "    └─ Text: sec:data_aug_effects\n",
      "    └─ Text: , \n",
      "    └─ Text: nabarro2021data\n",
      "    └─ Text:  point out that tempering does not always provide ...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text: : Cold posteriors are unlikely to arise from a sin...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text:  show examples, where the cold posterior effect ar...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text: , we note that all of the considered causes are di...\n",
      "    └─ Text: Noci2021DisentanglingTR\n",
      "    └─ Text: .\n",
      "\n",
      "\n",
      "    └─ Text: Summary.\n",
      "    └─ Text:  Overall, properly representing aleatoric uncertai...\n",
      "    └─ Text: wilson2020bayesian\n",
      "    └─ Text: , we should not be alarmed if \n",
      "    └─ Text: T=1\n",
      "    └─ Text:  is not \n",
      "optimal in sophisticated models on comple...\n",
      "    └─ Text: are\n",
      "    └─ Text:  practical challenges to the adoption of Bayesian ...\n",
      "    └─ Text: izmailov2021dangers\n",
      "    └─ Text:  shows that Bayesian neural networks can profoundl...\n",
      "    └─ Text: EmpCov\n",
      "    └─ Text:  prior provides a partial remedy, there is still m...\n",
      "    └─ Text: Acknowledgments.\n",
      "    └─ Text:  \n",
      "We would like to thank Micah Goldblum and Wanqia...\n",
      "    └─ Text: -5mm\n",
      "    └─ Text: references\n",
      "  └─ section: Detailed Description of the Synthetic Experiment\n",
      "    └─ Text: sec:app_toy_exp\n",
      "    └─ Text: \n",
      "To intuitively grasp the impact of aleatoric unce...\n",
      "    └─ Text: two spirals\n",
      "    └─ Text:  binary classification problem \n",
      "    └─ Text: huang2020understanding\n",
      "    └─ Text:  where,\n",
      "for training we generate \n",
      "    └─ Text: 50\n",
      "    └─ Text:  samples with both \n",
      "    └─ Text: x_1,x_2 < 0\n",
      "    └─ Text: .\n",
      "\n",
      "    └─ Text: fig:aleatoric_pitfalls\n",
      "    └─ Text:  visualizes the posterior predictive density for v...\n",
      "    └─ Text: fig:da_crossent\n",
      "    └─ Text: . But, by virtue of our a priori knowledge\n",
      "about t...\n",
      "    └─ Text: (x,y)\n",
      "    └─ Text:  pair in the training data, we create augmentation...\n",
      "    └─ Text: (\n",
      "    └─ Text:  x, \n",
      "    └─ Text:  y)\n",
      "    └─ Text:  for use in training. With this modification, we f...\n",
      "    └─ Text: fig:da_crossent_da\n",
      "    └─ Text:  appears to improve but remains very diffuse, impl...\n",
      "    └─ Text: fig:da_crossent_da_tempering\n",
      "    └─ Text:  we see that\n",
      "tempering allows us to sharpen the so...\n",
      "    └─ Text: fig:da_dirichlet_da\n",
      "    └─ Text: .\n",
      "\n",
      "This toy illustration is representative of how ...\n",
      "  └─ section: Sharpening with the Smoothed Softmax Likelihood\n",
      "    └─ Text: sec:sharp_logits\n",
      "    └─ Text: \n",
      "A natural approach to consider sharpening the sof...\n",
      "    └─ Text: tempered softmax likelihood\n",
      "    └─ Text:  for a given class observation \n",
      "    └─ Text: y=c\n",
      "    └─ Text:  \n",
      "conditional on input \n",
      "    └─ Text: x\n",
      "    └─ Text:  is given by,\n",
      "\n",
      "    └─ Text: 1\n",
      "    └─ Text: T\n",
      "    └─ Text: p(y = c \n",
      "    └─ Text:  x)\n",
      "    └─ Text:  = \n",
      "    └─ Text: \\{\n",
      "    └─ Text:  f_c(x; \n",
      "    └─ Text: ) / T\n",
      "    └─ Text: \\}\n",
      "    └─ Text: (\n",
      "    └─ Text: _\n",
      "    └─ Text: j=1\n",
      "    └─ Text: ^C \n",
      "    └─ Text: \\{\n",
      "    └─ Text: f_j(x; \n",
      "    └─ Text: )\n",
      "    └─ Text: \\}\n",
      "    └─ Text: )^\n",
      "    └─ Text: 1/T\n",
      "    └─ Text: eq:tempered_softmax\n",
      "    └─ Text: \n",
      "where \n",
      "    └─ Text: f_j(x; \n",
      "    └─ Text: ) \n",
      "    └─ Text:  represents the logit for the \n",
      "    └─ Text: j^\n",
      "    └─ Text: th\n",
      "    └─ Text:  class of a total of \n",
      "    └─ Text: C\n",
      "    └─ Text:  classes. A\n",
      "properly normalized distribution, howe...\n",
      "    └─ Text: smoothed softmax likelihood\n",
      "    └─ Text: ,\n",
      "\n",
      "    └─ Text: p_\n",
      "    └─ Text: T\n",
      "    └─ Text: (y = c \n",
      "    └─ Text:  x)\n",
      "    └─ Text:  =  \n",
      "    └─ Text: \\{\n",
      "    └─ Text:  f_c(x; \n",
      "    └─ Text: ) / T\n",
      "    └─ Text: \\}\n",
      "    └─ Text: _\n",
      "    └─ Text: j=1\n",
      "    └─ Text: ^C \n",
      "    └─ Text: \\{\n",
      "    └─ Text: f_j(x;\n",
      "    └─ Text: )/T\n",
      "    └─ Text: \\}\n",
      "    └─ Text: . \n",
      "    └─ Text: eq:smoothed_softmax\n",
      "    └─ Text: \n",
      "Versions of this likelihood have appeared in prio...\n",
      "    └─ Text: hinton2015distilling\n",
      "    └─ Text:  and temperature (e.g. Platt) scaling \n",
      "    └─ Text: platt1999probabilistic,guo2017calibration\n",
      "    └─ Text: . \n",
      "More recently, \n",
      "    └─ Text: zeno2020cold\n",
      "    └─ Text:  have pointed out a connection between \n",
      "    └─ Text: eq:smoothed_softmax\n",
      "    └─ Text:  and the cold posteriors \n",
      "    └─ Text: eq:cold_posterior\n",
      "    └─ Text:  studied by \n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text: .\n",
      "\n",
      "\n",
      "    └─ Text: R\n",
      "    └─ Text: .333\n",
      "    └─ Text: width=\n",
      "    └─ Text: figures/fig2_combined.pdf\n",
      "    └─ Text: A smoothed likelihood \n",
      "    └─ Text: eq:smoothed_softmax\n",
      "    └─ Text:  is insufficient to achieve arbitrary sharpness (\n",
      "    └─ Text: sec:sharp_logits\n",
      "    └─ Text: ). An arbitrarily sharp likelihood would concentra...\n",
      "    └─ Text:  rather than being effectively uniform over ranges...\n",
      "    └─ Text: fig:logit_scaling\n",
      "    └─ Text: -1.5em\n",
      "    └─ paragraph: Insufficiency of Smoothed Softmax\n",
      "      └─ Text: \n",
      "Consider a toy coinflip example --- we are intere...\n",
      "      └─ Text: [\n",
      "      └─ Text: 0,1\n",
      "      └─ Text: ]\n",
      "      └─ Text:  using a single observation of\n",
      "a heads \n",
      "      └─ Text:  = \n",
      "      └─ Text: \\{\n",
      "      └─ Text: H\n",
      "      └─ Text: \\}\n",
      "      └─ Text: . The posterior over \n",
      "      └─ Text: , under the assumption of a uniform prior \n",
      "      └─ Text: p(\n",
      "      └─ Text: ) = \n",
      "      └─ Text: U\n",
      "      └─ Text: 0,1\n",
      "      └─ Text:  is given by \n",
      "      └─ Text: p(\n",
      "      └─ Text: ) \n",
      "      └─ Text:  p(H \n",
      "      └─ Text: ) = \n",
      "      └─ Text: . The smoothed softmax likelihood corresponds to r...\n",
      "      └─ Text: (\n",
      "      └─ Text: ) = \n",
      "      └─ Text: \\{\n",
      "      └─ Text: /(1-\n",
      "      └─ Text: )\n",
      "      └─ Text: \\}\n",
      "      └─ Text:  as \n",
      "      └─ Text: p_T(H \n",
      "      └─ Text: ) = \n",
      "      └─ Text: (\n",
      "      └─ Text: (\n",
      "      └─ Text: ) / T)\n",
      "      └─ Text: , where \n",
      "      └─ Text: (z) = 1/(1+\n",
      "      └─ Text: \\{\n",
      "      └─ Text: -z\n",
      "      └─ Text: \\}\n",
      "      └─ Text: )\n",
      "      └─ Text:  is the sigmoid function  \n",
      "      └─ Text: Note here that \n",
      "      └─ Text: (\n",
      "      └─ Text: (\n",
      "      └─ Text: )) = \n",
      "      └─ Text: .\n",
      "\n",
      "Using the tempered softmax likelihood by raisin...\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  where \n",
      "      └─ Text: T<1\n",
      "      └─ Text: , we\n",
      "effectively sharpen the density function and ...\n",
      "      └─ Text: 1\n",
      "      └─ Text:  owing to the single observation. On the other han...\n",
      "      └─ Text: fig:logit_scaling\n",
      "      └─ Text: . In fact, no matter what temperature we use for a...\n",
      "      └─ Text: fig:smooth_vs_tempered_softmax\n",
      "      └─ Text: , we empirically verify with ResNet-\n",
      "      └─ Text: 18\n",
      "      └─ Text:  on CIFAR-\n",
      "      └─ Text: 10\n",
      "      └─ Text:  that no matter the value of temperature \n",
      "      └─ Text: T\n",
      "      └─ Text: , a smoothed softmax likelihood does not\n",
      "outperfor...\n",
      "  └─ section: Connecting Cold Posteriors and Tempered Likelihood Posteriors\n",
      "    └─ Text: sec:conn_cold_tempered\n",
      "    └─ subsection: Connections in Bayesian Linear Models\n",
      "      └─ Text: sec:conn_lin_reg\n",
      "      └─ Text: \n",
      "We begin by taking a look at the connection betwe...\n",
      "      └─ Text:  = \n",
      "      └─ Text: \\{\n",
      "      └─ Text: X,y\n",
      "      └─ Text: \\}\n",
      "      └─ Text:  consisting of \n",
      "      └─ Text: d\n",
      "      └─ Text: -dimensional inputs \n",
      "      └─ Text: X = \n",
      "      └─ Text: [\n",
      "      └─ Text: x_1;x_2;\n",
      "      └─ Text: ;x_N\n",
      "      └─ Text: ]\n",
      "      └─ Text: ^\n",
      "      └─ Text: N \n",
      "      └─ Text:  d\n",
      "      └─ Text: , corresponding \n",
      "outputs \n",
      "      └─ Text: y \n",
      "      └─ Text: ^\n",
      "      └─ Text: N\n",
      "      └─ Text: , a known observation noise variance \n",
      "      └─ Text: ^2\n",
      "      └─ Text: , and prior\n",
      "over parameters \n",
      "      └─ Text: 0, \n",
      "      └─ Text: I\n",
      "      └─ Text: , the posterior over parameters is given by,\n",
      "\n",
      "      └─ Text: %\\begin{split}\n",
      "      └─ Text: \n",
      "p(\n",
      "      └─ Text: ) = \n",
      "      └─ Text: N\n",
      "      └─ Text: ( & \n",
      "      └─ Text: X^\n",
      "      └─ Text:  X + \n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ^2 \n",
      "      └─ Text: I\n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  y, \n",
      "      └─ Text: %\\\\\n",
      "      └─ Text: ^\n",
      "      └─ Text: -2\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  X + \n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: I\n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ).\t\n",
      "\n",
      "      └─ Text: %\\end{split}\n",
      "      └─ Text: \n",
      "Following \n",
      "      └─ Text: grunwald2017inconsistency\n",
      "      └─ Text: ,who study \n",
      "      └─ Text: T > 1\n",
      "      └─ Text:  in the context of model mis-specfication, the tem...\n",
      "      └─ Text: T\n",
      "      └─ Text:  is,\n",
      "\n",
      "      └─ Text: %\\begin{split}\n",
      "      └─ Text: \n",
      "p_\n",
      "      └─ Text: temp\n",
      "      └─ Text: (\n",
      "      └─ Text: ) = \n",
      "      └─ Text: N\n",
      "      └─ Text: ( & \n",
      "      └─ Text: 1\n",
      "      └─ Text: T\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  X + \n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text:  T\n",
      "      └─ Text: ^2 \n",
      "      └─ Text: I\n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  y, \n",
      "      └─ Text: %\\\\\n",
      "      └─ Text: 1\n",
      "      └─ Text: T\n",
      "      └─ Text: ^\n",
      "      └─ Text: -2\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  X + \n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: I\n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ).\n",
      "\n",
      "      └─ Text: %\\end{split}\n",
      "      └─ Text: \n",
      "Further, for a \n",
      "      └─ Text: cold posterior\n",
      "      └─ Text: , where both the likelihood and the prior are\n",
      "rais...\n",
      "      └─ Text: aitchison2020statistical\n",
      "      └─ Text:  and rewrite \n",
      "the prior over \n",
      "      └─ Text:  as \n",
      "      └─ Text: 0, \n",
      "      └─ Text:  T  \n",
      "      └─ Text: I\n",
      "      └─ Text: , giving the posterior as,\n",
      "\n",
      "      └─ Text: %\\begin{split}\n",
      "      └─ Text: \n",
      "p_\n",
      "      └─ Text: cold\n",
      "      └─ Text: (\n",
      "      └─ Text: ) = \n",
      "      └─ Text: N\n",
      "      └─ Text: ( & \n",
      "      └─ Text: X^\n",
      "      └─ Text:  X + \n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ^2 \n",
      "      └─ Text: I\n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  y, \n",
      "      └─ Text: %\\\\\n",
      "      └─ Text: \n",
      "    T\n",
      "      └─ Text: ^\n",
      "      └─ Text: -2\n",
      "      └─ Text:  X^\n",
      "      └─ Text:  X + \n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: I\n",
      "      └─ Text: ^\n",
      "      └─ Text: -1\n",
      "      └─ Text: ).\t\n",
      "\n",
      "      └─ Text: %\\end{split}\n",
      "      └─ Text: \n",
      "Thus, ``cold\" posteriors in this setting reproduc...\n",
      "      └─ Text:  in \n",
      "      └─ Text: fig:tempering_explainer\n",
      "      └─ Text:  left.\n",
      "\n",
      "\n",
      "      └─ Text: !ht\n",
      "      └─ Text: width=.7\n",
      "      └─ Text: figures/tempering_explainer.pdf\n",
      "      └─ Text: Left:\n",
      "      └─ Text:  Posterior confidence ellipses for Bayesian linear...\n",
      "      └─ Text: Right:\n",
      "      └─ Text:  Posterior functions for all three approaches. The...\n",
      "      └─ Text: fig:tempering_explainer\n",
      "      └─ Text: \n",
      "For \n",
      "      └─ Text: T < 1\n",
      "      └─ Text: , we tend to increase the concentration of the pos...\n",
      "      └─ Text: T > 1\n",
      "      └─ Text: , we tend to decrease the concentration of the \n",
      "po...\n",
      "      └─ Text:  to the resulting distribution over the function \n",
      "      └─ Text: y_\n",
      "      └─ Text:  = \n",
      "      └─ Text: ^\n",
      "      └─ Text:  x_\n",
      "      └─ Text:  for novel inputs \n",
      "      └─ Text: x_\n",
      "      └─ Text: , the cold posterior over \n",
      "      └─ Text: y_\n",
      "      └─ Text:  has the same mean as the Bayesian posterior, whil...\n",
      "      └─ Text: T\n",
      "      └─ Text: .\n",
      "We also demonstrate this effect in \n",
      "      └─ Text: fig:tempering_explainer\n",
      "      └─ Text:  right, showing that the slope of the cold model i...\n",
      "      └─ Text: adlam2020cold\n",
      "      └─ Text:  in the context of Gaussian process regression, wh...\n",
      "    └─ subsection: Cold Posteriors and Sharp Likelihoods\n",
      "      └─ Text: sec:cp_and_sharpness\n",
      "      └─ Text: \n",
      "While constructing a cold posterior \n",
      "      └─ Text: eq:cold_posterior\n",
      "      └─ Text:  involves tempering both the likelihood and the pr...\n",
      "      └─ Text: eq:tempered_posterior\n",
      "      └─ Text:  alone. \n",
      "Many classes of prior distributions, spec...\n",
      "      └─ Text: 1/T\n",
      "      └─ Text: .\n",
      "We formalize this statement in \n",
      "      └─ Text: thm:rescaled_priors\n",
      "      └─ Text: .\n",
      "\n",
      "\n",
      "      └─ Text: thm:rescaled_priors\n",
      "      └─ Text: \n",
      "For proper prior distributions, \n",
      "      └─ Text: p(\n",
      "      └─ Text: ),\n",
      "      └─ Text:  that have bounded density functions, e.g. \n",
      "      └─ Text: p(\n",
      "      └─ Text: ) \n",
      "      └─ Text:  M\n",
      "      └─ Text:  almost everywhere, then for \n",
      "      └─ Text: T \n",
      "      └─ Text:  1,\n",
      "      └─ Text: p(\n",
      "      └─ Text: )^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  / \n",
      "      └─ Text: I\n",
      "      └─ Text:  is a proper prior distribution, where \n",
      "      └─ Text: I\n",
      "      └─ Text:  = \n",
      "      └─ Text:  p(\n",
      "      └─ Text: )^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  d\n",
      "      └─ Text:  < \n",
      "      └─ Text: .\n",
      "      └─ Text: \n",
      "As \n",
      "      └─ Text: p(\n",
      "      └─ Text: )\n",
      "      └─ Text:  is bounded above by \n",
      "      └─ Text: M\n",
      "      └─ Text: , then \n",
      "      └─ Text: p(\n",
      "      └─ Text: ) / M \n",
      "      └─ Text:  1\n",
      "      └─ Text:  for all \n",
      "      └─ Text: x.\n",
      "      └─ Text:  As \n",
      "      └─ Text: 1/T \n",
      "      └─ Text:  1,\n",
      "      └─ Text:  then \n",
      "      └─ Text: (p(\n",
      "      └─ Text: ) / M)^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  p(\n",
      "      └─ Text: ) / M \n",
      "      └─ Text:  1.\n",
      "      └─ Text: \n",
      "By construction \n",
      "      └─ Text: p(\n",
      "      └─ Text: ) / M\n",
      "      └─ Text:  is integrable as \n",
      "      └─ Text: p(\n",
      "      └─ Text: )\n",
      "      └─ Text:  is integrable, so then \n",
      "      └─ Text: (p(\n",
      "      └─ Text: )/M)^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  must also be integrable implying that \n",
      "      └─ Text: p(\n",
      "      └─ Text: )^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  is integrable. \n",
      "To make \n",
      "      └─ Text: p(\n",
      "      └─ Text: )^\n",
      "      └─ Text: 1/T\n",
      "      └─ Text:  a distribution, we only need to construct a norma...\n",
      "      └─ Text: I\n",
      "      └─ Text: .\n",
      "\n",
      "      └─ Text: R\n",
      "      └─ Text: .4167\n",
      "      └─ Text: % \\begin{figure}[!ht]\n",
      "      └─ Text: -1em\n",
      "      └─ Text: width=.85\n",
      "      └─ Text: figures/rescaled_sgld_acc_v3.pdf\n",
      "      └─ Text: \n",
      "    ResNet-\n",
      "      └─ Text: 18\n",
      "      └─ Text:  on CIFAR-\n",
      "      └─ Text: 10\n",
      "      └─ Text: . Cold posteriors are matched by tempered posterio...\n",
      "      └─ Text: fig:smooth_vs_tempered_softmax\n",
      "      └─ Text: -2em\n",
      "      └─ Text: thm:rescaled_priors\n",
      "      └─ Text:  shows that the cold posterior effect in the Bayes...\n",
      "      └─ Text: tempered likelihood\n",
      "      └─ Text:  posterior alone. The only difference is that the ...\n",
      "      └─ Text: sec:conn_cold_tempered\n",
      "      └─ Text:  for further examples and discussion.\n",
      "\n",
      "The above r...\n",
      "      └─ Text: zhang2019cyclical,heek2019bayesian\n",
      "      └─ Text:  to those with the cold posterior studies \n",
      "      └─ Text: wenzel2020good,fortuin2021bayesian\n",
      "      └─ Text: . Using a ResNet-\n",
      "      └─ Text: 18\n",
      "      └─ Text:  on CIFAR-\n",
      "      └─ Text: 10\n",
      "      └─ Text: , we empirically demonstrate such an equivalence i...\n",
      "      └─ Text: fig:smooth_vs_tempered_softmax\n",
      "      └─ Text: . A rescaled prior and step size \n",
      "      └─ Text:  can match the performance of a cold posterior. To...\n",
      "      └─ Text:  T\n",
      "      └─ Text: . \n",
      "We give specific examples of these priors in th...\n",
      "      └─ Text: eq:tempered_posterior\n",
      "      └─ Text: . Tempering the likelihood with \n",
      "      └─ Text: T < 1\n",
      "      └─ Text:  is sharpening the density function to facilitate ...\n",
      "      └─ Text: T < 1\n",
      "      └─ Text: ; tempering the likelihood produces a sharper like...\n",
      "      └─ subsubsection: More Cold Priors\n",
      "        └─ Text: app:cold_priors\n",
      "        └─ Text: thm:rescaled_priors\n",
      "        └─ Text:  covers many common prior choices in the Bayesian ...\n",
      "        └─ Text: Gaussian and Laplace priors\n",
      "        └─ Text: , the scaling term (variance in the case of the Ga...\n",
      "        └─ Text:  T\n",
      "        └─ Text:  where \n",
      "        └─ Text:  is the original scale.\n",
      "It also holds for correlat...\n",
      "        └─ Text: fortuin2021bayesian,izmailov2021dangers\n",
      "        └─ Text: .\n",
      "This effect was previously noted for Gaussian pr...\n",
      "        └─ Text: aitchison2020statistical\n",
      "        └─ Text: .\n",
      "\n",
      "For \n",
      "        └─ Text: Student's \n",
      "        └─ Text: t\n",
      "        └─ Text:  priors\n",
      "        └─ Text: , we show below that the degrees of freedom increa...\n",
      "        └─ Text: Cauchy priors\n",
      "        └─ Text: , the entire form of the distribution changes and ...\n",
      "        └─ Text: https://en.wikipedia.org/wiki/Noncentral_t-distrib...\n",
      "        └─ Text:  is\n",
      "\n",
      "        └─ Text:  p(x) = \n",
      "        └─ Text: (v + 1/2) - \n",
      "        └─ Text: (v/2) - \n",
      "        └─ Text: v \n",
      "        └─ Text:  - \n",
      "        └─ Text: v + 1\n",
      "        └─ Text: 2\n",
      "        └─ Text: 1 + \n",
      "        └─ Text: x^2\n",
      "        └─ Text: v \n",
      "        └─ Text: ^2\n",
      "        └─ Text: \n",
      "Rescaling by a temperature \n",
      "        └─ Text: T\n",
      "        └─ Text:  gives\n",
      "\n",
      "        └─ Text: 1\n",
      "        └─ Text: T\n",
      "        └─ Text:  p(x) = \n",
      "        └─ Text: 1\n",
      "        └─ Text: T\n",
      "        └─ Text: (v + 1/2) - \n",
      "        └─ Text: (v/2) - \n",
      "        └─ Text: v \n",
      "        └─ Text:  - \n",
      "        └─ Text: v + 1\n",
      "        └─ Text: 2T\n",
      "        └─ Text: 1 + \n",
      "        └─ Text: x^2\n",
      "        └─ Text: v \n",
      "        └─ Text: ^2\n",
      "        └─ Text: \n",
      "and letting \n",
      "        └─ Text:  v = \n",
      "        └─ Text: v + 1\n",
      "        └─ Text: T\n",
      "        └─ Text:  - 1\n",
      "        └─ Text:  produces\n",
      "\n",
      "        └─ Text: 1\n",
      "        └─ Text: T\n",
      "        └─ Text:  p(x) &= \n",
      "        └─ Text: const.\n",
      "        └─ Text:  - \n",
      "        └─ Text:  v + 1\n",
      "        └─ Text: 2\n",
      "        └─ Text: 1 + \n",
      "        └─ Text:  v\n",
      "        └─ Text:  v\n",
      "        └─ Text: x^2\n",
      "        └─ Text: v \n",
      "        └─ Text: ^2\n",
      "        └─ Text: %\\\\\n",
      "        └─ Text: \n",
      "    =\n",
      "        └─ Text: const.\n",
      "        └─ Text:  - \n",
      "        └─ Text:  v + 1\n",
      "        └─ Text: 2\n",
      "        └─ Text: 1 + \n",
      "        └─ Text: x^2\n",
      "        └─ Text:  v \n",
      "        └─ Text: ^2\n",
      "        └─ Text: \n",
      "with \n",
      "        └─ Text: ^2 = v \n",
      "        └─ Text: ^2 / \n",
      "        └─ Text:  v = \n",
      "        └─ Text: v \n",
      "        └─ Text: ^2 T\n",
      "        └─ Text: v + 1 - T\n",
      "        └─ Text: .\n",
      "        └─ Text: \n",
      "For \n",
      "        └─ Text: v > 2,\n",
      "        └─ Text:  the original variance was \n",
      "        └─ Text: ^2 \n",
      "        └─ Text: v\n",
      "        └─ Text: v - 2\n",
      "        └─ Text:  and now it is \n",
      "        └─ Text: ^2 \n",
      "        └─ Text: T\n",
      "        └─ Text: v + 1 - 3T\n",
      "        └─ Text: ,\n",
      "        └─ Text:  which is less whenever \n",
      "        └─ Text: v > 2\n",
      "        └─ Text:  and so the variance exists.\n",
      "Note additionally tha...\n",
      "        └─ Text:  v > v\n",
      "        └─ Text:  as well.\n",
      "\n",
      "For a Cauchy distribution (or equivalen...\n",
      "        └─ Text: t\n",
      "        └─ Text:  with \n",
      "        └─ Text:  = 1\n",
      "        └─ Text: ), the tempered prior follows the form \n",
      "\n",
      "        └─ Text: \n",
      "    p_T(x) \n",
      "        └─ Text: (1 + x^2)^\n",
      "        └─ Text: -1/T\n",
      "        └─ Text: ,\n",
      "\n",
      "        └─ Text: \n",
      "which produces a finite integral and therefore a ...\n",
      "        └─ Text: T \n",
      "        └─ Text:  1.\n",
      "        └─ Text: \n",
      "This class of distributions has finite means for ...\n",
      "        └─ Text: T < 1\n",
      "        └─ Text:  and finite variances for \n",
      "        └─ Text: T < 2/3\n",
      "        └─ Text: , whereas the Cauchy distributions do not have fin...\n",
      "        └─ Text: ghosh2018use\n",
      "        └─ Text: .\n",
      "\n",
      "\n",
      "    └─ subsection: Discussion with the Cold Posterior Effect\n",
      "      └─ Text: \n",
      "These types of findings tend to suggest that ``co...\n",
      "      └─ Text:  to \n",
      "      └─ Text:  T\n",
      "      └─ Text: .\n",
      "One natural question is if this finding is an ex...\n",
      "      └─ Text: wenzel2020good\n",
      "      └─ Text: . However, \n",
      "      └─ Text: wilson2020bayesian\n",
      "      └─ Text:  found that standard Gaussian \n",
      "      └─ Text: N\n",
      "      └─ Text: (0, 1)\n",
      "      └─ Text:  priors tend to be reasonably well-calibrated and ...\n",
      "      └─ Text: izmailov2021bayesian\n",
      "      └─ Text:  verified this evidence with high-quality HMC samp...\n",
      "      └─ Text: !ht\n",
      "      └─ Text: width=.7\n",
      "      └─ Text: figures/prior_study_v2.pdf\n",
      "      └─ Text: Priors of various scales for BNNs with temperature...\n",
      "      └─ Text: 1\n",
      "      └─ Text: . No prior with the same step size is able to matc...\n",
      "      └─ Text: fig:prior_scales\n",
      "      └─ Text: \n",
      "In \n",
      "      └─ Text: fig:prior_scales\n",
      "      └─ Text: , we show that no choice of prior alone with tempe...\n",
      "      └─ Text: 1\n",
      "      └─ Text:  is able to reach the same accuracy as a tempered ...\n",
      "  └─ section: Visualizations of Tempered Cross-Entropy and Noisy Dirichlet Likelihoods\n",
      "    └─ Text: sec:app_likelihood_viz\n",
      "    └─ Text: !ht\n",
      "    └─ Text: width=0.7\n",
      "    └─ Text: figures/dirichlet_likelihoods\n",
      "    └─ Text: Dirichlet densities for various values of \n",
      "    └─ Text: _\n",
      "    └─ Text:  for both the observed class (\n",
      "    └─ Text: left\n",
      "    └─ Text: ) and any unobserved classes (\n",
      "    └─ Text: right\n",
      "    └─ Text: ). Higher values of \n",
      "    └─ Text: _\n",
      "    └─ Text:  tend to produce sharper likelihoods as the distri...\n",
      "    └─ Text: f\n",
      "    └─ Text:  is the equivalent to the logit, while \n",
      "    └─ Text: y\n",
      "    └─ Text:  is the rescaled observation.\n",
      "\n",
      "    └─ Text: fig:noisy_dirichlet\n",
      "  └─ section: Targeting Distributions for Variational Inference\n",
      "    └─ Text: app:alt_inferences\n",
      "    └─ Text: \n",
      "A naive attempt to incorporate data augmentation ...\n",
      "    └─ Text: K\n",
      "    └─ Text:  copies of the data into the posterior and then to...\n",
      "    └─ Text: NK\n",
      "    └─ Text:  data points rather than \n",
      "    └─ Text: N\n",
      "    └─ Text:  data points.\n",
      "A second approach would be then to d...\n",
      "    └─ Text: N\n",
      "    └─ Text:  data points by raising the likelihood of each (au...\n",
      "    └─ Text: 1/K\n",
      "    └─ Text: , which produces exactly \n",
      "    └─ Text: eq:implied_posterior\n",
      "    └─ Text: .\n",
      "\n",
      "Variational inference with stochastic gradient ...\n",
      "    └─ Text: eq:stochastic_gradient\n",
      "    └─ Text:  as \n",
      "\n",
      "    └─ Text: ELBO\n",
      "    └─ Text: (\n",
      "    └─ Text: ) =&~ \n",
      "    └─ Text: N\n",
      "    └─ Text: m\n",
      "    └─ Text: _\n",
      "    └─ Text: (x_i,y_i) \n",
      "    └─ Text: _m\n",
      "    └─ Text: E\n",
      "    └─ Text: _\n",
      "    └─ Text: q_\n",
      "    └─ Text: (\n",
      "    └─ Text: )\n",
      "    └─ Text: _\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_j(x_i))\n",
      "    └─ Text: \n",
      "+ \n",
      "    └─ Text: _\n",
      "    └─ Text: KL\n",
      "    └─ Text: (q_\n",
      "    └─ Text: (\n",
      "    └─ Text: ) || p(\n",
      "    └─ Text: )), \n",
      "    └─ Text: eq:aug_vi_grad\n",
      "    └─ Text: \n",
      "switching the parameters of the variational distr...\n",
      "    └─ Text:  and again with transformations \n",
      "    └─ Text: t_j\n",
      "    └─ Text:  sampled uniformly from \n",
      "    └─ Text: T\n",
      "    └─ Text: .\n",
      "    └─ Text: \n",
      "The same two sources of randomness, sub-sampling ...\n",
      "    └─ Text: eq:aug_vi_grad\n",
      "    └─ Text:  produces a similar target as \n",
      "    └─ Text: eq:aug_sgld_grad_expectation\n",
      "    └─ Text: :\n",
      "\n",
      "    └─ Text: E\n",
      "    └─ Text: ELBO\n",
      "    └─ Text: (\n",
      "    └─ Text: )\n",
      "    └─ Text:  =&~ \n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N \n",
      "    └─ Text: _\n",
      "    └─ Text: j=1\n",
      "    └─ Text: ^K \n",
      "    └─ Text: [\n",
      "    └─ Text: _\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_j(x_i))^\n",
      "    └─ Text: 1/K\n",
      "    └─ Text: ]\n",
      "    └─ Text: \n",
      "+ \n",
      "    └─ Text: _\n",
      "    └─ Text: KL\n",
      "    └─ Text: (q_\n",
      "    └─ Text: (\n",
      "    └─ Text: ) || p(\n",
      "    └─ Text: )),\n",
      "\n",
      "    └─ Text: eq:aug_vi_grad_expectation\n",
      "    └─ Text: \n",
      "implying that the full optimization problem VI is...\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_j(x_i))^\n",
      "    └─ Text: 1/K\n",
      "    └─ Text: .\n",
      "We note that variational inference for BNNs ofte...\n",
      "    └─ Text: wenzel2020good\n",
      "    └─ Text:  for several examples.\n",
      "While we do not consider La...\n",
      "    └─ Text: immer2021scalable\n",
      "    └─ Text:  for example), suggesting that they also tend to t...\n",
      "  └─ section: A Proper Likelihood for Data Augmentation\n",
      "    └─ Text: sec:app_aug_lik\n",
      "    └─ Text: \n",
      "Treating data augmentations as independent sample...\n",
      "    └─ Text: sec:data_aug_gp\n",
      "    └─ Text: , we see that stacking the augmented samples in to...\n",
      "    └─ Text: t_j \n",
      "    └─ Text: T\n",
      "    └─ Text: , consider a likelihood which extends the usual ob...\n",
      "    └─ Text: eq:aug_likelihood\n",
      "    └─ Text: \n",
      "    p_\n",
      "    └─ Text: aug\n",
      "    └─ Text: (\n",
      "    └─ Text: ) = \n",
      "    └─ Text: _\n",
      "    └─ Text: i=1\n",
      "    └─ Text: ^N p(y_i \n",
      "    └─ Text:  x_i) \n",
      "    └─ Text: _\n",
      "    └─ Text: t_j \n",
      "    └─ Text: T\n",
      "    └─ Text:  p\n",
      "    └─ Text: f(t_j(x_i); \n",
      "    └─ Text: )) \n",
      "    └─ Text:  f(x_i; \n",
      "    └─ Text: ) \n",
      "    └─ Text: .\n",
      "\n",
      "    └─ Text: \n",
      "The augmentation likelihood \n",
      "    └─ Text: p_\n",
      "    └─ Text: aug\n",
      "    └─ Text:  properly accounts for aleatoric uncertainty when ...\n",
      "    └─ Text: _m \n",
      "    └─ Text:  of size \n",
      "    └─ Text: m\n",
      "    └─ Text: , and a subset of \n",
      "    └─ Text: k\n",
      "    └─ Text:  augmentations \n",
      "    └─ Text: T\n",
      "    └─ Text: _k \n",
      "    └─ Text: T\n",
      "    └─ Text: , the valid unbiased stochastic gradient estimator...\n",
      "    └─ Text: U\n",
      "    └─ Text: _\n",
      "    └─ Text: aug\n",
      "    └─ Text: (\n",
      "    └─ Text: ) =&~ \n",
      "    └─ Text: N\n",
      "    └─ Text: m\n",
      "    └─ Text: _\n",
      "    └─ Text: (x_i,y_i) \n",
      "    └─ Text: _m\n",
      "    └─ Text: _\n",
      "    └─ Text: p(y_i \n",
      "    └─ Text:  t_k(x_i))\n",
      "    └─ Text:  + \n",
      "    └─ Text: _\n",
      "    └─ Text: p(\n",
      "    └─ Text: )\n",
      "    └─ Text: \\\\\n",
      "    └─ Text: &~+ \n",
      "    └─ Text: N\n",
      "    └─ Text: m\n",
      "    └─ Text: _\n",
      "    └─ Text: x_i \n",
      "    └─ Text: _m\n",
      "    └─ Text: K\n",
      "    └─ Text: k\n",
      "    └─ Text: _\n",
      "    └─ Text: t_j \n",
      "    └─ Text: T\n",
      "    └─ Text: _k\n",
      "    └─ Text: _\n",
      "    └─ Text:  p\n",
      "    └─ Text: f(t_j(x_i); \n",
      "    └─ Text: )) \n",
      "    └─ Text:  f(x_i; \n",
      "    └─ Text: ) \n",
      "    └─ Text:   .\n",
      "\n",
      "    └─ Text: eq:stochastic_gradient_aug\n",
      "    └─ Text: \n",
      "Note that to use the augmentation likelihood in \n",
      "    └─ Text: eq:aug_likelihood\n",
      "    └─ Text: , we need to know the number \n",
      "    └─ Text: K\n",
      "    └─ Text:  of possible augmentations that we consider.\n",
      "\n",
      "\n",
      "  └─ section: Further Experimental Results\n",
      "    └─ Text: app:further_experiments\n",
      "    └─ Text: \n",
      "In the top left panel of \n",
      "    └─ Text: fig:noisy_dirichlet_perf\n",
      "    └─ Text: , we demonstrate that softmax scaling in the prese...\n",
      "    └─ Text: _\n",
      "    └─ Text:  then vary the sharpness of the resulting posterio...\n",
      "    └─ Text: fig:noisy_dirichlet_perf\n",
      "    └─ Text:  for a two spirals problem with \n",
      "    └─ Text: 150\n",
      "    └─ Text:  data points and \n",
      "    └─ Text: 20\n",
      "    └─ Text:  noisy labels.\n",
      "The posterior predictive for \n",
      "    └─ Text: _\n",
      "    └─ Text:  = 0.001\n",
      "    └─ Text:  is only confident in a very small region of the d...\n",
      "    └─ Text:  are confident in a broader region indicating a sh...\n",
      "    └─ Text: _\n",
      "    └─ Text:  = 0.1\n",
      "    └─ Text:  but performs similarly in terms of confidence to ...\n",
      "    └─ Text: _\n",
      "    └─ Text:  = 0.25,\n",
      "    └─ Text:  suggesting that too sharp of distributions can re...\n",
      "    └─ Text: !ht\n",
      "    └─ Text: width=\n",
      "    └─ Text: figures/gp_classifiers.pdf\n",
      "    └─ Text: Modeling label noise explicitly using \n",
      "    └─ Text: eq:noisy_dirichlet_gauss\n",
      "    └─ Text:  and using tuned values of \n",
      "    └─ Text:  enables better modelling of aleatoric uncertainty...\n",
      "    └─ Text: 150\n",
      "    └─ Text:  data points and \n",
      "    └─ Text: 20\n",
      "    └─ Text:  flipped labels.\n",
      "    └─ Text: fig:noisy_dirichlet_perf\n",
      "    └─ Text: !ht\n",
      "    └─ Text: width=0.8\n",
      "    └─ Text: figures/bma_lr_rescaling.pdf\n",
      "    └─ Text: We empirically verify that a tempered likelihood c...\n",
      "    └─ Text: sec:conn_cold_tempered\n",
      "    └─ Text: .\n",
      "  └─ section: Experimental Details\n",
      "    └─ Text: app:experimental_details\n",
      "    └─ Text: \n",
      "We use PyTorch for all experiments \n",
      "    └─ Text: paszke2019pytorch\n",
      "    └─ Text: .\n",
      "For the Gaussian process experiments, we used GP...\n",
      "    └─ Text: gardner2018gpytorch\n",
      "    └─ Text:  for all but the Laplace bernoulli classifier wher...\n",
      "    └─ Text: pedregosa2011scikit\n",
      "    └─ Text: .\n",
      "\n",
      "We used \n",
      "    └─ Text: N\n",
      "    └─ Text: (0,1)\n",
      "    └─ Text:  priors unless otherwise stated and ran cyclical l...\n",
      "    └─ Text: 1000\n",
      "    └─ Text:  epochs with an initial learning rate of \n",
      "    └─ Text: 10^\n",
      "    └─ Text: -6\n",
      "    └─ Text:  and a momentum term of \n",
      "    └─ Text: 0.99\n",
      "    └─ Text: .\n",
      "We used ResNet-\n",
      "    └─ Text: 18\n",
      "    └─ Text: he2016identity\n",
      "    └─ Text:  architectures unless otherwise stated and used th...\n",
      "    └─ Text: 10\n",
      "    └─ Text: https://www.cs.toronto.edu/~kriz/cifar.html\n",
      "    └─ Text:  dataset\n",
      "    └─ Text: .\n",
      "\n",
      "For experiment management, we used wandb \n",
      "    └─ Text: wandb\n",
      "    └─ Text:  to manage experiments.\n",
      "Experiments were performed...\n",
      "    └─ Text: 10\n",
      "    └─ Text:  hours to run, with a total of 3 trials per experi...\n",
      "    └─ Text: 3\n",
      "    └─ Text:  random seeds and we plotted the mean with shading...\n",
      "    └─ subsection: Synthetic problem\n",
      "      └─ Text: sec:app_exp_synthetic\n",
      "      └─ Text: \n",
      "We present the code used to generate the data for...\n",
      "      └─ Text: sec:exp_synthetic\n",
      "      └─ Text:  in \n",
      "      └─ Text: code:synthdata\n",
      "      └─ Text: .\n",
      "\n",
      "For all models we use an iid Gaussian prior \n",
      "      └─ Text:  N(0, 0.3^2)\n",
      "      └─ Text:  over the parameters.\n",
      "We use HMC with a step size \n",
      "      └─ Text: 3 \n",
      "      └─ Text:  10^\n",
      "      └─ Text: -6\n",
      "      └─ Text:  and the trajectory length is \n",
      "      └─ Text:  0.3 / 2\n",
      "      └─ Text: , amounting to \n",
      "      └─ Text: 150\n",
      "      └─ Text:  10^3\n",
      "      └─ Text:  leapfrog steps per iteration, following the advic...\n",
      "      └─ Text: izmailov2021bayesian\n",
      "      └─ Text: .\n",
      "We run HMC for 100 iterations, discarding the fi...\n",
      "      └─ Text: K = 4\n",
      "      └─ Text:  augmentations of each datapoint.\n",
      "We run HMC on th...\n",
      "      └─ Text: eq:implied_posterior\n",
      "      └─ Text: .\n",
      "For the tempered likelihood, we use \n",
      "      └─ Text: T=0.1\n",
      "      └─ Text: , and for the noisy Dirichlet model we use \n",
      "      └─ Text: _\n",
      "      └─ Text: =10^\n",
      "      └─ Text: -5\n",
      "      └─ Text: .\n",
      "\n",
      "\n",
      "      └─ Text: language=Python, caption=Data generation for the s...\n",
      "      └─ Text: code:synthdata\n",
      "      └─ Text: \n",
      "import numpy as np\n",
      "    \n",
      "def twospirals(n_samples,...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from TexSoup import TexSoup\n",
    "from TexSoup.data import TexNode\n",
    "\n",
    "@dataclass\n",
    "class LatexElement:\n",
    "    \"\"\"Represents a node in the LaTeX document tree.\"\"\"\n",
    "    type: str  # The type of element (section, subsection, text, etc.)\n",
    "    name: str  # The name or title of the element\n",
    "    content: str  # Raw content\n",
    "    level: int  # Nesting level\n",
    "    children: List['LatexElement']  # Child elements\n",
    "    parent: Optional['LatexElement'] = None  # Parent element\n",
    "\n",
    "class LatexParser:\n",
    "    \"\"\"Parser for LaTeX documents that creates a tree structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: Union[str, Path]):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.root = None\n",
    "        self.current_node = None\n",
    "        self.section_levels = {\n",
    "            'chapter': 0,\n",
    "            'section': 1,\n",
    "            'subsection': 2,\n",
    "            'subsubsection': 3,\n",
    "            'paragraph': 4,\n",
    "            'subparagraph': 5\n",
    "        }\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def parse_main_file(self, main_file: str) -> LatexElement:\n",
    "        \"\"\"\n",
    "        Parse the main LaTeX file and create a document tree.\n",
    "        \n",
    "        Args:\n",
    "            main_file: Name of the main .tex file\n",
    "            \n",
    "        Returns:\n",
    "            LatexElement: Root node of the document tree\n",
    "        \"\"\"\n",
    "        try:\n",
    "            file_path = self.base_dir / main_file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Parse the LaTeX content\n",
    "            soup = TexSoup(content)\n",
    "            \n",
    "            # Create root node\n",
    "            self.root = LatexElement(\n",
    "                type='document',\n",
    "                name='root',\n",
    "                content='',\n",
    "                level=-1,\n",
    "                children=[]\n",
    "            )\n",
    "            self.current_node = self.root\n",
    "            \n",
    "            # Process document body\n",
    "            self._process_node(soup)\n",
    "            \n",
    "            return self.root\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File not found: {file_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing LaTeX document: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _process_node(self, node: TexNode) -> None:\n",
    "        \"\"\"\n",
    "        Recursively process a TexSoup node and its children.\n",
    "        \n",
    "        Args:\n",
    "            node: Current TexSoup node being processed\n",
    "        \"\"\"\n",
    "        # Handle different types of LaTeX elements\n",
    "        for child in node.contents:\n",
    "            if hasattr(child, 'name'):\n",
    "                name = child.name\n",
    "                \n",
    "                # Handle sections and their variants\n",
    "                if name in self.section_levels:\n",
    "                    level = self.section_levels[name]\n",
    "                    title = str(child.string) if hasattr(child, 'string') else ''\n",
    "                    \n",
    "                    # Create new section node\n",
    "                    new_node = LatexElement(\n",
    "                        type=name,\n",
    "                        name=title,\n",
    "                        content=str(child),\n",
    "                        level=level,\n",
    "                        children=[]\n",
    "                    )\n",
    "                    \n",
    "                    # Adjust current node based on section level\n",
    "                    while (self.current_node != self.root and \n",
    "                           self.current_node.level >= level):\n",
    "                        self.current_node = self.current_node.parent\n",
    "                    \n",
    "                    new_node.parent = self.current_node\n",
    "                    self.current_node.children.append(new_node)\n",
    "                    self.current_node = new_node\n",
    "                \n",
    "                # Handle included files\n",
    "                elif name == 'input' or name == 'include':\n",
    "                    included_file = str(child.string)\n",
    "                    if not included_file.endswith('.tex'):\n",
    "                        included_file += '.tex'\n",
    "                    \n",
    "                    try:\n",
    "                        with open(self.base_dir / included_file, 'r', encoding='utf-8') as f:\n",
    "                            included_content = f.read()\n",
    "                        included_soup = TexSoup(included_content)\n",
    "                        self._process_node(included_soup)\n",
    "                    except FileNotFoundError:\n",
    "                        self.logger.warning(f\"Included file not found: {included_file}\")\n",
    "                \n",
    "                # Process other environments and commands\n",
    "                else:\n",
    "                    self._process_node(child)\n",
    "            \n",
    "            # Handle text content\n",
    "            elif str(child).strip():\n",
    "                text_node = LatexElement(\n",
    "                    type='text',\n",
    "                    name='',\n",
    "                    content=str(child),\n",
    "                    level=self.current_node.level + 1,\n",
    "                    children=[],\n",
    "                    parent=self.current_node\n",
    "                )\n",
    "                self.current_node.children.append(text_node)\n",
    "\n",
    "    def print_tree(self, node: Optional[LatexElement] = None, level: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Print the document tree in a hierarchical format.\n",
    "        \n",
    "        Args:\n",
    "            node: Current node to print (defaults to root)\n",
    "            level: Current indentation level\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "            \n",
    "        indent = \"  \" * level\n",
    "        if node.type == 'text':\n",
    "            content_preview = node.content[:50] + \"...\" if len(node.content) > 50 else node.content\n",
    "            print(f\"{indent}└─ Text: {content_preview}\")\n",
    "        else:\n",
    "            print(f\"{indent}└─ {node.type}: {node.name}\")\n",
    "        \n",
    "        for child in node.children:\n",
    "            self.print_tree(child, level + 1)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the LaTeX parser.\"\"\"\n",
    "    # Example usage\n",
    "    parser = LatexParser(\"./sources/2203.16481v1\")\n",
    "    try:\n",
    "        root = parser.parse_main_file(\"paper.tex\")\n",
    "        print(\"\\nDocument Structure:\")\n",
    "        parser.print_tree()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "     \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16ca51c-8ef6-498c-87a8-7f700f14224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sources/2203.16481v1/paper.tex\n"
     ]
    }
   ],
   "source": [
    "!ls ./sources/2203.16481v1/paper.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24de870-cbda-4a2e-a4cf-6e8410bcb3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\\\documentclass[twoside,11pt]{article}\\n\\n\\\\newcommand{\\\\theHalgorithm}{\\\\arabic{algorithm}}\\n\\n\\\\usepackage[abbrvbib,nohyperref,preprint]{jmlr2e}\\n\\\\usepackage[ref,caption]{leaf}\\n\\n\\\\usepackage{listings}\\n\\\\usepackage{xcolor}\\n\\n\\\\definecolor{codegreen}{rgb}{0,0.6,0}\\n\\\\definecolor{codegray}{rgb}{0.5,0.5,0.5}\\n\\\\definecolor{codepurple}{rgb}{0.58,0,0.82}\\n\\\\definecolor{backcolour}{rgb}{0.95,0.95,0.92}\\n\\\\definecolor{dark-blue}{rgb}{0.15,0.15,0.4}\\n\\\\definecolor{medium-blue}{rgb}{0,0,0.5}\\n\\n\\\\lstdefinestyle{mystyle}{\\n    backgroundcolor=\\\\color{backcolour},   \\n    commentstyle=\\\\color{codegreen},\\n    keywordstyle=\\\\color{magenta},\\n    numberstyle=\\\\tiny\\\\color{codegray},\\n    stringstyle=\\\\color{codepurple},\\n    basicstyle=\\\\ttfamily\\\\footnotesize,\\n    breakatwhitespace=false,         \\n    breaklines=true,                 \\n    captionpos=b,                    \\n    keepspaces=true,                 \\n    numbers=left,                    \\n    numbersep=5pt,                  \\n    showspaces=false,                \\n    showstringspaces=false,\\n    showtabs=false,                  \\n    tabsize=2\\n}\\n\\n\\n\\\\hypersetup{\\n  colorlinks, linkcolor={dark-blue},\\n  citecolor={dark-blue}, urlcolor={medium-blue}\\n}\\n\\n\\\\lstset{style=mystyle}\\n\\n\\n%% Layout\\n\\\\setlength{\\\\parindent}{0em}\\n\\\\setlength{\\\\parskip}{1em}\\n\\n\\\\newcommand{\\\\xxcomment}[4]{\\\\textcolor{#1}{[$^{\\\\textsc{#2}}_{\\\\textsc{#3}}$ #4]}}\\n\\\\newcommand{\\\\sanyam}[1]{\\\\xxcomment{blue}{S}{K}{#1}}\\n\\\\newcommand{\\\\wesley}[1]{\\\\xxcomment{green}{W}{M}{#1}}\\n\\\\newcommand{\\\\pavel}[1]{\\\\xxcomment{cyan}{P}{I}{#1}}\\n\\\\newcommand{\\\\agw}[1]{\\\\xxcomment{red}{A}{W}{#1}}\\n\\\\newcommand{\\\\done}{\\\\textcolor{green}{$\\\\checkmark$ Done}}\\n\\n\\n%% Notations\\n\\\\newcommand{\\\\params}{\\\\theta}\\n\\n\\\\ShortHeadings{}{On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification}\\n\\n\\\\begin{document}\\n\\n\\\\title{On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification}\\n\\n\\\\author{\\\\name Sanyam Kapoor\\\\thanks{Equal contribution.}, \\\\addr New York University  \\\\\\\\\\n      \\\\name Wesley J. Maddox$^*$, \\\\addr New York University \\\\\\\\\\n      \\\\name Pavel Izmailov$^*$, \\\\addr New York University \\\\\\\\\\n      \\\\name Andrew Gordon Wilson, \\\\addr New York University}\\n\\n\\\\maketitle\\n\\n\\\\begin{abstract}\\nAleatoric uncertainty captures the inherent randomness of the data, such as measurement noise. In Bayesian regression, we often use a Gaussian observation model, where we control the level of aleatoric uncertainty with a noise variance parameter. By contrast, for Bayesian classification we use a categorical distribution with no mechanism to represent our beliefs about aleatoric uncertainty. Our work shows that explicitly accounting for aleatoric uncertainty significantly improves the performance of Bayesian neural networks. We note that many standard benchmarks, such as CIFAR, have essentially no aleatoric uncertainty. Moreover, we show data augmentation in approximate inference has the effect of softening the likelihood, leading to underconfidence and profoundly misrepresenting our honest beliefs about aleatoric uncertainty. Accordingly, we find that a cold posterior, tempered by a power greater than one, often more honestly reflects our beliefs about aleatoric uncertainty than no tempering --- providing an explicit link between data augmentation and cold posteriors. We show that we can match or exceed the performance of posterior tempering by using a Dirichlet observation model, where we explicitly control the level of aleatoric uncertainty, without any need for tempering.\\n\\\\end{abstract}\\n\\n\\\\section{Introduction}\\n\\\\label{sec:intro}\\n\\nUncertainty is often compartmentalized into \\\\emph{epistemic uncertainty} and \\\\emph{aleatoric uncertainty} \\\\citep{Hora1996AleatoryAE,Kendall2017WhatUD,Malinin2018PredictiveUE}. Epistemic uncertainty, sometimes called \\\\emph{model uncertainty}, is the reducible uncertainty over which solution is correct given limited information. Bayesian methods naturally represent epistemic uncertainty through a distribution over model parameters, leading to a posterior distribution over functions that are consistent with data. As we observe more data, this posterior distribution concentrates around a single solution. Aleatoric uncertainty is intrinsic irreducible uncertainty, often representing measurement noise in regression, or mislabeled training points in classification \\\\citep[e.g.][]{beyer2020we}. Although measurement noise can be reduced, for example, with better instrumentation, it is often a fixed property of the data we are given. Correctly expressing our assumptions about aleatoric uncertainty is crucial for achieving good predictive performance, both with Bayesian and non-Bayesian models \\n\\\\citep{Senge2014ReliableCL}.\\n\\nIn particular, our assumptions about aleatoric uncertainty profoundly affect predictive \\\\emph{accuracy}, not only predictive uncertainty.\\\\footnote{Epistemic uncertainty also has a significant effect on predictive accuracy, as discussed in \\\\citet{wilson2020bayesian}.} In \\\\cref{fig:conceptual}(a,b) we show the predictive distributions of two Gaussian process regression models \\\\citep{rasmussen2006gaussian} trained on the same data and using the same RBF kernel function.\\nThe only difference between these models is their assumptions about the aleatoric uncertainty.\\nThe model in \\\\cref{fig:conceptual}(a) assumes a high observation noise (each point is corrupted by $\\\\mathcal{N}(0,\\\\sigma^2 )$, with $\\\\sigma^2 = 1$).\\nConsequently, this model explains many of the observations with noise --- such that the predictive mean does not closely fit the training datapoints.\\nThe model in \\\\cref{fig:conceptual}(b) assumes a low observation noise ($\\\\sigma^2 = 10^{-2}$), and consequently the predictive mean runs through the training data, leading to very different predictions compared to the model in panel (a).\\n\\n\\n\\\\begin{figure*}[!t]\\n\\\\centering\\n    \\n\\\\begin{tabular}{c cc cc}\\n\\n\\\\hspace{-0.4cm}\\\\includegraphics[height=.277\\\\linewidth]{figures/intro_gp_highsigma.pdf}\\n&&\\n\\\\hspace{-.7cm}\\\\includegraphics[height=.277\\\\linewidth]{figures/intro_gp_lowsigma.pdf}\\n&\\\\quad\\\\quad&\\n\\\\includegraphics[height=.27\\\\linewidth]{figures/conceptual_aleatoric.pdf}\\n\\\\\\\\[1mm]\\n(a) GP regression, $\\\\sigma^2 = 1$ &&\\n\\\\hspace{-.4cm}(b) GP regression, $\\\\sigma^2 = 10^{-2}$&&\\n(c) Classification\\n\\\\end{tabular}\\n\\\\caption{\\n    \\\\textbf{Effect of aleatoric uncertainty.}\\n    In regression problems, we can express our assumptions about aleatoric uncertainty by setting the observation noise parameter $\\\\sigma^2$.\\n    \\\\textbf{(a)}: Gaussian process regression with high observation noise $\\\\sigma^2$ explains many of the observations with noise.\\n    Here the datapoints are shown with purple circles, the dashed line shows the predictive mean, and the shaded region shows one standard deviation of the predictive distribution. \\n    \\\\textbf{(b)}: Gaussian process regression on the same data, but with low observation noise $\\\\sigma^2$ fits the training data nearly perfectly.\\n    \\\\textbf{(c)}: In classification problems, we do not have a direct way to specify our assumptions about aleatoric uncertainty.\\n    In particular, we might use the same Bayesian neural network model if we know the data contains label noise (scenario A) and if we know that there is no label noise (scenario B), leading to poor performance in at least one of these scenarios.\\n}\\n\\\\label{fig:conceptual}\\n\\\\end{figure*}\\n\\n\\nOur assumptions about the aleatoric uncertainty are just as important in classification.\\nWe illustrate this point in \\\\cref{fig:conceptual}(c), where we fix a dataset and consider two possible scenarios.\\n\\\\textbf{Scenario A}: suppose we know that the data contains label noise, such that some of the training examples are labeled incorrectly.\\nIn this case, it is reasonable to believe that the data points shown in red in \\\\cref{fig:conceptual}(c) are highly likely to be incorrectly labeled, and we should opt for a solution with a decision boundary shown with a dotted red line.\\n\\\\textbf{Scenario B}: now, suppose that we know that the data are correctly labeled.\\nIn this case, we want our model to perfectly classify all the training data and would prefer a solution shown by the dashed blue line.\\nBoth of these solutions are reasonable descriptions of the data, and we can only make an informed choice between them by incorporating our assumptions about the aleatoric uncertainty.\\n\\nWhile in regression problems we can easily express our assumptions about aleatoric uncertainty, such as through a noise variance parameter in a Gaussian likelihood, \\nin classification we do not have a direct way to express these beliefs.\\nIndeed, in practice we might use the same Bayesian neural network model (with the same likelihood and prior) in both scenarios A and B above, necessarily leading to poor performance in at least one of these scenarios.\\n\\nIn this paper, we investigate ways of expressing our beliefs about aleatoric uncertainty in classification. In \\\\cref{sec:aleatoric_uncertainty_understanding} we show that the standard softmax likelihood is equivalent to a Dirichlet likelihood over the predicted class probabilities, providing a mechanism to directly reason about aleatoric uncertainty in classification. Using this interpretation, we show that posterior tempering is a natural way of expressing beliefs about aleatoric uncertainty: a ``cold posterior'', raised to a power $1 / T$ with $T < 1$, corresponds to increasing the concentration parameter of the Dirichlet distribution for the predicted probability of the observed class, forcing the model to be confident on the train data. As an alternative, we consider a \\\\emph{noisy Dirichlet model}, which can be viewed as an input dependent prior, to provide direct control over aleatoric uncertainty for Bayesian neural network classifiers. This approach achieves competitive performance on image classification problems with a valid likelihood and no need for tempering. In \\\\cref{sec:data_aug_effects}, we show theoretically how data augmentation counterintuitively softens the likelihood, leading to solutions that underfit the training data. These results precisely characterize the empirical link between data augmentation and the success of cold posteriors observed by \\\\citet{izmailov2021bayesian} and \\\\citet{fortuin2021bayesian}, and show that tempering in fact serves to \\\\emph{correct} for the effects of data augmentation on representing aleatoric uncertainty. In \\\\cref{sec:experiments} we exemplify several of the conceptual findings in previous sections.\\n\\nIn short, we show how it is possible to explicitly characterize aleatoric uncertainty in Bayesian neural network classification, and how we can naturally accommodate data augmentation in a Bayesian setting. We find that cold posteriors in Bayesian neural networks more honestly reflect our beliefs about aleatoric uncertainty than $T=1$, as data augmentation softens the likelihood, and many standard benchmarks like CIFAR have essentially no aleatoric uncertainty for the observed training points. While \\\\citet{wenzel2020good} note that cold posteriors can provide good performance in Bayesian neural network classifiers, we show that cold posteriors are not required --- simple modifications to the likelihood to reflect our honest beliefs about aleatoric uncertainty provide comparable performance.\\n\\n\\n\\\\section{Related Work}\\n\\\\label{sec:related}\\n\\nEarly work on Bayesian neural networks (BNNs) focused on hyperparameter learning and mitigating overfitting, using Laplace approximations, variational methods, and Hamiltonian Monte Carlo based MCMC \\\\citep[e.g.][]{Mackay1992APB, hinton1993keeping, Neal1995BayesianLF}. More recently, \\\\citet{wilson2020bayesian} show how Bayesian model averaging is especially compelling for \\\\emph{modern} deep networks --- covering deep ensembles \\\\citep{lakshminarayanan2017simple} as approximate Bayesian inference, induced priors in function space, mitigating double descent, generalization behaviour in deep learning \\\\citep{zhang2021understanding}, posterior tempering \\\\citep{grunwald2012safe, wenzel2020good}, and connections with loss surface structure such as mode connectivity \\\\citep{garipov2018loss}. A number of recent works have focused on making Bayesian deep learning practical \\\\citep[e.g.,][]{wilson2016deep, maddox2019simple, osawa2019practical, zhang2019cyclical, wilson2020bayesian, dusenberry2020efficient, daxberger2021laplace}, often with better results than classical training, and essentially no additional runtime overhead. BNNs also have an exciting range of applications, from astrophysics \\\\citep{cranmer2021bayesian}, to click-through rate prediction \\\\citep{liu2017pbodl}, to diagnosis of diabetic retinopathy \\\\citep{filos2019systematic}, to fluid dynamics \\\\citep{geneva2020modeling}. \\n\\n\\n\\\\citet{wenzel2020good} demonstrated with several examples that raising the posterior to a power $1/T$, with $T<1$, in conjunction with SGLD inference, improves performance over classical training, but $T=1$ can provide worse results than classical training, in what has been termed the \\\\emph{cold posterior effect}. However, in a study using HMC inference \\\\citet{izmailov2021bayesian} showed that for \\\\emph{all} cases considered in \\\\citet{wenzel2020good} there is no cold posterior effect when data augmentation is removed. Indeed, while \\\\citet{Noci2021DisentanglingTR} claims that data augmentation only explains the cold posterior effect for CIFAR-10 but not IMDB in \\\\citet{wenzel2020good}, \\\\citet{izmailov2021bayesian} in fact also shows no cold posterior effect on IMDB without data augmentation. The sufficiency of data augmentation to observe the cold posterior effect has since been confirmed by several works using SG-MCMC inference \\\\citep{fortuin2021bayesian,Noci2021DisentanglingTR,nabarro2021data}. \\n\\nSeveral works have suggested that misspecified priors explain cold posteriors, arguing that the current default choice of isotropic Gaussian priors in BNNs are inadequate \\\\citep{wenzel2020good,zeno2020cold,fortuin2021bayesian}. \\nHowever, \\\\citet{fortuin2021bayesian} find that the cold posterior effect cannot be alleviated for convolutional neural networks by using heavy tailed or correlated priors under data augmentation.\\nWhile \\\\citet{fortuin2021bayesian} claim a cold posterior effect on Fashion MNIST for fully connected \\nMLPs with Gaussian priors and no data augmentation, their experiments show very minimal effects (about $0.25\\\\%$ on test accuracy on Fashion MNIST) that are generally not present \\nin terms of calibration or out-of-distribution detection. Moreover, \\\\citet{wilson2020bayesian} show that the experiments in \\\\citet{wenzel2020good} suggesting a poor prior are easily resolved\\nby tuning the variance scale of the prior, and that isotropic Gaussian priors are practically effective and provide desirable properties in function space. \\\\citet{izmailov2021bayesian} also show that \\nstandard Gaussian priors perform similarly to a range of other priors, such as heavy-tailed logistic priors, and mixture of Gaussian priors, and generally are high performing, providing better results\\nthan standard training and deep ensembles with HMC inference.\\n\\nSeparately from cold posteriors, \\\\citet{izmailov2021dangers} explain how standard priors for Bayesian neural networks can cause profound deteriorations in performance under covariate shift --- \\nwith the potential to impact virtually any real-world application of BNNs. They introduce the input dependent \\\\emph{EmpCov} priors, which helps remedy this issue.\\n\\n\\\\citet{Noci2021DisentanglingTR} argue that many different factors --- likelihoods, priors, data augmentation --- can cause cold posteriors, and that there may be no one cause in general. \\nThese observations are actually aligned with the earlier work by \\\\citet{wilson2020bayesian}, which argues that tempering can partially address a wide range of misspecifications, such that it\\nwould be surprising if $T=1$ in general provided the best performance. In other words, if our model is at all misspecified we would expect $T=1$ to be suboptimal, and since in any realistic\\nsetting our model will not be perfectly specified, despite our best efforts, it is unreasonable to demand that $T=1$ or be particularly alarmed if it is not.\\n\\\\citet{Noci2021DisentanglingTR} further suggest that Gaussian priors may be putting weight on complicated hypotheses, which ends up hurting performance, while \\\\citet{wilson2020bayesian} on the other hand demonstrate how Gaussian priors over parameters induce useful priors in function space. Either way, as above, \\nthe cold posterior effect is nearly removed in practice if we remove data augmentation, including all examples in \\\\citet{wenzel2020good}. \\n\\n\\\\citet{aitchison2020statistical} argue that BNN likelihoods are misspecified since many benchmark datasets have been carefully curated by human labelers; when we rely on a curation strategy that discards any samples which do not arrive at label consensus, our likelihood has the form ${p(y\\\\mid x)^H}$ for $H$ human labelers, which connects to likelihood tempering. They also show that posterior tempering is less helpful in the presence of label noise. \\\\citet{adlam2020cold} also consider model misspecification, showing a cold posterior effect can arise even with exact inference in Gaussian process regression when the aleatoric uncertainty is mis-estimated. \\\\citet{nabarro2021data} modify the likelihood to accommodate data augmentation, using as the observation model the average of the neural network outputs over augmentations, but still find a cold posterior effect --- despite the lack of a cold posterior without augmentation. \\n\\nOur paper makes several distinctive contributions in the context of prior work: (1) we argue that standard likelihoods do not represent our beliefs about aleatoric uncertainty, and that standard benchmarks have essentially no aleatoric uncertainty; (2) we show how tempering and data augmentation specify aleatoric uncertainty, beyond curation; (3) we show the \\\\emph{precise way in which data augmentation with SGLD leads to underconfidence in the likelihood}, a counterintuitive result which finally resolves the empirical connection between data augmentation and cold posteriors; (4) we show that a $T<1$, particularly with data augmentation, is a more honest reflection of our beliefs about aleatoric uncertainty than $T=1$; (5) we show how a lognormal approximation of the Dirichlet likelihood, originally used for tractable Gaussian process classification \\\\citep{Milios2018DirichletbasedGP}, can naturally reflect our beliefs about aleatoric uncertainty, and for the first time remove the cold posterior effect in the presence of data augmentation; (6) we show that priors can also be used to specify our beliefs about aleatoric uncertainty. The code to reproduce experiments is available at \\\\url{https://github.com/activatedgeek/bayesian-classification}.\\n\\n\\\\section{Background}\\n\\\\label{sec:background}\\n\\n\\\\textbf{Bayesian Model Averaging.}\\\\quad With Bayesian inference, we aim to infer the\\nposterior distribution over parameters having observed a \\ndataset ${\\\\dset = \\\\{(x_i,y_i)\\\\}_{i=1}^N}$ of input-output pairs, given by\\n${p(\\\\params\\\\mid \\\\dset) \\\\propto p(\\\\dset \\\\mid \\\\params)p(\\\\params)}$\\nfor a given observation likelihood under the i.i.d. assumption ${p(\\\\dset\\\\mid\\\\params) = \\\\prod_{i=1}^N p(y_i\\\\mid x_i, \\\\params)}$, and prior over parameters $p(\\\\params)$. For any novel input $x_\\\\star$, we estimate the posterior predictive distribution via \\\\emph{Bayesian model averaging} (BMA) given by\\n\\\\begin{align}\\n\\\\begin{split}\\np(y_\\\\star \\\\mid x_\\\\star)\\t&= \\\\int p(y_\\\\star \\\\mid x_\\\\star,\\\\params)p(\\\\params\\\\mid \\\\dset) d\\\\params . \\n\\\\end{split} \\\\label{eq:bma}\\n\\\\end{align}\\nThis integral cannot be expressed in closed form for a Bayesian neural network, and is typically approximated with Variational Inference (VI), the Laplace\\napproximation, or Markov Chain Monte Carlo (MCMC) \\\\citep[e.g.,][]{wilson2020bayesian, pml2Book}. \\n\\n\\\\textbf{Cold Posteriors and Tempering.}\\\\quad Let ${p(\\\\dset \\\\mid \\\\params)}$ denote the likelihood function, $p(\\\\params)$ \\nthe prior over parameters of the neural network, and $U(\\\\params)$ the \\\\emph{posterior energy function}. Following \\\\citet{wenzel2020good}, we then define a \\\\emph{cold \\nposterior} for $T<1$ as\\n\\\\begin{align}\\np_{\\\\mathrm{cold}}(\\\\params \\\\mid \\\\dset) \\\\propto \\\\exp \\\\bigg\\\\{ -\\\\frac{1}{T} \\\\underbrace{ \\\\left( -\\\\log{p(\\\\dset \\\\mid \\\\params)} - \\\\log{p(\\\\params)} \\\\right)}_{U(\\\\params)} \\\\bigg\\\\}, \\\\label{eq:cold_posterior}\\n\\\\end{align}\\nwhich is effectively raises both the likelihood and the prior to a power $1/T$. $T=1$ recovers the standard Bayes' posterior. \\nBy comparison, a \\\\emph{tempered likelihood posterior} (e.g. \\\\citet{grunwald2012safe}) only raises the likelihood term to a power $1/T$ as\\n\\\\begin{align}\\np_{\\\\mathrm{temp}}(\\\\params \\\\mid \\\\dset) \\\\propto \\\\exp\\\\bigg\\\\{- \\\\left(-\\\\frac{1}{T}\\\\log{p(\\\\dset \\\\mid \\\\params)} - \\\\log{p(\\\\params)}\\\\right)\\\\bigg\\\\}. \\\\label{eq:tempered_posterior}\\n\\\\end{align}\\n\\n\\\\textbf{Stochastic Gradient Langevin Dynamics (SGLD).}\\\\quad For large-scale neural \\nnetworks, exact posterior inference remains intractable. To approximate samples from the posterior ${p(\\\\params \\\\mid \\\\dset)}$ over parameters of a neural network, we simulate the time-discretized Langevin stochastic differential equation (SDE) \\\\citep{Srkk2019AppliedSD,welling2011bayesian}\\n\\\\begin{align}\\n\\\\begin{split}\\n\\\\Delta \\\\params &= \\\\mbf{M}^{-1} \\\\mbf{m} \\\\epsilon, \\\\\\\\\\n\\\\Delta \\\\mbf{m} &= - \\\\nabla_\\\\params \\\\widetilde{U}(\\\\params) \\\\epsilon - \\\\gamma \\\\mbf{m} \\\\epsilon + \\\\sqrt{2\\\\gamma T} \\\\eta, \\\\textrm{ where } \\\\eta \\\\sim \\\\gaussian{\\\\mbf{0}, \\\\mbf{M}},\\n\\\\end{split} \\\\label{eq:langevin_sde}\\n\\\\end{align}\\nwhere $\\\\mbf{m}$ are the auxiliary momentum variables, $\\\\mbf{M}$ is\\nthe mass matrix which acts as a preconditioner (often identity), $\\\\gamma$ is the friction parameter, $T$ is the temperature, \\n$\\\\Delta t = \\\\epsilon$ is the time discretization (step size),\\nand $\\\\nabla_\\\\params \\\\widetilde{U}(\\\\params)$ is an unbiased estimator of the gradient $\\\\nabla_\\\\params U(\\\\params)$ using only a subset of the dataset $\\\\dset$ for computational efficiency. \\nFor $\\\\epsilon \\\\to 0$ in the limit of time $t \\\\to \\\\infty$, simulating \\\\cref{eq:langevin_sde} produces a trajectory \\ndistributed according to the stationary distribution \\n${\\\\exp\\\\left\\\\{ -U(\\\\params)/T \\\\right\\\\}}$, which is exactly the posterior\\n$p_{\\\\mathrm{cold}}$ in \\\\cref{eq:cold_posterior} we desire.\\nWhen $\\\\gamma = 0$, \\\\cref{eq:langevin_sde} describes the Stochastic Gradient Langevin Dynamics (SGLD) \\\\citep{welling2011bayesian}, and otherwise it describes the Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) where $1-\\\\gamma$ represents the momentum \\\\citep{cheni14}. \\nFurthermore, we can sample from \\\\cref{eq:tempered_posterior} by setting $T = 1$ in \\\\cref{eq:langevin_sde} and raising only the likelihood to a power $T$. In addition, it is often beneficial to use a cyclical time-stepping schedule for $\\\\epsilon$ in \\\\cref{eq:langevin_sde}, as proposed by \\\\citet{zhang2019cyclical} for cyclical-SGLD (cSGLD) when $\\\\gamma = 0$, and cyclical-SGHMC (cSGHMC) when $\\\\gamma > 0$.\\n\\n\\\\textbf{Bayesian Classification.}\\\\quad For a $C$-class classification problem, a standard choice of observation likelihood is the categorical distribution ${p(y \\\\mid x,\\\\params) = \\\\mathrm{Cat}([\\\\pi_1,\\\\pi_2,\\\\dots,\\\\pi_C])}$, where each class probability is computed using a softmax transform of logits ${f(x;\\\\params) \\\\in \\\\reals^C}$ as $\\\\pi_c \\\\propto \\\\exp\\\\{ [f(x; \\\\params)]_c \\\\}$, and hence called the \\\\emph{softmax likelihood}. The negative log of the softmax likelihood, i.e. $-\\\\log{p(y \\\\mid x,\\\\params)}$ is exactly the standard cross-entropy loss. The prior over parameters $p(\\\\params)$ is often chosen to be an isotropic Gaussian $\\\\gaussian{0, \\\\sigma^2 \\\\mbf{I}}$ with some fixed variance $\\\\sigma^2$. Approximate posterior samples are often obtained with SGLD, and then used to approximate the Bayesian model average in \\\\cref{eq:bma} using simple Monte Carlo.\\n\\n\\\\section{Aleatoric Uncertainty in Bayesian Classification}\\n\\\\label{sec:aleatoric_uncertainty_understanding}\\n\\nWe will now discuss how we represent aleatoric uncertainty in classification (\\\\cref{sec: howaleatoric}), how tempering reduces aleatoric uncertainty (\\\\cref{sec: liktemp}), and how\\nwe can explicitly represent aleatoric uncertainty without tempering in a modified Dirichlet observation model (\\\\cref{sec: ndm}). In \\\\cref{sec:data_aug_effects} we will use these \\nfoundations to show precisely how data augmentation affects our representation of aleatoric uncertainty.\\n\\n\\\\subsection{How do we represent aleatoric uncertainty in classification?}\\n\\\\label{sec: howaleatoric}\\n\\nLet us consider the Bayesian neural network posterior over parameters $w$ in a classification problem:\\n\\\\begin{equation}\\n    \\\\label{eq:posterior}\\n    p(w \\\\mid D)\\n    \\\\propto\\n    p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} f_y(x, w),\\n\\\\end{equation}\\nwhere we denote the output of the softmax layer of the model corresponding to class $y$ on input $x$ as $f_y(x, w)$.\\nWe can think of the class probability vectors $f(x) = (f_1(x, w), \\\\ldots, f_C(x, w))$ as latent variables, where the prior distribution $p(w)$ over\\nthe parameters of the network implies a joint prior distribution over $\\\\{f(x)\\\\}_{x \\\\in \\\\mathcal D}$.\\nWe will initially focus on the observation model.\\nFirst, we note that we can introduce a uniform prior over the predicted class probabilities $f(x)$:\\n\\\\begin{equation}\\n    \\\\label{eq:adding_uniform_prior}\\n    p(y \\\\mid f(x)) = f_y(x) \\\\propto\\n    \\\\text{Dir.}(1, \\\\ldots, 1)(f(x)) \\\\cdot f_y(x),\\n\\\\end{equation}\\nwhere Dir. denotes the Dirichlet distribution and $\\\\text{Dir.}(1, \\\\ldots, 1)$ is a uniform distribution over the class probabilities $f(x)$ (with support on the unit $C$-simplex), and the proportionality is with respect to the latent variables $f(x)$.\\nWe can view the right hand side of Eq. \\\\eqref{eq:adding_uniform_prior} as the unnormalized posterior in a multinomial model, where we use a uniform prior over the the parameters $f(x)$, and observe a single count of class $y$.\\nUsing the fact that the Dirichlet distribution is conjugate to the multinomial likelihood \\\\citep[e.g.,][Ch. 2.2.1]{bishop06}, we can rewrite the right hand side of \\\\cref{eq:adding_uniform_prior} as follows:\\n\\\\begin{equation}\\n    \\\\label{eq:likelihood_dirichlet}\\n    p(y \\\\mid f(x)) \\\\propto\\n    \\\\text{Dir.}(1, \\\\ldots, 1, \\\\underbrace{2}_{\\\\text{position}~y}, 1, \\\\ldots, 1)(f(x)).\\n\\\\end{equation}\\nEq. \\\\eqref{eq:likelihood_dirichlet} describes the distribution induced by the observation $y$ on the predicted class probabilities for the corresponding input $x$.\\nThe posterior over the parameters $w$ of the network can then be written as a product of Eq. \\\\eqref{eq:likelihood_dirichlet} and the prior $p(w)$:\\n\\\\begin{equation}\\n    \\\\label{eq:posterior_dir}\\n    p(w \\\\mid D)\\n    \\\\propto\\n    p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} \\\\text{Dir.}(1, \\\\ldots, 1, \\\\underbrace{2}_{\\\\text{position}~y}, 1, \\\\ldots, 1)(f(x)).\\n\\\\end{equation}\\n\\\\cref{eq:posterior_dir} provides intuition for how Bayesian neural networks estimate aleatoric uncertainty in classification.\\nIf we ignore the prior $p(w)$ and assume that the implied prior over $f(x)$ is uniform, then for an observation $(x, y)$ the posterior over $f(x)$ is Dir.$(1, \\\\ldots, 2, \\\\ldots, 1)$ and the posterior mean for the probability of the correct class $\\\\mathbb E f_y(x)$ is $\\\\frac 2 {C + 1}$.\\nFor example, for a dataset with $100$ classes (e.g. CIFAR-100), the model on average will only be $2 / 101 \\\\approx 2\\\\%$ confident in the correct label on the \\\\textit{training data}!\\n\\nIn practice, the prior $p(w)$ will imply a non-trivial joint prior distribution over the latent variables $\\\\{f(x)\\\\}_{x \\\\in \\\\mathcal D}$, so the actual posterior may be more (or less) confident than suggested by the analysis above.\\nFurthermore, we have very limited understanding of the implied distribution \\\\citep[see][for empirical analysis]{wenzel2020good, wilson2020bayesian}.\\nIn particular, most practitioners use simple $p(w) = \\\\mathcal N(0, \\\\alpha^2 I)$ priors, regardless of the amount of label noise in the data.\\nExplicitly constructing a prior in the parameter space that would lead to highly confident $f(x)$ is challenging, but we will show how modifying the likelihood \\ncan be viewed as providing an input-dependent prior that more fully reflects our beliefs about aleatoric uncertainty.\\n\\nThe expression in \\\\cref{eq:posterior_dir} suggests two natural ways of modifying the posterior to account for the aleatoric uncertainty:\\nincreasing the Dirichet concentration for the observed class (equivalent to posterior tempering) and decreasing the concentration for the unobserved classes (noisy Dirichlet model).\\nBelow, we describe both of these approaches in detail.\\n\\n\\\\subsection{Likelihood tempering reduces aleatoric uncertainty}\\n\\\\label{sec: liktemp}\\n\\nThe tempered likelihood posterior (see \\\\cref{sec:background}) corresponding to the posterior in \\\\cref{eq:posterior} in BNN classification can be written as\\n\\\\begin{equation}\\n    \\\\label{eq:temp_posterior}\\n    p_{temp}(w \\\\mid D)\\n    \\\\propto\\n    p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} f_y(x, w)^{1/T},\\n\\\\end{equation}\\nwhere $T$ is the temperature.\\nAnalogously to the derivation above, we can rewrite this posterior as \\n\\\\begin{equation}\\n    \\\\label{eq:temp_posterior_dirichlet}\\n    p_{temp}(w \\\\mid D)\\n    \\\\propto\\n    p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} \\\\text{Dir.}\\\\bigg(1, \\\\ldots, 1, \\\\underbrace{1 + \\\\frac 1 T}_{\\\\text{position}~y}, 1, \\\\ldots, 1\\\\bigg)(f(x)).\\n\\\\end{equation}\\nIn other words, the tempered posterior corresponds to the same model as the regular posterior, but assumes that we observed $1 / T$ counts of class $y$ for each input $x$ in the training data.\\nIn particular, assuming the prior $p(w)$ implies a uniform distribution over $f(x)$, the confidence in the correct labels on the train data under the tempered posterior is $\\\\mathbb E f_y(x)$ is $\\\\frac {1 + 1 / T} {C + 1 / T} = \\\\frac{T + 1}{C T + 1}$.\\nFor a dataset with $100$ classes and at temperature $T = 10^{-2}$ we get an average confidence of $50.5\\\\%$, much higher than the $2\\\\%$ for the standard observation model.\\n\\n\\\\textbf{Tempering the Likelihood vs Tempering the Posterior.}\\\\quad\\nPrior work has mostly considered tempering the full Bayesian posterior in \\\\cref{eq:cold_posterior} as opposed to just tempering the likelihood in \\\\cref{eq:tempered_posterior}. \\nIn \\\\cref{sec:cp_and_sharpness} we show that tempering the full Bayesian posterior is almost always equivalent to changing the prior distribution, and tempering the likelihood.\\nMoreover, we show that tempering just the likelihood recovers the same cold posterior effect \\\\citep{wenzel2020good} as tempering the full posterior.\\n\\n\\\\textbf{How should we think about tempering?}\\\\quad\\nPrior work has asserted that posterior tempering sharply deviates from the Bayesian paradigm and the cold posterior effect is highly problematic \\\\citep{wenzel2020good,Noci2021DisentanglingTR,fortuin2021bayesian}.\\nWe, on the other hand, argue that likelihood tempering is in fact a practical way to incorporate our assumptions about aleatoric uncertainty.\\nRelatedly, \\\\citet{aitchison2020statistical} argue that the cold posterior effect can be caused by data curation.\\nFor example, CIFAR-$10$ and ImageNet datasets have very relatively little label noise and are carefully curated \\\\citep[e.g.][]{pleiss2020identifying}.\\nThus, we may\\nexpect that tempered likelihoods with low temperatures will lead to optimal performance.\\n\\\\citet{wilson2020bayesian} also argue that posterior tempering can be viewed as a change of the observation model.\\n\\n\\\\textbf{Is Tempered Likelihood a Valid Likelihood?}\\\\quad\\nIn classical Bayesian inference, the observation model is a distribution $p(y \\\\mid x)$ over the labels conditioned on the input.\\n\\\\citet{wenzel2020good} argue that the tempered softmax likelihood is in general not a valid likelihood because \\nit does not sum to $1$ over classes.\\nHowever, \\\\citet{wenzel2020good} show, the tempered softmax likelihood with $T < 1$ can be interpreted as a valid likelihood if we introduce a new class, which is not observed in the training data. While\\nthey discard this interpretation as incoherent, it is not necessarily unreasonable to include an unobserved class in the model.\\nMoreover, from \\\\cref{eq:temp_posterior_dirichlet}, we can naturally interpret the tempered likelihood as using the multinomial observation model, assuming $1 / T$ counts of the label are observed for each of the training datapoints, which is uncontroversial and perfectly valid.\\n\\n\\\\subsection{Noisy Dirichlet model: changing the prior over class probabilities}\\n\\\\label{sec: ndm}\\n\\nAs we have seen in \\\\cref{eq:temp_posterior_dirichlet}, likelihood tempering increases the posterior confidence by increasing the Dirichlet distribution concentration parameter for the observed class $y$ from $2$ to $1 + 1/T$. \\nWe can achieve a similar effect by \\\\textit{decreasing the concentration of the unobserved classes} instead\\\\footnote{\\nWhile for concreteness we discuss increasing the confidence of the model here, we can equivalently decrease the confidence of the model by \\\\textit{increasing} the concentration parameters for the unobserved classes, if we expect the level of label noise to be high.}!\\nIndeed, consider the distribution\\n\\\\begin{equation}\\n    \\\\label{eq:posterior_dir_noisy}\\n    p_{ND}(w \\\\mid D)\\n    \\\\propto\\n    p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} \\\\text{Dir.}(\\\\alpha_{\\\\epsilon}, \\\\ldots, \\\\alpha_{\\\\epsilon}, \\\\underbrace{\\\\alpha_{\\\\epsilon}+1}_{\\\\text{position}~y}, \\\\alpha_{\\\\epsilon}, \\\\ldots, \\\\alpha_{\\\\epsilon})(f(x)),\\n\\\\end{equation}\\nwhere ND in $p_{ND}$ stands for \\\\textit{Noisy Dirichlet} and $\\\\alpha_{\\\\epsilon}$ is a tunable parameter.\\nThe noisy Dirichlet model was originally proposed by \\\\citet{Milios2018DirichletbasedGP} in the context of Gaussian process classification,\\nwhere they designed a tractable approximation to this model.\\nUsing our running example, if $p(w)$ induces a uniform distribution over $f(x)$, for a problem with $100$ classes and $\\\\alpha_{\\\\epsilon} = 10^{-2}$ we have expected confidence \\n$\\\\mathbb E f_y(x) = \\\\frac {\\\\alpha_\\\\epsilon + 1} {C\\\\alpha_\\\\epsilon + 1} = 50.5\\\\%$, which is the same as for the tempered likelihood with $T=10^{-2}$.\\n\\nNow, again using the conjugacy of the Dirichlet and multinomial distributions, we can rewrite $p_{ND}$ as follows:\\n\\\\begin{equation}\\n    \\\\label{eq:posterior_dir_noisy_final}\\n    p_{ND}(w \\\\mid D)\\n    \\\\propto\\n    \\\\overbrace{p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} \\\\text{Dir.}(\\\\alpha_{\\\\epsilon}, \\\\ldots, \\\\alpha_{\\\\epsilon})(f(x))}^{q_{ND}(w)} \\\\cdot f_y(x) = q_{ND}(w) f_y(w).\\n\\\\end{equation}\\nWe can interpret $q_{ND}(w) = p(w) \\\\cdot \\\\prod_{x \\\\in \\\\mathcal D} \\\\text{Dir.}(\\\\alpha_{\\\\epsilon}, \\\\ldots, \\\\alpha_{\\\\epsilon})(f(x))$ as a prior over the parameters $w$ of the model.\\nIndeed, $q_{ND}(w)$ does not depend on the labels $y$, and simply forces the predicted class probabilities to be confident in any one of the classes for the training data.\\nSee the \\\\textit{EmpCov} prior in \\\\citet{izmailov2021dangers} for another example of a prior that depends on the training inputs but not training labels.\\n\\nThe Noisy Dirichlet prior is intuitively appealing: in many practical settings a priori we believe that the aleatoric uncertainty on the training data is low, and the model should be confident in one of the classes.\\nAt the same time, we would not want to enforce low aleatoric uncertainty everywhere in the input space.\\nIndeed, while we believe that the aleatoric uncertainty should be low on the training data, we do not expect that all the possible inputs to the model should be classified with high confidence.\\n\\nIn short, the Noisy Dirichlet model corresponds to using a \\\\textit{valid likelihood} --- the standard softmax likelihood --- with a prior that explicitly enforces the model predictions to be confident on the training data.\\nIn \\\\cref{sec:exp_bnn_images} we will see that the noisy Dirichlet model removes the cold posterior effect: tempering is not needed to achieve strong performance.\\n\\n\\\\textbf{Gaussian Approximation.}\\n\\\\quad\\n\\\\citet{Milios2018DirichletbasedGP} considered the distribution in \\\\cref{eq:posterior_dir_noisy_final} in the context of Gaussian process classification,\\nbut with a different goal:\\nthey aimed to create a regression likelihood which would approximate the softmax likelihood.\\nThey further approximated the Dirichlet distribution $\\\\text{Dir.}(f(x))$ over the class probabilities $f(x)$ with a product of independent Gaussian distributions over the logits $z(x)$:\\n\\\\begin{equation}\\n\\\\begin{split}\\n    \\\\label{eq:posterior_dir_noisy_gaussian}\\n    p_{NDG}(w \\\\mid D)\\n    \\\\propto\\n    p(w) \\\\prod_{x, y \\\\in \\\\mathcal D} \\\\prod_{c=1}^C \\\\mathcal N (z_c(x) \\\\mid \\\\mu_c, \\\\sigma_c^2),~~\\\\text{with} \\\\\\\\\\n    \\\\alpha_c = 1 + \\\\alpha_{\\\\epsilon} \\\\cdot I[c = y],~~\\n    \\\\sigma_c^2 = \\\\log(1 / \\\\alpha_c + 1),~~\\n    \\\\mu_c = \\\\log(\\\\alpha_c) - \\\\frac{\\\\sigma^2_c}{2},\\n\\\\end{split}\\n\\\\end{equation}\\nwhere $I[c = y]$ is the indicator function equal to $1$ for $c = y$ and $0$ for $c \\\\ne y$.\\nHere \\\\emph{NDG} stands for \\\\emph{Noisy Dirichlet Gaussian} approximation.\\nWhile theoretically we do not need to use the Gaussian approximation in \\\\cref{eq:posterior_dir_noisy_gaussian} for Bayesian neural networks and can directly use \\\\cref{eq:posterior_dir_noisy_final}, we found that in practice the approximation is much more stable numerically.\\nIndeed, the approximation in \\\\cref{eq:posterior_dir_noisy_gaussian} amounts to solving a regression problem in the space of logits $z(x)$ with a Gaussian observation model.\\nIn the experiments in this paper we will use the $p_{NDG}$ model.\\n\\n\\\\begin{figure}[!t]\\n    \\\\centering\\n    \\\\includegraphics[width=0.8\\\\linewidth]{figures/coinflip_comparison.pdf}\\n    \\\\caption{\\n    \\\\textbf{Comparison of Bayesian classification models.}\\n    The probability CDF of the posterior over the confidence $f_y$ for the correct class $y$ with the standard cross entropy likelihood,\\n    tempered likelihood and the noisy Dirichlet model in a binary classification problem.\\n    \\\\textbf{Left}: Tempering and noisy Dirichlet models both allow to concentrate the posterior on solutions that are confident in the correct label.\\n    \\\\textbf{Middle}: Lower likelihood temperatures lead to higher concentration on confident solutions.\\n    \\\\textbf{Right}: In the noisy Dirichlet model, lower values of the noise parameter $\\\\alpha_{\\\\epsilon}$ also lead to higher confidence.\\n    }\\n    \\\\label{fig:coinflip}\\n\\\\end{figure}\\n\\n\\\\textbf{Visual comparison.}\\\\quad\\nIn \\\\cref{fig:coinflip} (left), we show the CDF of the posterior distribution over the confidence $f_y$ for the correct class $y$ with the standard softmax likelihood, tempered likelihood and the noisy Dirichlet model for a binary classification problem.\\nHere, we assume that the prior over $\\\\{f(x)\\\\}_{x \\\\in \\\\mathcal D}$ is uniform and independent across $x$ for the purpose of visualization.\\nBoth tempering and the noisy Dirichlet model indeed push the predictions $f_y$ to be much more confident in the correct class $y$ compared to the standard softmax likelihood.\\nIn \\\\cref{fig:coinflip} (middle) we show the effect of the value of temperature: lower temperatures correspond to more confident predictions.\\nSimilarly, in \\\\cref{fig:coinflip} (right) lower values of $\\\\alpha_\\\\epsilon$ lead to more confident predictions in the noisy Dirichlet model.\\nThe Gaussian approximation NDG matches the CDF of the noisy Dirichlet model well, but smooths it slightly, making it more amenable to numerical sampling.\\n\\n\\n\\\\section{The Effect of Data Augmentation}\\n\\\\label{sec:data_aug_effects}\\n\\nData augmentation is a key ingredient of modern deep learning.\\nSurprisingly, however, data augmentation has not been considered in detail in the context of Bayesian methods until recently.\\nSeveral works have shown that naive data augmentation can often cause the cold posterior effect in Bayesian deep learning \\\\citep{izmailov2021bayesian,zeno2020cold,fortuin2021bayesian, nabarro2021data}.\\nWe show that naive data augmentation, as typically applied in practice, is closely connected to likelihood tempering with a temperature $T > 1$.\\nIn this case, the cold posteriors counterbalance the effect of data augmentation.\\n\\nData augmentation is a practical procedure, where at each iteration of optimization or sampling, we draw random augmentation for each object in each mini-batch of the data.\\nConsequently, we can only reason about the effect of the data augmentation on the limiting distributions of approximate inference procedures:\\ndata augmentation is not defined for \\\\textit{true} Bayesian neural networks.\\n\\nConsider  the interaction between data augmenation and SGLD (or, equivalently, any other SGMCMC method). In SGLD, we aim to construct an unbiased estimator of the full gradient required in \\\\cref{eq:langevin_sde}. \\nIn prior work, e.g. in \\\\citet{wenzel2020good}, \\nthe stochastic gradient is estimated using a randomly augmented mini-batch of the data.\\nFor a minibatch of\\nthe full dataset $\\\\dset_m = \\\\{x_i,y_i\\\\}_{i=1}^m \\\\subset \\\\dset$, and a finite set of augmentations $\\\\mathcal{T} = \\\\{ t_1,\\\\dots,t_K \\\\}$, \\nthis stochastic gradient is given by\\n\\\\begin{align}\\n\\\\begin{split}\\n\\\\nabla \\\\widetilde{U}(\\\\params) =&~ \\\\frac{N}{m} \\\\sum_{(x_i,y_i) \\\\in \\\\dset_m} \\\\nabla_\\\\params \\\\log{p(y_i \\\\mid t_j(x_i))}\\n+ \\\\nabla_\\\\params \\\\log{p(\\\\params)},\\n\\\\end{split}\\\\label{eq:stochastic_gradient}\\n\\\\end{align}\\nwhere the transformations $t_j$ are sampled uniformly from $\\\\mathcal{T}$.\\n\\nThere are two sources of randomness in $\\\\nabla \\\\widetilde{U}(\\\\params)$ --- the choice of the mini-batch $\\\\dset_m$ and the choice of augmentation $t_j$ used for each $x_i$. The limiting distribution of SGLD is determined by the expectation of the stochastic gradient in \\\\cref{eq:stochastic_gradient}, which is given by\\n\\\\begin{align}\\n\\\\begin{split}\\n\\\\mathbb{E}[\\\\nabla \\\\widetilde{U}(\\\\params)] &= \\\\sum_{i=1}^N \\\\mathbb{E}_{t_j \\\\sim \\\\mathcal{T}}[\\\\nabla_\\\\params \\\\log{p(y_i \\\\mid t_j(x_i))}] + \\\\nabla_\\\\params \\\\log{p(\\\\params)}, \\\\\\\\\\n&= \\\\sum_{i=1}^N \\\\sum_{j=1}^K [\\\\nabla_\\\\params \\\\log{p(y_i \\\\mid t_j(x_i))^{1/K}}] + \\\\nabla_\\\\params \\\\log{p(\\\\params)},\\n\\\\label{eq:aug_sgld_grad_expectation}\\n\\\\end{split}\\n\\\\end{align}\\nwhere $t_j \\\\sim \\\\mathcal{T}$ are augmentations sampled uniformly from $\\\\mathcal{T}$.\\nTherefore, we conclude that the limiting distribution of SGLD under data augmentation is given by \\n\\\\begin{align}\\np_{\\\\mathrm{aug}}(\\\\params\\\\mid \\\\dset) \\\\propto p(\\\\params)\\\\prod_{i=1}^N\\\\prod_{j=1}^K p(y_i \\\\mid t_j(x_i))^{1/K}.\\n\\\\label{eq:implied_posterior}\\n\\\\end{align}\\n\\nWe can interpret the limiting distribution in \\\\cref{eq:implied_posterior} as a \\\\emph{tempered likelihood posterior} for a new dataset $\\\\dset^\\\\prime = \\\\{(t_j(x_i), y_i)\\\\}_{i, j}$ which contains all augmentations of every data point from the original dataset $\\\\dset$.\\nFurthermore, the likelihood is tempered with a temperature $K > 1$, corresponding to a \\\\emph{warm posterior}.\\nIn other words, the limiting distribution of SGLD with data augmentation corresponds to a posterior over the augmented dataset with a \\\\emph{softened} likelihood, leading to less confidence about the labels.\\nBy applying a cold temperature $T = 1 / K$ to the posterior in \\\\cref{eq:implied_posterior}, we can recover the standard Bayesian posterior on the augmented dataset $\\\\dset^\\\\prime$.\\n\\nIn \\\\cref{sec:exp_bnn_images}, we explore the effect of data augmentation on aleatoric uncertainty in practice and show that it does indeed soften the likelihood and lead to increased aleatoric uncertainty.\\n\\n\\\\textbf{What about Other Inference Procedures?}\\\\quad\\nWhile the derivation in \\\\cref{eq:implied_posterior} comes directly from studying the stochastic gradient evaluated in SGMCMC, we can equivalently derive the same posterior distribution in variational inference as we show in \\\\cref{app:alt_inferences}. \\n\\n\\\\textbf{Should we always use temperature $T=1/K$?}\\\\quad\\nWhile using the temperature $T = 1 / K$ recovers the standard Bayesian posterior on the augmented dataset $\\\\dset'$, it is not necessarily a correct approach to modeling data augmentation.\\nIndeed, the standard posterior  $p(\\\\params \\\\mid \\\\dset') \\\\propto p(\\\\params)\\\\prod_{i=1}^N\\\\prod_{j=1}^K p(y_i \\\\mid t_j(x_i))$ assumes indepdendence across both augmentations and data points.\\nIn practice, however, we share the same label $y_i$ across all the augmented versions of the image; treating the observations as independent then leads to \\\\textit{underestimating} the aleatoric uncertainty.\\nIndeed, consider the extreme scenario, where all the augmentations $t_j(x)$ simply return $x$.\\nIn this case, treating the observations $(t_j(x), y)$ as independent will simply raise the likelihood to a power of $K$, while in reality we have not received any additional information from the augmentation policy.\\nCorrecting the likelihood by a factor of $1/K$ is the correct approach only when the predictions on $t_j(x)$ are completely independent from each other (see \\\\cref{sec:data_aug_gp}).\\nSee also \\\\citet{nabarro2021data} for a related discussion.\\n\\n\\\\textbf{A Proper Augmentation Likelihood.}\\\\quad\\nAs we have noted, simple tempering does not model the augmented data perfectly, as it ignores the dependencies between the observations. \\nIn \\\\cref{sec:app_aug_lik}, we develop a proper likelihood that correctly accounts for data augmentation. \\nIn our experiments, however, we found that simply sharpening the likelihood via tempering or the noisy Dirichlet model is sufficient, and the proper augmentation likelihood does not provide significant benefits in practice. \\nIn particular, we found the proper augmentation likelihood much harder to sample from with standard SGMCMC samplers, compared to the standard likelihood.\\nWe leave a detailed exploration of the augmentation likelihood for future work.\\n\\\\citet{nabarro2021data} also derived a valid likelihood for data augmentation: in their model, the predictions are averaged across all data augmentations of a given datapoint.\\n\\n\\\\section{Experiments}\\n\\\\label{sec:experiments}\\n\\nIn this section we provide empirical support for the observations presented in \\\\cref{sec:aleatoric_uncertainty_understanding,sec:data_aug_effects}.\\nFirst, in \\\\cref{sec:exp_synthetic} we illustrate the effects of tempering, noisy Dirichlet model and data augmentation using a Bayesian neural network on a synthetic 2-D classification problem.\\nNext, in \\\\cref{sec:data_aug_gp} we visualize the effect of data augmentation on the limiting distribution of SGMCMC using a Gaussian process regression model.\\nFinally, in \\\\cref{sec:exp_bnn_images} we report the results for BNNs on image classification problems.\\n\\n\\\\begin{figure*}[!t]\\n\\t\\\\centering\\n\\t\\\\begin{subfigure}{0.24\\\\linewidth}\\n\\t\\t\\\\captionsetup{font=small}\\n\\t\\t\\\\includegraphics[height=1.1\\\\textwidth]{figures/fig1_ce_noaug_fit.pdf}\\n\\t\\t\\\\caption{Softmax Lik. (SL)}\\n\\t\\t\\\\label{fig:da_crossent}\\n\\t\\\\end{subfigure}\\n% \\t\\\\hspace{0.1cm}\\n\\t\\\\begin{subfigure}{0.24\\\\linewidth}\\n\\t\\t\\\\captionsetup{font=small}\\n\\t\\t\\\\includegraphics[height=1.1\\\\textwidth]{figures/fig1_ce_aug_fit.pdf}\\n\\t\\t\\\\caption{SL+Data Aug.(DA)}\\n\\t\\t\\\\label{fig:da_crossent_da}\\n\\t\\\\end{subfigure}\\n% \\t\\\\hspace{0.1cm}\\n\\t\\\\begin{subfigure}{0.24\\\\linewidth}\\n\\t\\t\\\\captionsetup{font=small}\\n\\t\\t\\\\includegraphics[height=1.1\\\\textwidth]{figures/fig1_ce_aug_cold_fit.pdf}\\n\\t\\t\\\\caption{SL+DA+Tempering}\\n\\t\\t\\\\label{fig:da_crossent_da_tempering}\\n\\t\\\\end{subfigure}\\n% \\t\\\\hspace{0.1cm}\\n\\t\\\\begin{subfigure}{0.24\\\\linewidth}\\n\\t\\t\\\\captionsetup{font=small}\\n\\t\\t\\\\includegraphics[height=1.1\\\\textwidth]{figures/fig1_dirichlet_aug_fit.pdf}\\n\\t\\t\\\\caption{Noisy Dirichlet+DA}\\n\\t\\t\\\\label{fig:da_dirichlet_da}\\n\\t\\\\end{subfigure}\\n\\t\\\\caption{\\n\\t\\t\\\\textbf{Insufficient Sharpness of Softmax Likelihood.}\\n\\t\\tDecision boundary of BNN classifiers on a synthetic problem.\\n\\t\\tIn each panel, the decision boundary is shown with a black line, datapoints are shown with magenta (class 0) and red (class 1) circles.\\n\\t\\tWe consider data augmentation -- random flip about the axes shown with blue lines; the augmented datapoints are shown semi-transparently.\\n\\t\\tWe use HMC to approximate the posterior in each of the panels.\\n\\t\\t\\\\textbf{(a)} A standard softmax likelihood provides a reasonable but imperfect fit of the training data.\\n\\t\\t\\\\textbf{(b)} Adding data augmentation leads to a more diffuse likelihood which is underconfident, and therefore an even worse fit.\\n\\t\\t\\\\textbf{(c)} Tempering with $T=0.1$ sharpens the softmax likelihood leading to a perfect fit on the train data.\\n\\t\\t\\\\textbf{(d)} A noisy Dirichlet model, provides a similar fit without the need for tempering.\\n\\t}\\n\\t\\\\label{fig:aleatoric_pitfalls}\\n\\\\end{figure*}\\n\\n\\n\\\\subsection{Synthetic problem}\\n\\\\label{sec:exp_synthetic}\\n\\nWe generate the data from a modified version of the two spirals dataset \\\\citep[see e.g.][]{maddox2020rethinking}, where we restrict the data to the $x_1 <0, x_2 <0$ quadrant.\\nThe exact code used to generate the data is available in the \\\\cref{sec:app_exp_synthetic}.\\n\\nIn \\\\cref{fig:aleatoric_pitfalls}(a) we visualize the data and the decision boundary of a Bayesian neural network.\\nFor the Bayesian neural network, we use an iid Gaussian prior $\\\\mathcal N(0, 0.3^2)$ over the parameters of the model;\\nwe use full batch Hamiltonian Monte Carlo (HMC) to sample from the posterior, following \\\\citet{izmailov2021bayesian}.\\nWith the chosen prior, the model is not able to fit the data perfectly, but provides a reasonable fit of the training data.\\n\\nIn \\\\cref{fig:aleatoric_pitfalls}(b) we consider the effect of data augmentation.\\nSpecifically, we apply random flips about the $x_1$ and $x_2$ axes.\\nIn the figure, the augmented datapoints are shown semi-transparently.\\nTo evaluate the network under data augmentation, we run HMC to sample from the posterior distribution in \\\\cref{eq:implied_posterior}.\\nThe Bayesian neural network provides a lower quality fit to the data compared to the fit without data augmentation shown in \\\\cref{fig:aleatoric_pitfalls}(a).\\nIndeed, according to our analysis in \\\\cref{sec:data_aug_effects}, the observation model is softened by the data augmentation, leading the model to fit the training data poorly.\\n\\nAccording to \\\\cref{sec:aleatoric_uncertainty_understanding}, we can sharpen the model leading to a much better fit on the train data by using the tempered likelihood as in \\\\cref{fig:aleatoric_pitfalls}(c) or the noisy Dirichlet observation model as in \\\\cref{fig:aleatoric_pitfalls}(d).\\nWith both of these approaches we achieve a near-perfect fit on the training data, which is desirable if we assume low aleatoric unceratinty.\\nFor further details on this experiment, please see \\\\cref{sec:app_exp_synthetic}.\\n\\n\\\\subsection{Visualizing the Effect of Data Augmentation}\\n\\\\label{sec:data_aug_gp}\\n\\n\\\\begin{figure}[!t]\\n    \\\\centering\\n    \\\\includegraphics[width=.9\\\\linewidth]{figures/augmentation_gp_v2}\\n    \\\\caption{\\n    \\\\textbf{Effect of data augmentation on GP regression.}\\n    As we increase the number $K$ of augmentations of the dataset, a GP regression model fit becomes more diffuse.\\n    Here the original training data is shown with black stars, and the augmented datapoints $t_j(x) = x + \\\\tau_j$ that are obtained by shifting the inputs $x$ by a large constant $\\\\tau_j$ are not shown.\\n    Note that both the predictive mean and confidence change with the number $K$ of augmentations.\\n    }\\n    \\\\label{fig:augmentation_shift_gp}\\n\\\\end{figure}\\n\\nTo illustrate our analysis in \\\\cref{sec:data_aug_effects} we visualize the posterior in \\\\cref{eq:implied_posterior} under data augmentation in a Gaussian process regression model.\\nWe construct a Gaussian process \\\\citep[GP][]{rasmussen2006gaussian} with a truncated RBF kernel such that $k(x,x^\\\\prime) = 0$ when $\\\\norm{x-x^\\\\prime} > \\\\delta$, where $\\\\delta$ is the lengthscale of the kernel.\\nConsequently, the implied correlations between inputs beyond the distance $\\\\delta$ are zero.\\nWe use an augmentation policy such that an augmented sample is given by $t_j(x) = x + \\\\tau_j$, for some set of vectors $\\\\{\\\\tau_j\\\\}_{j=1}^K$ such that  $\\\\norm{\\\\tau_j} \\\\gg \\\\delta$.\\nTherefore we assume that $k(x, t_j(x')) = 0$ for all datapoints $x, x'$ and all augmentations $t_j$.\\nIn other words, the outputs of the Gaussian process on the training data are independent from the outputs on the augmented datapoints. \\n\\nWe visualize the posterior in \\\\cref{eq:implied_posterior} for $K=1, 4$ and $10$ augmentations per datapoint in \\\\cref{fig:augmentation_shift_gp}.\\nIn the visualization, we show the posterior just on the original data, and do not show the augmented datapoints.\\nAs the predictions on the train data are independent from the predictions on the augmented datapoints, the posterior in \\\\cref{eq:implied_posterior} corresponds to tempering the posterior on the original training data with a warm temperature equal to the number $K$ of augmentations.\\nAs a result, increasing the number of augmentations softens the likelihood, leading to a less confident fit on the training data.\\nIn the next section, we show similar underconfidence on the training data for Bayesian neural networks under data augmentation.\\n\\n\\n\\n\\\\subsection{Image Classification with Bayesian Neural Networks}\\n\\\\label{sec:exp_bnn_images}\\n\\nIn this section we experimentally verify the results of the analysis in \\\\cref{sec:aleatoric_uncertainty_understanding,sec:data_aug_effects} for Bayesian neural networks in image classification problems.\\nFirst, we show that the noisy Dirichlet model does not require tempering to achieve optimal performance.\\nThen, we show that both the noisy Dirichlet model and tempered softmax likelihood can be successfully used to express our beliefs about the amount of label noise in the data.\\nFinally, we show that data augmentation softens the likelihood for BNNs, and that the optimal temperature depends on the complexity of the data augmentation policy.\\n\\nFor all experiments we use a ResNet-18 model \\\\citep{he2016identity} and the CIFAR-10 \\\\citep{Krizhevsky2009LearningML} and Tiny Imagenet \\\\citep{Le2015TinyIV} datasets.\\nWe use the SGLD sampler with a cyclical learning rate schedule \\\\citep{welling2011bayesian, zhang2019cyclical} to sample from the posterior.\\nWe provide details on the hyper-parameters in \\\\cref{app:experimental_details}.\\n\\n\\n\\\\begin{wrapfigure}{R}{.4\\\\textwidth}\\n    \\\\vspace{-1em}\\n    \\\\centering\\n    \\\\includegraphics[width=.9\\\\linewidth]{figures/reduced_cpe.pdf}\\n    \\\\caption{\\n    BMA test accuracy for the noisy Dirichlet model with noise parameter $\\\\alpha_\\\\epsilon = 10^{-6}$ and the softmax likelihood\\n    as a function of posterior temperature on CIFAR-10.\\n    The noisy Dirichlet model shows no cold posterior effect.\\n    }\\n    \\\\label{fig:no_cpe}\\n\\\\end{wrapfigure}\\n\\\\textbf{No cold posterior effect in the noisy Dirichlet model.}\\n\\\\quad\\nFirst, we explore the effect of posterior tempering on the standard softmax classification likelihood and the noisy Dirichlet model with noise parameter $\\\\alpha_\\\\epsilon=10^{-6}$.\\nIn \\\\cref{fig:no_cpe} we show the results on the CIFAR-10 dataset.\\nAs reported by \\\\citet{wenzel2020good}, for the standard softmax likelihood, tempering is required to achieve optimal performance, with $T=10^{-3}$ providing the best results.\\nFor the noisy Dirichlet model on the other hand, tempering does not significantly improve the results with roughly constant performance across different temperature values.\\nIn particular, the noisy Dirichlet model achieves near-optimal results at temperature $1$!\\nThese results agree with our analysis in \\\\cref{sec:aleatoric_uncertainty_understanding}: both tempering and the noisy Dirichlet observation model are alternative ways of expressing our beliefs about the aleatoric uncertainty in the data;\\nwith the noisy Dirichlet model, we can achieve strong results without any need for tempering.\\n\\n\\n\\\\begin{figure}[!ht]\\n\\\\centering\\n\\\\begin{tabular}{cc}\\n    \\\\includegraphics[width=.47\\\\linewidth]{figures/c10_label_noise_acc_nll}\\n    &\\n    \\\\includegraphics[width=.47\\\\linewidth]{figures/ti_label_noise_acc_nll}\\n\\\\\\\\\\n   (a) CIFAR-10 & (b) Tiny Imagenet\\n\\\\end{tabular}\\n\\\\caption{\\n\\\\textbf{Label noise in BNN image classification.}\\nBMA test accuracy and negative log likelihood for the standard softmax, tempered softmax and noisy Dirichlet model \\n\\\\textbf{(a)} on CIFAR-10 and \\\\textbf{(b)} Tiny Imagenet.\\nAccounting for the label noise via the noisy Dirichlet model or the tempered softmax likelihood significantly improves accuracy accross the board.\\nMoreover, the optimal performance is achieved by different values of temperature $T^{*}$ in the tempered softmax likelihood or noise $\\\\alpha_\\\\epsilon^{*}$ parameter in the noisy Dirichlet model, i.e.\\nno one model can describe the data optimally across all levels of label noise.\\n}\\n\\\\label{fig:noisy_dirichlet_perf_cifar_labelnoise}\\n\\\\end{figure}\\n\\n\\\\textbf{Modeling label noise.}\\\\quad\\nIn classification problems the aleatoric uncertainty corresponds to the amount of label noise present in the data.\\nThroughout the paper we argued that aleatoric uncertainty is misrepresented by standard Bayesian neural networks, and that\\nlikelihood tempering and the noisy Dirichlet model are compelling approaches to incorporate information about the amount of label noise.\\nIn \\\\cref{fig:noisy_dirichlet_perf_cifar_labelnoise}, we show the BMA test accuracy and negative log-likelihood for the standard softmax likelihood, tempered softmax likelihood and the noisy Dirichlet model on CIFAR-10 and Tiny Imagenet under varying amounts of label noise.\\nWe plot the results for the best performing temperature $T \\\\in \\\\{10^{-5},10^{-4},10^{-3},10^{-2},10^{-1},1,3,10\\\\}$ for the standard softmax likelihood or noise $\\\\alpha_\\\\epsilon \\\\in \\\\{ 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}  \\\\}$ for the noisy Dirichlet model.\\nBoth on CIFAR-10 and on Tiny Imagenet and across all values of label noise, we can significantly improve performance over the standard softmax likelihood by explicitly modeling the aleatoric uncertainty with either tempering or the noisy Dirichlet model.\\nMoreover, different amounts of label noise require different values of the temperature or $\\\\alpha_\\\\epsilon$ parameter.\\n\\nIn \\\\cref{fig:noisy_dirichlet_perf_cifar_labelnoise}(b), for Tiny Imagenet the noisy Dirichlet model provides a significant improvement in performance compared to the tempered softmax likelihood for high levels of label noise.\\nConsequently, one may wonder whether the noisy Dirichlet model should become the default for Bayesian classification.\\nGenerally, no one approach is optimal across the board: we should pick the model that best desribes our beliefs about the data, which may be expressed by tempering the softmax likelihood, the noisy Dirichlet model or another approach.\\n\\n\\\\textbf{Data augmentation leads to underfitting on train data.}\\n\\\\quad\\nNext, we verify the results from \\\\cref{sec:data_aug_effects} for Bayesian neural networks in image classification.\\nIn \\\\cref{fig:train_diffuse_aug_lik}(a,b) we show the BMA train negative log-likelihood for the models trained with and without data augmentation.\\nAcross a wide range of temperatures for the tempered softmax and $\\\\alpha_\\\\epsilon$ parameters for the noisy Dirichlet model,\\nadding the data augmentation reduces the quality of the fit on the original training data.\\nThis result is analogous to the results presented in \\\\cref{sec:data_aug_gp} and agrees with our analysis in \\\\cref{sec:data_aug_effects}: data augmentation softens the likelihood, leading to a more diffuse fit.\\n\\n\\\\textbf{Complex augmentations require lower temperatures.}\\n\\\\quad\\nFinally, we explore the effect of the number $K$ of data augmentations applied to each datapoint.\\nIn \\\\cref{sec:data_aug_effects} we showed that under data augmentation the likelihood is effectively tempered with a temperature $K$, assuming the predictions on the augmentated datapoints are completely independent from the original datapoints.\\nIn practice however, the augmentations can be close to the original images, and the corresponding predictions can be highly correlated.\\nIn \\\\cref{fig:train_diffuse_aug_lik}(c), we consider five separate types of augmentations for ResNet-$18$ on CIFAR-$10$ at various posterior temperatures:\\nhorizontal and vertical flips, random crops, combinations of flips and crops, and  AugMix \\\\citep{hendrycks2019augmix} --- an augmentation policy employing a very diverse set of augmentations.\\nWe find that, as predicted by the analysis in \\\\cref{sec:data_aug_effects}, the optimal temperatures are different for different augmentation policies:\\nin terms of test NLL, the simple policies (vertical and horizontal flips) corresponding to $K = 2$ work best at warmer temperatures $T=1$, \\nintermediate policies (crops and crops$+$flips) corresponding to $K \\\\approx 100$ require lower temperatures $T \\\\in [10^{-2}, 10^{-1}]$,\\nand the most complex AugMix policy requires the lowest temperature $T \\\\le 10^{-4}$.\\n\\n\\\\begin{figure}[!t]\\n\\\\centering\\n\\\\begin{tabular}{cc}\\n    \\\\includegraphics[width=0.48\\\\linewidth]{figures/aug_noaug_train_lik} &\\n    \\\\includegraphics[width=0.49\\\\linewidth]{figures/dataaug_counts}\\n    \\\\\\\\\\n    (a) Softmax \\\\& Noisy Dirichlet & (b) Augmentations vs Tempering\\n\\\\end{tabular}\\n\\n\\\\caption{\\n\\\\textbf{Effect of data augmentation on BNN image classification.}\\nFor all plots we use ResNet-18 on the CIFAR-10 dataset.\\n\\\\textbf{(a)}: The BMA negative log-likelihood on the train data for the tempered softmax model across different temperatures $T$ and the noisy Dirichlet model across different values of noise $\\\\alpha_\\\\epsilon$, both with and without data augmentation.\\nAs predicted in \\\\cref{sec:data_aug_effects}, data augmentation softens the likelihood, leading to a more diffuse fit on the train data.\\nTempering or reducing the noise parameter $\\\\alpha_\\\\epsilon$ is needed to counteract the effect of data augmentation.\\n\\\\textbf{(b)} BMA test accuracy and NLL for various augmentation policies.\\nAs predicted in \\\\cref{sec:data_aug_effects}, more complex policies corresponding to a higher effective number of augmentations $K$ require lower temperatures for optimal performance.\\n}\\n\\\\label{fig:train_diffuse_aug_lik}\\n\\\\end{figure}\\n\\n\\\\section{Discussion}\\n\\nA correct representation of aletoric uncertainty is crucial to achieve strong performance with Bayesian neural networks (BNNs).\\nStandard Bayesian classifiers have no explicit mechanism to represent our beliefs about aleatoric uncertainty, which can lead to \\ninadequate fits to the training data.\\nThis effect is exaggerated by data augmentation, which softens the likelihood, making the models underconfident on the training data.\\nPosterior tempering is a simple and practical way to correct for this misspecification and express our beliefs about the aleatoric uncertainty:\\nmost benchmark datasets have very low levels of label noise, and we can express this belief by tempering the softmax likelihood.\\nWe also showed that we can achieve a similar effect without tempering, by using a prior that forces the model to be confident on the training datapoints with the noisy Dirichlet model.\\n\\nFor practitioners, for any classification problem with Bayesian neural networks, we recommend to use one of the mechanisms we discussed to incorporate beliefs \\nabout aleatoric uncertainty. We have found both tempered softmax likelihood and noisy Dirichlet model highly effective both for data with low and high levels of label noise.\\n\\nThis work is generally about representing aleatoric uncertainty in Bayesian classification, and precisely how data augmentation counterintuitively reduces\\nthe confidence in the observation labels. However, we also resolve many observations about the cold posterior effect in Bayesian neural networks, \\nwhich we now discuss in the context of recent work.\\n\\n\\\\textbf{\\\\citet{Noci2021DisentanglingTR}: Data augmentation is sufficient but not necessary for CPE.}\\n\\\\citet{Noci2021DisentanglingTR} confirm the findings in \\\\citet{izmailov2021bayesian} and \\\\citet{fortuin2021bayesian} that data augmentation plays a significant role in the cold posterior effect.\\nIn particular, \\\\citet{izmailov2021bayesian} show that removing data augmentation from the models in \\\\citet{wenzel2020good} alleviates the cold posterior effect in all of their examples.\\nHowever, \\\\citet{Noci2021DisentanglingTR} also show that it is sometimes possible to achieve CPE without data augmentation, for example by subsampling the dataset. Similarly, \\n\\\\citet{wilson2020bayesian} note that many types of misspecification could lead to a cold posterior effect.\\n\\nIn our work, we provide a more nuanced perspective on the cold posterior effect and data augmentation:\\ndata augmentation leads to a poor representation of aleatoric uncertainty, which can be addressed by tempering the posterior.\\nWe note that BNNs can misrepresent aleatoric uncertainty even without data augmentation, and can have a reasonable representation of aleatoric uncertainty in the presence of data augmentation.\\nHowever, in the vast majority of practical scenarios where the cold posterior effect has been demonstrated, it appears due to the use of data augmentation. Thus data augmentation is \\n\\\\emph{largely} responsible for the cold posterior effect, and we have shown here that this is precisely because augmentation changes our representation of aleatoric uncertainty to be \\nunderconfident, and tempering can correct for this underconfidence, more honestly representing our beliefs.\\n\\nFinally, \\\\citet{izmailov2021dangers} show that BNNs have issues under covariate shift, due to the posterior not contracting sufficiently along some directions in the parameter space.\\nThe same issue occurs when BNNs are applied to extremely small datasets, which may affect the results on data subsampling presented in \\\\citet{Noci2021DisentanglingTR}. \\n\\n\\\\textbf{\\\\citet{nabarro2021data}: the lack of a CPE without DA is an artifact of using the wrong model (i.e. without DA).}\\\\quad\\nSimilar to our discussion in \\\\cref{sec:data_aug_effects}, \\\\citet{nabarro2021data} point out that tempering does not always provide a principled approach to data augmentation.\\nThey instead devise a new observation model, where the model outputs are averaged over all the augmented datapoints.\\nWith this observation model, they still observe the cold posterior effect.\\nThese results are interesting in that they show that a valid likelihood is not sufficient to remove the cold posterior effect.\\nHowever, these results do not rule out the possibility that the proposed likelihood is a poor description of the data generation process: indeed, in reality the labels for the training image are not produced by considering all the possible augmentations of these images.\\n\\nGenerally, our results suggest that the cold posterior effect is caused by a poor representation of the aleatoric uncertainty in the BNNs.\\nIt can happen with or without data augmentation, with valid or invalid likelihoods. A valid likelihood is not necessarily a well-specified likelihood.\\nHowever, we believe that the interpretation that the lack of CPE in models without data augmentation is an artifact of miss-specification is highly questionable:\\nwhen our beliefs about the aleatoric uncertainty in the data are correctly captured by the model, we should not need posterior tempering to achieve strong results;\\nit is the need for tempering which is an indication of miss-specification.\\n\\n\\\\textbf{\\\\citet{Noci2021DisentanglingTR}: Cold posteriors are unlikely to arise from a single simple cause.}\\\\quad\\n\\\\citet{Noci2021DisentanglingTR} show examples, where the cold posterior effect arises in isolation from data augmentation, data curation, or prior misspecification.\\nConsequently, they argue that no one cause can fully explain the cold posterior effect.\\nWhile we agree with this general argument, which is also aligned with the discussion in \\\\citet{wilson2020bayesian}, we note that all of the considered causes are directly related to aleatoric uncertainty.\\nIndeed, in this work we have shown that data augmentation significantly affects the level of aleatoric uncertainty assumed by the model.\\nThe data curation is also directly connected to aleatoric uncertainty, as curated datasets are expected to have low label noise.\\nFinally, in BNNs the prior over the parameters specifies the assumptions about aleatoric uncertainty, and tempering can be used to correct for the effect of the prior.\\nWe believe that our results in this paper provide a compelling explanation for the observations in \\\\citet{Noci2021DisentanglingTR}.\\n\\n\\\\textbf{Summary.} Overall, properly representing aleatoric uncertainty is a challenging but fundamentally important consideration in Bayesian classification. We have shown that posterior tempering provides a mechanism\\nto more honestly represent our beliefs about aleatoric uncertainty, especially in the presence of data augmentation. In general, as in \\\\citet{wilson2020bayesian}, we should not be alarmed if $T=1$ is not \\noptimal in sophisticated models on complex real-world datasets. Moreover, we have shown how other mechanisms to represent aleatoric uncertainty, such as the noisy Dirichlet model, do not suffer from a\\ncold posterior effect in the presence of data augmentation. Indeed, while an interesting phenomenon, cold posteriors should not be conflated with the success or failure of Bayesian deep learning. In general, \\napproximate inference in Bayesian neural networks has been making great strides forward, often providing better performance at a comparable computational cost to standard methods. However, there\\n\\\\emph{are} practical challenges to the adoption of Bayesian deep learning. For example, \\\\citet{izmailov2021dangers} shows that Bayesian neural networks can profoundly degrade in performance under\\na wide range of relatively minor distribution shifts --- behaviour which could affect applicability on virtually any real-world problem, since train and test rarely come from exactly the same distribution. While their \\\\emph{EmpCov} prior provides a partial remedy, there is still much work to be done. For example one could develop priors that protect against covariate shift by accounting for linear dependencies in the internal representations of the network.\\n\\n\\\\textbf{Acknowledgments.} \\nWe would like to thank Micah Goldblum and Wanqian Yang for helpful comments. This research is supported by an Amazon Research Award, NSF I-DISRE 193471, NIH R01DA048764-01A1, NSF IIS-1910266, and NSF 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science.\\nWe also thank NYU IT High Performance Computing for providing the infrastructure to run our experiments.\\n\\n\\\\vspace{-5mm}\\n\\\\bibliography{references}\\n\\n\\\\clearpage\\n\\\\input{supp}\\n\\n\\\\end{document}\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad78240-391a-4993-a962-d579915b189f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
