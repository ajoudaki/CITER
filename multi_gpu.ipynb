{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12fd09-153c-4b73-8cee-a281d7074468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataParallel(nn.Module):\n",
    "    \"\"\"Custom DataParallel implementation for more efficient multi-GPU training.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device_ids=None):\n",
    "        super().__init__()\n",
    "        if device_ids is None:\n",
    "            device_ids = list(range(torch.cuda.device_count()))\n",
    "        self.device_ids = device_ids\n",
    "        self.num_gpus = len(device_ids)\n",
    "        \n",
    "        # Create a model copy for each GPU\n",
    "        self.models = nn.ModuleList([\n",
    "            copy.deepcopy(model).to(f'cuda:{device_id}')\n",
    "            for device_id in device_ids\n",
    "        ])\n",
    "        \n",
    "        # Sync initial parameters across all models\n",
    "        self._sync_params()\n",
    "    \n",
    "    # def _sync_params(self):\n",
    "    #     \"\"\"Synchronize parameters across all model copies.\"\"\"\n",
    "    #     for param_list in zip(*[m.parameters() for m in self.models]):\n",
    "    #         param_data = param_list[0].data\n",
    "    #         for param in param_list[1:]:\n",
    "    #             param.data.copy_(param_data)\n",
    "\n",
    "    def _sync_params(self):\n",
    "        for param_list in zip(*[m.parameters() for m in self.models]):\n",
    "            # Average gradients across all GPUs\n",
    "            mean_param = sum(p.data.to('cpu') for p in param_list) / len(param_list)\n",
    "            # Update all GPUs with mean\n",
    "            for param in param_list:\n",
    "                param.data.copy_(mean_param)\n",
    "    \n",
    "    def forward(self, batch_list):\n",
    "        \"\"\"\n",
    "        Forward pass handling multiple batches on multiple GPUs.\n",
    "        \n",
    "        Args:\n",
    "            batch_list: List of batches, one for each GPU\n",
    "        \"\"\"\n",
    "        assert len(batch_list) == len(self.device_ids), \\\n",
    "            f\"Number of batches ({len(batch_list)}) must match number of GPUs ({len(self.device_ids)})\"\n",
    "        \n",
    "        # Process each batch on its corresponding GPU\n",
    "        outputs = []\n",
    "        for i, (device_id, batch) in enumerate(zip(self.device_ids, batch_list)):\n",
    "            # Move batch to appropriate device\n",
    "            batch = {k: v.to(f'cuda:{device_id}') for k, v in batch.items()}\n",
    "            # Process batch\n",
    "            outputs.append(self.models[i](**batch))\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "def train_citation_model(\n",
    "    model,\n",
    "    results,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    train_ratio: float = 0.8,\n",
    "    num_epochs: int = 5,\n",
    "    learning_rate: float = 1.5e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_steps: int = 0,\n",
    "    device: str = None,\n",
    "    save_path: str = \"citation_model.pt\",\n",
    "    batch_size: int = 128,\n",
    "    temperatures = [],\n",
    "    gradient_accumulation_steps: int = 1\n",
    "):\n",
    "    # Set up multi-GPU training\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Using {num_gpus} GPUs with custom data parallel implementation!\")\n",
    "        model = CustomDataParallel(model)\n",
    "        devices = [f'cuda:{i}' for i in range(num_gpus)]\n",
    "        per_gpu_batch_size = batch_size // num_gpus\n",
    "        print(f\"Per-GPU batch size: {per_gpu_batch_size}\")\n",
    "    else:\n",
    "        devices = ['cuda:0']\n",
    "        per_gpu_batch_size = batch_size\n",
    "    \n",
    "    # Initialize optimizer - one for each GPU model\n",
    "    optimizers = [\n",
    "        AdamW(model_copy.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        for model_copy in (model.models if num_gpus > 1 else [model])\n",
    "    ]\n",
    "    \n",
    "    # Initialize gradient scalers for mixed precision - one per GPU\n",
    "    scalers = [GradScaler() for _ in range(num_gpus)]\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    if num_gpus > 1:\n",
    "        for model_copy in model.models:\n",
    "            model_copy.transformer.gradient_checkpointing_enable()\n",
    "    else:\n",
    "        model.transformer.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch < len(temperatures):\n",
    "            temp = temperatures[epoch]\n",
    "            if num_gpus > 1:\n",
    "                for model_copy in model.models:\n",
    "                    model_copy.config.temperature = temp\n",
    "            else:\n",
    "                model.config.temperature = temp\n",
    "            print(f\"Temperature changed to {temp}\")\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Create new collated data for this epoch\n",
    "        print(\"Collating training data with new random masks...\")\n",
    "        collated = collate(results, tokenizer, config)\n",
    "        dataset = CitationDataset(collated)\n",
    "        train_size = int(len(dataset) * train_ratio)\n",
    "        train_dataset = dataset[:train_size]\n",
    "        val_dataset = dataset[train_size:]\n",
    "        \n",
    "        # Create separate dataloaders for each GPU\n",
    "        train_dataloaders = [\n",
    "            DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=per_gpu_batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=citation_collate_fn,\n",
    "                num_workers=4\n",
    "            )\n",
    "            for _ in range(num_gpus)\n",
    "        ]\n",
    "        \n",
    "        val_dataloaders = [\n",
    "            DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=per_gpu_batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=citation_collate_fn,\n",
    "                num_workers=4\n",
    "            )\n",
    "            for _ in range(num_gpus)\n",
    "        ]\n",
    "        \n",
    "        # Training phase\n",
    "        if num_gpus > 1:\n",
    "            for model_copy in model.models:\n",
    "                model_copy.train()\n",
    "        else:\n",
    "            model.train()\n",
    "            \n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        # Create iterators for each dataloader\n",
    "        train_iterators = [iter(loader) for loader in train_dataloaders]\n",
    "        \n",
    "        # Calculate number of steps\n",
    "        total_steps = min(len(loader) for loader in train_dataloaders)\n",
    "        progress_bar = tqdm.tqdm(range(total_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress_bar:\n",
    "            # Reset gradients for all optimizers\n",
    "            for opt in optimizers:\n",
    "                opt.zero_grad()\n",
    "            \n",
    "            step_loss = 0\n",
    "            \n",
    "            # Process a batch on each GPU\n",
    "            for accumulation_step in range(gradient_accumulation_steps):\n",
    "                try:\n",
    "                    # Get batches for all GPUs\n",
    "                    batches = [next(iterator) for iterator in train_iterators]\n",
    "                    \n",
    "                    # Forward pass with mixed precision\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        if num_gpus > 1:\n",
    "                            outputs = model(batches)\n",
    "                            # Move losses to CPU before combining\n",
    "                            losses = [output.loss.detach().cpu() for output in outputs]\n",
    "                            # Calculate mean loss on CPU\n",
    "                            batch_loss = torch.stack(losses).mean()\n",
    "                            # Move mean loss back to GPU for backward pass\n",
    "                            batch_loss = batch_loss.to(devices[0])\n",
    "                        else:\n",
    "                            outputs = model(**batches[0])\n",
    "                            batch_loss = outputs.loss\n",
    "                    \n",
    "                    # Scale loss by accumulation steps\n",
    "                    batch_loss = batch_loss / gradient_accumulation_steps\n",
    "                    step_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    if num_gpus > 1:\n",
    "                        # Each GPU processes its own backward pass\n",
    "                        for i, (output, scaler, opt) in enumerate(zip(outputs, scalers, optimizers)):\n",
    "                            scaled_loss = scaler.scale(output.loss / gradient_accumulation_steps)\n",
    "                            scaled_loss.backward()\n",
    "                    else:\n",
    "                        scalers[0].scale(batch_loss).backward()\n",
    "                \n",
    "                except StopIteration:\n",
    "                    break\n",
    "            \n",
    "            # Step optimizers and update scalers\n",
    "            if num_gpus > 1:\n",
    "                for scaler, opt in zip(scalers, optimizers):\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                # Sync parameters after optimization step\n",
    "                model._sync_params()\n",
    "            else:\n",
    "                scalers[0].step(optimizers[0])\n",
    "                scalers[0].update()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            total_train_loss += step_loss\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': step_loss})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"Running validation...\")\n",
    "        if num_gpus > 1:\n",
    "            for model_copy in model.models:\n",
    "                model_copy.eval()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        total_val_loss = 0\n",
    "        val_steps = 0\n",
    "        \n",
    "        val_iterators = [iter(loader) for loader in val_dataloaders]\n",
    "        total_val_steps = min(len(loader) for loader in val_dataloaders)\n",
    "        progress_bar = tqdm.tqdm(range(total_val_steps), desc=\"Validation\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in progress_bar:\n",
    "                try:\n",
    "                    # Get batches for all GPUs\n",
    "                    batches = [next(iterator) for iterator in val_iterators]\n",
    "                    \n",
    "                    # Forward pass with mixed precision\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        if num_gpus > 1:\n",
    "                            outputs = model(batches)\n",
    "                            # Move losses to CPU before combining\n",
    "                            losses = [output.loss.detach().cpu() for output in outputs]\n",
    "                            # Calculate mean loss on CPU\n",
    "                            batch_loss = torch.stack(losses).mean().item()\n",
    "                        else:\n",
    "                            outputs = model(**batches[0])\n",
    "                            batch_loss = outputs.loss.item()\n",
    "                    \n",
    "                    total_val_loss += batch_loss\n",
    "                    val_steps += 1\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({'loss': batch_loss})\n",
    "                \n",
    "                except StopIteration:\n",
    "                    break\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "        print(f\"\\nValidation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Save the first model if using multiple GPUs (they're all synced)\n",
    "            model_to_save = model.models[0] if num_gpus > 1 else model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizers[0].state_dict(),\n",
    "                'scaler_state_dict': scalers[0].state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, save_path)\n",
    "            print(f\"Saved new best model to {save_path}\")\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
