{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64717a5c-3b9b-41e8-81a0-ace988f60d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 10753.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 300/300 [00:00<00:00, 3469.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pre-processing dataset...\n",
      "Initial samples: 1488\n",
      "Skipped 163 samples due to missing CITE token\n",
      "Skipped 0 samples due to processing errors\n",
      "Final processed samples: 1325\n",
      "Dataset preprocessing completed.\n",
      "Pre-processing dataset...\n",
      "Initial samples: 1404\n",
      "Skipped 249 samples due to missing CITE token\n",
      "Skipped 0 samples due to processing errors\n",
      "Final processed samples: 1155\n",
      "Dataset preprocessing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█| 42/42 [01:13<00:00,  1.74s/it, loss=0.3263, avg_loss=1.1375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n",
      "Computing validation loss and collecting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|███████████████| 19/19 [00:16<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing global rankings for 1155 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing rankings: 100%|████████████████████████| 3/3 [00:00<00:00, 204.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "\n",
      "Validation Metrics:\n",
      "top_1_accuracy: 0.2026\n",
      "top_3_accuracy: 0.4372\n",
      "top_5_accuracy: 0.5576\n",
      "top_10_accuracy: 0.7186\n",
      "top_50_accuracy: 0.9186\n",
      "mrr: 0.3652\n",
      "median_rank: 4.0000\n",
      "mean_rank: 23.5437\n",
      "val_size: 1155.0000\n",
      "val_loss: 1.7173\n",
      "Saved new best model with validation loss: 1.7173\n",
      "\n",
      "Epoch 1 average training loss: 1.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█| 42/42 [01:13<00:00,  1.76s/it, loss=0.0447, avg_loss=0.4000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n",
      "Computing validation loss and collecting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|███████████████| 19/19 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing global rankings for 1155 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing rankings: 100%|████████████████████████| 3/3 [00:00<00:00, 470.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "\n",
      "Validation Metrics:\n",
      "top_1_accuracy: 0.2139\n",
      "top_3_accuracy: 0.4502\n",
      "top_5_accuracy: 0.5654\n",
      "top_10_accuracy: 0.7082\n",
      "top_50_accuracy: 0.9169\n",
      "mrr: 0.3744\n",
      "median_rank: 4.0000\n",
      "mean_rank: 22.1931\n",
      "val_size: 1155.0000\n",
      "val_loss: 1.6663\n",
      "Saved new best model with validation loss: 1.6663\n",
      "\n",
      "Epoch 2 average training loss: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█| 42/42 [01:14<00:00,  1.77s/it, loss=0.0522, avg_loss=0.2361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n",
      "Computing validation loss and collecting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|███████████████| 19/19 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing global rankings for 1155 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing rankings: 100%|████████████████████████| 3/3 [00:00<00:00, 464.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "\n",
      "Validation Metrics:\n",
      "top_1_accuracy: 0.2121\n",
      "top_3_accuracy: 0.4468\n",
      "top_5_accuracy: 0.5671\n",
      "top_10_accuracy: 0.7238\n",
      "top_50_accuracy: 0.9048\n",
      "mrr: 0.3727\n",
      "median_rank: 4.0000\n",
      "mean_rank: 23.0459\n",
      "val_size: 1155.0000\n",
      "val_loss: 1.7231\n",
      "\n",
      "Epoch 3 average training loss: 0.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█| 42/42 [01:14<00:00,  1.77s/it, loss=0.0773, avg_loss=0.1843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n",
      "Computing validation loss and collecting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|███████████████| 19/19 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing global rankings for 1155 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing rankings: 100%|████████████████████████| 3/3 [00:00<00:00, 458.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "\n",
      "Validation Metrics:\n",
      "top_1_accuracy: 0.2104\n",
      "top_3_accuracy: 0.4554\n",
      "top_5_accuracy: 0.5680\n",
      "top_10_accuracy: 0.7169\n",
      "top_50_accuracy: 0.9048\n",
      "mrr: 0.3731\n",
      "median_rank: 4.0000\n",
      "mean_rank: 29.7056\n",
      "val_size: 1155.0000\n",
      "val_loss: 1.7157\n",
      "\n",
      "Epoch 4 average training loss: 0.1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█| 42/42 [01:14<00:00,  1.77s/it, loss=0.0404, avg_loss=0.1642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n",
      "Computing validation loss and collecting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|███████████████| 19/19 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing global rankings for 1155 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing rankings: 100%|████████████████████████| 3/3 [00:00<00:00, 477.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final metrics...\n",
      "\n",
      "Validation Metrics:\n",
      "top_1_accuracy: 0.2147\n",
      "top_3_accuracy: 0.4485\n",
      "top_5_accuracy: 0.5732\n",
      "top_10_accuracy: 0.7333\n",
      "top_50_accuracy: 0.9091\n",
      "mrr: 0.3766\n",
      "median_rank: 4.0000\n",
      "mean_rank: 24.2701\n",
      "val_size: 1155.0000\n",
      "val_loss: 1.7268\n",
      "\n",
      "Epoch 5 average training loss: 0.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  36%|▎| 15/42 [00:26<00:48,  1.80s/it, loss=0.1702, avg_loss=0.1502]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Iterator, Dict, Union, Optional\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import bz2\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os \n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class WikiDumpParser:\n",
    "    def __init__(self, dump_path: str):\n",
    "        self.dump_path = dump_path\n",
    "        self.ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "\n",
    "    def _open_dump(self) -> Iterator:\n",
    "        if self.dump_path.endswith('.bz2'):\n",
    "            return bz2.BZ2File(self.dump_path)\n",
    "        return open(self.dump_path, 'rb')\n",
    "\n",
    "    def iter_pages(self) -> Iterator[Dict]:\n",
    "        context = ET.iterparse(self._open_dump(), events=('end',))\n",
    "        \n",
    "        for event, elem in context:\n",
    "            if elem.tag.endswith('page'):\n",
    "                title_elem = elem.find('.//mw:title', self.ns)\n",
    "                revision = elem.find('.//mw:revision', self.ns)\n",
    "                \n",
    "                if revision is not None:\n",
    "                    text_elem = revision.find('mw:text', self.ns)\n",
    "                    timestamp_elem = revision.find('mw:timestamp', self.ns)\n",
    "                    text = text_elem.text if text_elem is not None else ''\n",
    "                    timestamp = timestamp_elem.text if timestamp_elem is not None else ''\n",
    "                else:\n",
    "                    text = ''\n",
    "                    timestamp = ''\n",
    "                \n",
    "                title = title_elem.text if title_elem is not None else ''\n",
    "                \n",
    "                yield {\n",
    "                    'title': title,\n",
    "                    'text': text,\n",
    "                    'timestamp': timestamp,\n",
    "                    'is_redirect': bool(re.match(r'#REDIRECT', text or '', re.IGNORECASE))\n",
    "                }\n",
    "                \n",
    "                elem.clear()\n",
    "\n",
    "    def iter_articles(self, skip_redirects: bool = True) -> Iterator[Dict]:\n",
    "        for page in self.iter_pages():\n",
    "            title = page['title']\n",
    "            \n",
    "            if any(title.startswith(prefix) for prefix in [\n",
    "                'Wikipedia:', 'Template:', 'Category:', 'Portal:',\n",
    "                'File:', 'MediaWiki:', 'Help:', 'Book:', 'Draft:',\n",
    "                'TimedText:', 'Module:', 'Special:'\n",
    "            ]):\n",
    "                continue\n",
    "                \n",
    "            if skip_redirects and page['is_redirect']:\n",
    "                continue\n",
    "                \n",
    "            yield page\n",
    "\n",
    "    def save_to_jsonl(self, output_path: Union[str, Path], sample_size: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        Save articles to a JSONL file (one JSON object per line).\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to the output JSONL file\n",
    "            sample_size: Optional number of articles to save\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of articles saved\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for i, article in enumerate(self.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                json.dump(article, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def save_to_sqlite(self, db_path: Union[str, Path], sample_size: Optional[int] = None,\n",
    "                      batch_size: int = 1000) -> int:\n",
    "        \"\"\"\n",
    "        Save articles to a SQLite database.\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to the SQLite database file\n",
    "            sample_size: Optional number of articles to save\n",
    "            batch_size: Number of articles to insert in each batch\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of articles saved\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        # Create table if it doesn't exist\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                    (title TEXT PRIMARY KEY,\n",
    "                     text TEXT,\n",
    "                     timestamp TEXT,\n",
    "                     is_redirect INTEGER)''')\n",
    "        \n",
    "        # Create index on title\n",
    "        c.execute('CREATE INDEX IF NOT EXISTS idx_title ON articles(title)')\n",
    "        \n",
    "        count = 0\n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for i, article in enumerate(self.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                batch.append((\n",
    "                    article['title'],\n",
    "                    article['text'],\n",
    "                    article['timestamp'],\n",
    "                    1 if article['is_redirect'] else 0\n",
    "                ))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    c.executemany(\n",
    "                        'INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)',\n",
    "                        batch\n",
    "                    )\n",
    "                    conn.commit()\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "            \n",
    "            # Insert remaining articles\n",
    "            if batch:\n",
    "                c.executemany(\n",
    "                    'INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)',\n",
    "                    batch\n",
    "                )\n",
    "                conn.commit()\n",
    "                count += len(batch)\n",
    "                \n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "        return count\n",
    "\n",
    "def read_from_jsonl(file_path: Union[str, Path], limit: Optional[int] = None) -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Read articles from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSONL file\n",
    "        limit: Optional maximum number of articles to read\n",
    "        \n",
    "    Yields:\n",
    "        Dict: Article information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            yield json.loads(line)\n",
    "\n",
    "def read_from_sqlite(db_path: Union[str, Path], limit: Optional[int] = None) -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Read articles from a SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database\n",
    "        limit: Optional maximum number of articles to read\n",
    "        \n",
    "    Yields:\n",
    "        Dict: Article information\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        query = 'SELECT title, text, timestamp, is_redirect FROM articles'\n",
    "        if limit is not None:\n",
    "            query += f' LIMIT {limit}'\n",
    "            \n",
    "        for row in c.execute(query):\n",
    "            yield {\n",
    "                'title': row[0],\n",
    "                'text': row[1],\n",
    "                'timestamp': row[2],\n",
    "                'is_redirect': bool(row[3])\n",
    "            }\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def extract_citations(text: str) -> List[str]:\n",
    "    \"\"\"Extract all citations from a text and clean them.\"\"\"\n",
    "    citations = re.findall(r'\\[\\[(.*?)\\]\\]', text)\n",
    "    # Clean up citations (take first part if pipe character exists)\n",
    "    return [c.split('|')[0]for c in citations]\n",
    "\n",
    "def clean_wiki_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean wiki content by:\n",
    "    1. Finding the start of the actual article (from triple quotes)\n",
    "    2. Removing category tags, file links, and other metadata\n",
    "    \n",
    "    Args:\n",
    "        text: Raw wiki text\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned article content starting from title\n",
    "    \"\"\"\n",
    "    # Find the main article content starting with triple quotes\n",
    "    title_match = re.search(r\"'''([^']+?)'''\", text)\n",
    "    if not title_match:\n",
    "        return text  # Return original if no title found\n",
    "        \n",
    "    # Get the position where the title starts\n",
    "    start_pos = title_match.start()\n",
    "    content = text[start_pos:]\n",
    "    \n",
    "    # Remove category tags\n",
    "    content = re.sub(r'\\[\\[Category:.*?\\]\\]', '', content)\n",
    "    \n",
    "    # Remove file/image links\n",
    "    content = re.sub(r'\\[\\[File:.*?\\]\\]', '', content)\n",
    "    \n",
    "    # Remove stub templates\n",
    "    content = re.sub(r'\\{\\{stub\\}\\}', '', content)\n",
    "    \n",
    "    # Remove empty lines that might remain\n",
    "    content = '\\n'.join(line for line in content.split('\\n') if line.strip())\n",
    "    \n",
    "    return content.strip()\n",
    "\n",
    "def create_article_dict(jsonl_path, sample_size: Optional[int] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create a dictionary of articles from the wiki parser, with cleaned content.\n",
    "    \n",
    "    Args:\n",
    "        jsonl: the path to the saved wiki pages as [Title] -> [Text] key value format. \n",
    "        sample_size: Optional number of articles to include\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary with article titles as keys and cleaned content as values\n",
    "    \"\"\"\n",
    "    articles = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            article = json.loads(line)\n",
    "            if sample_size is not None and i >= sample_size:\n",
    "                break\n",
    "                \n",
    "            # Clean the content to start from title\n",
    "            cleaned_content = clean_wiki_content(article['text'])\n",
    "            \n",
    "            if cleaned_content:  # Only include if content remains after cleaning\n",
    "                articles[article['title'].lower()] = cleaned_content\n",
    "            \n",
    "    return articles\n",
    "\n",
    "def prepare_sources_targets(articles_dict, sample_size=1000, cite_sample=1):\n",
    "    articles = np.random.permutation(list(articles_dict.keys()))[:sample_size]\n",
    "    # Initialize your model (assuming it's defined elsewhere)\n",
    "    model = AutoregressiveCitationMatcher()\n",
    "    sources = []\n",
    "    targets = []\n",
    "    for article in tqdm.tqdm(articles):\n",
    "        source_text = articles_dict[article]\n",
    "        # Get all citations from this source article\n",
    "        citations = extract_citations(source_text)\n",
    "        valid_citations = [c for c in citations if c.lower() in articles_dict]\n",
    "    \n",
    "        for citation in np.random.choice(valid_citations,min(cite_sample,len(valid_citations))):\n",
    "            try:\n",
    "                source_content = model.prepare_source_context(source_text, citation)\n",
    "                target_content = model.prepare_target_page(articles_dict[citation.lower()])\n",
    "                sources.append(source_content)\n",
    "                targets.append(target_content)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return sources, targets \n",
    "\n",
    "\n",
    "class AutoregressiveCitationMatcher(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        max_length: int = 512,\n",
    "        cite_token: str = \"<CITE>\",\n",
    "        ref_token: str = \"<REF>\",\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Add special tokens for citation and reference\n",
    "        special_tokens = {\n",
    "            'additional_special_tokens': [cite_token, ref_token]\n",
    "        }\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # For GPT models, we might need to set pad token if it's not set\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize encoder for both source and target texts\n",
    "        # We use the same model for both since autoregressive models maintain context\n",
    "        self.model = AutoModel.from_pretrained(model_name, gradient_checkpointing=True ).to(self.device)\n",
    "        \n",
    "        # Resize token embeddings\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.cite_token = cite_token\n",
    "        self.ref_token = ref_token\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def prepare_source_context(\n",
    "        self,\n",
    "        text: str,\n",
    "        target_citation: str,\n",
    "        window_size: int = 100\n",
    "    ) -> str:\n",
    "        \"\"\"Prepare source context with citation and reference tokens.\"\"\"\n",
    "        citation_pattern = re.escape(f\"[[{target_citation}]]\")\n",
    "        # Add both <CITE> and <REF> tokens\n",
    "        modified_text = re.sub(citation_pattern, f\"{self.cite_token}\", text)\n",
    "        # modified_text = f\"{modified_text} {self.ref_token}\"\n",
    "\n",
    "        if self.cite_token not in modified_text:\n",
    "            raise ValueError(f\"There is no cite token\")\n",
    "        \n",
    "        return modified_text\n",
    "\n",
    "    def prepare_target_page(\n",
    "        self,\n",
    "        page_content: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Prepare target page with reference token.\"\"\"\n",
    "        # Take first paragraph as summary and add <REF> token\n",
    "            \n",
    "        return f\"{page_content} {self.ref_token}\"\n",
    "\n",
    "    def get_token_embedding(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        token_id: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Extract embedding for specific token for each item in the batch.\"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        embeddings = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            # Find token positions for this batch item\n",
    "            token_positions = (input_ids[batch_idx] == token_id).nonzero()\n",
    "            \n",
    "            if len(token_positions) == 0:\n",
    "                raise ValueError(f\"Token id {token_id} not found in sequence for batch item {batch_idx}\")\n",
    "                    \n",
    "            if len(token_positions) > 1:\n",
    "                # If multiple occurrences, take the last one\n",
    "                position = token_positions[-1].item()\n",
    "            else:\n",
    "                position = token_positions[0].item()\n",
    "                    \n",
    "            # Extract embedding for this batch item\n",
    "            embedding = hidden_states[batch_idx, position, :]  # Explicitly select all hidden dimensions\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        # Stack all embeddings into a single tensor\n",
    "        return torch.stack(embeddings)  # Shape will be [batch_size, hidden_size]\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        is_source: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode text and extract reference token embedding.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Get hidden states from the model\n",
    "        outputs = self.model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get [REF] token embedding from the last hidden state\n",
    "        ref_token_id = self.tokenizer.convert_tokens_to_ids(self.ref_token)\n",
    "        ref_embeddings = self.get_token_embedding(\n",
    "            outputs.hidden_states[-1],\n",
    "            inputs['input_ids'],\n",
    "            ref_token_id\n",
    "        )\n",
    "        return ref_embeddings\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source_contexts: List[str],\n",
    "        target_pages: List[str],\n",
    "        temperature: float = 0.07\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute similarity between source and target [REF] embeddings.\"\"\"\n",
    "        # Encode all contexts and references\n",
    "        source_embeddings = []\n",
    "        target_embeddings = []\n",
    "        \n",
    "        for context in source_contexts:\n",
    "            source_emb = self.encode_text(context, is_source=True)\n",
    "            source_embeddings.append(source_emb)\n",
    "            \n",
    "        for page in target_pages:\n",
    "            target_emb = self.encode_text(page, is_source=False)\n",
    "            target_embeddings.append(target_emb)\n",
    "        \n",
    "        # Stack embeddings\n",
    "        source_embeddings = torch.cat(source_embeddings, dim=0)\n",
    "        target_embeddings = torch.cat(target_embeddings, dim=0)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        source_embeddings = nn.functional.normalize(source_embeddings, dim=-1)\n",
    "        target_embeddings = nn.functional.normalize(target_embeddings, dim=-1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity = torch.matmul(\n",
    "            source_embeddings,\n",
    "            target_embeddings.transpose(0, 1)\n",
    "        ) / temperature\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    def train_step(\n",
    "        self,\n",
    "        source_contexts: List[str],\n",
    "        target_pages: List[str],\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        temperature: float = 0.07\n",
    "    ) -> float:\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        similarity = self(source_contexts, target_pages, temperature)\n",
    "        \n",
    "        # The diagonal elements should be the positive pairs\n",
    "        labels = torch.arange(len(source_contexts)).to(self.device)\n",
    "        # print('#'*20)\n",
    "        # print('similarity = ', similarity, ', labels = ', labels)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.CrossEntropyLoss()(similarity, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "class CitationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sources: List[str],\n",
    "        targets: List[str],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset with preprocessing of all samples.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of source texts\n",
    "            targets: List of target texts\n",
    "            tokenizer: Tokenizer to use\n",
    "            max_length: Maximum sequence length\n",
    "            verbose: Whether to print processing statistics\n",
    "        \"\"\"\n",
    "        assert len(sources) == len(targets), \"Sources and targets must have same length\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ref_token = \"<REF>\"\n",
    "        self.cite_token = \"<CITE>\"\n",
    "        \n",
    "        # Get token IDs\n",
    "        self.ref_token_id = self.tokenizer.convert_tokens_to_ids(self.ref_token)\n",
    "        self.cite_token_id = self.tokenizer.convert_tokens_to_ids(self.cite_token)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Pre-processing dataset...\")\n",
    "            print(f\"Initial samples: {len(sources)}\")\n",
    "        \n",
    "        # Pre-process and store all valid samples\n",
    "        self.processed_samples = self._preprocess_samples(sources, targets, verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Final processed samples: {len(self.processed_samples)}\")\n",
    "            print(\"Dataset preprocessing completed.\")\n",
    "\n",
    "    def _preprocess_samples(\n",
    "        self,\n",
    "        sources: List[str],\n",
    "        targets: List[str],\n",
    "        verbose: bool\n",
    "    ) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Pre-process all samples and filter invalid ones.\n",
    "        Returns list of valid processed samples.\n",
    "        \"\"\"\n",
    "        processed_samples = []\n",
    "        skipped_no_cite = 0\n",
    "        skipped_errors = 0\n",
    "        \n",
    "        for idx, (source, target) in enumerate(zip(sources, targets)):\n",
    "            try:\n",
    "                # Tokenize source and target\n",
    "                source_tokens = self.tokenizer(\n",
    "                    source,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                target_tokens = self.tokenizer(\n",
    "                    target,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                \n",
    "                # Ensure REF token in both source and target\n",
    "                # source_tokens['input_ids'] = self.ensure_ref_token(source_tokens['input_ids'])\n",
    "                target_tokens['input_ids'] = self.ensure_ref_token(target_tokens['input_ids'])\n",
    "                \n",
    "                # Store processed tensors\n",
    "                processed_sample = {\n",
    "                    'source_input_ids': source_tokens['input_ids'].squeeze(0),\n",
    "                    'source_attention_mask': source_tokens['attention_mask'].squeeze(0),\n",
    "                    'target_input_ids': target_tokens['input_ids'].squeeze(0),\n",
    "                    'target_attention_mask': target_tokens['attention_mask'].squeeze(0)\n",
    "                }\n",
    "                \n",
    "                # Check if CITE token exists in source after tokenization\n",
    "                if self.cite_token_id not in source_tokens['input_ids'][0]:\n",
    "                    skipped_no_cite += 1\n",
    "                    continue\n",
    "                \n",
    "                processed_samples.append(processed_sample)\n",
    "                \n",
    "            except Exception as e:\n",
    "                skipped_errors += 1\n",
    "                if verbose:\n",
    "                    print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if verbose and (idx + 1) % 10000 == 0:\n",
    "                print(f\"Processed {idx + 1} samples...\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Skipped {skipped_no_cite} samples due to missing CITE token\")\n",
    "            print(f\"Skipped {skipped_errors} samples due to processing errors\")\n",
    "        \n",
    "        return processed_samples\n",
    "\n",
    "    def ensure_ref_token(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Ensure the REF token is present in the sequence, replacing last non-pad token if needed.\"\"\"\n",
    "        batch_size = tokens.size(0)\n",
    "        \n",
    "        # For each sequence in the batch\n",
    "        for i in range(batch_size):\n",
    "            sequence = tokens[i]\n",
    "            # Find positions of REF token\n",
    "            ref_positions = (sequence == self.ref_token_id).nonzero()\n",
    "            \n",
    "            if len(ref_positions) == 0:\n",
    "                # If no REF token found, replace the last non-pad token\n",
    "                pad_token_id = self.tokenizer.pad_token_id\n",
    "                # Find the last non-pad token position\n",
    "                non_pad_positions = (sequence != pad_token_id).nonzero()\n",
    "                if len(non_pad_positions) > 0:\n",
    "                    last_non_pad_pos = non_pad_positions[-1]\n",
    "                    sequence[last_non_pad_pos] = self.ref_token_id\n",
    "            elif len(ref_positions) > 1:\n",
    "                # If multiple REF tokens, keep only the last one\n",
    "                for pos in ref_positions[:-1]:\n",
    "                    sequence[pos] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.unk_token)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_samples)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Return pre-processed sample.\"\"\"\n",
    "        return self.processed_samples[idx]\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Custom collate function to handle batching of tokenized inputs.\"\"\"\n",
    "    return {\n",
    "        key: torch.stack([item[key] for item in batch])\n",
    "        for key in batch[0].keys()\n",
    "    }\n",
    "\n",
    "def train_model(\n",
    "    model: AutoregressiveCitationMatcher,\n",
    "    train_sources: List[str],\n",
    "    train_targets: List[str],\n",
    "    val_sources: List[str] = None,\n",
    "    val_targets: List[str] = None,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    temperature: float = 0.07,\n",
    "    num_workers: int = 4,\n",
    "    save_path: str = 'checkpoint.pt'\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-optimized training function with combined validation.\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = CitationDataset(\n",
    "        train_sources,\n",
    "        train_targets,\n",
    "        model.tokenizer,\n",
    "        model.max_length\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    if val_sources and val_targets:\n",
    "        val_dataset = CitationDataset(\n",
    "            val_sources,\n",
    "            val_targets,\n",
    "            model.tokenizer,\n",
    "            model.max_length\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=2*batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2,\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criteria = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(model.device, non_blocking=True) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            source_outputs = model.model(\n",
    "                input_ids=batch['source_input_ids'],\n",
    "                attention_mask=batch['source_attention_mask'],\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            target_outputs = model.model(\n",
    "                input_ids=batch['target_input_ids'],\n",
    "                attention_mask=batch['target_attention_mask'],\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Get token embeddings\n",
    "            cite_token_id = model.tokenizer.convert_tokens_to_ids(model.cite_token)\n",
    "            ref_token_id = model.tokenizer.convert_tokens_to_ids(model.ref_token)\n",
    "\n",
    "            source_embeddings = model.get_token_embedding(\n",
    "                source_outputs.last_hidden_state,\n",
    "                batch['source_input_ids'],\n",
    "                cite_token_id\n",
    "            )\n",
    "            \n",
    "            target_embeddings = model.get_token_embedding(\n",
    "                target_outputs.last_hidden_state,\n",
    "                batch['target_input_ids'],\n",
    "                ref_token_id\n",
    "            )\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            source_embeddings = torch.nn.functional.normalize(source_embeddings, dim=-1)\n",
    "            target_embeddings = torch.nn.functional.normalize(target_embeddings, dim=-1)\n",
    "            \n",
    "            # Compute similarity matrix\n",
    "            similarity = torch.matmul(source_embeddings, target_embeddings.transpose(0, 1)) / temperature\n",
    "            \n",
    "            # Compute loss\n",
    "            labels = torch.arange(similarity.size(0)).to(model.device)\n",
    "            loss = criteria(similarity, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss': f'{total_loss / num_batches:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Free memory\n",
    "            del source_outputs, target_outputs, source_embeddings, target_embeddings, similarity\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Validation\n",
    "        if val_sources and val_targets:\n",
    "            print(\"\\nRunning validation...\")\n",
    "            val_loss, metrics = validate_model(model, val_loader, temperature)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
    "            \n",
    "            model.train()\n",
    "                \n",
    "        epoch_loss = total_loss / num_batches\n",
    "        print(f\"\\nEpoch {epoch + 1} average training loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "def validate_model(\n",
    "    model: AutoregressiveCitationMatcher,\n",
    "    val_loader: DataLoader,\n",
    "    temperature: float\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Combined validation function that calculates both loss and ranking metrics in a single pass.\n",
    "    \n",
    "    Args:\n",
    "        model: The citation matcher model\n",
    "        val_loader: Validation data loader\n",
    "        temperature: Temperature for similarity scaling\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation_loss, rank_metrics_dict)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Storage for embeddings\n",
    "    all_source_embeddings = []\n",
    "    all_target_embeddings = []\n",
    "    \n",
    "    print(\"Computing validation loss and collecting embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(val_loader, desc='Processing validation data'):\n",
    "            batch = {k: v.to(model.device, non_blocking=True) for k, v in batch.items()}\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                source_outputs = model.model(\n",
    "                    input_ids=batch['source_input_ids'],\n",
    "                    attention_mask=batch['source_attention_mask'],\n",
    "                    output_hidden_states=False,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                target_outputs = model.model(\n",
    "                    input_ids=batch['target_input_ids'],\n",
    "                    attention_mask=batch['target_attention_mask'],\n",
    "                    output_hidden_states=False,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Get token embeddings\n",
    "                cite_token_id = model.tokenizer.convert_tokens_to_ids(model.cite_token)\n",
    "                ref_token_id = model.tokenizer.convert_tokens_to_ids(model.ref_token)\n",
    "                \n",
    "                source_embeddings = model.get_token_embedding(\n",
    "                    source_outputs.last_hidden_state,\n",
    "                    batch['source_input_ids'],\n",
    "                    cite_token_id\n",
    "                )\n",
    "                \n",
    "                target_embeddings = model.get_token_embedding(\n",
    "                    target_outputs.last_hidden_state,\n",
    "                    batch['target_input_ids'],\n",
    "                    ref_token_id\n",
    "                )\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                source_embeddings = torch.nn.functional.normalize(source_embeddings, dim=-1)\n",
    "                target_embeddings = torch.nn.functional.normalize(target_embeddings, dim=-1)\n",
    "                \n",
    "                # Compute batch similarity and loss\n",
    "                similarity = torch.matmul(source_embeddings, target_embeddings.transpose(0, 1)) / temperature\n",
    "                labels = torch.arange(similarity.size(0)).to(model.device)\n",
    "                loss = torch.nn.CrossEntropyLoss()(similarity, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Store embeddings for ranking metrics\n",
    "                all_source_embeddings.append(source_embeddings.cpu())\n",
    "                all_target_embeddings.append(target_embeddings.cpu())\n",
    "                \n",
    "                # Clean up\n",
    "                del source_outputs, target_outputs, similarity\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping batch due to error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    val_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    all_source_embeddings = torch.cat(all_source_embeddings, dim=0)\n",
    "    all_target_embeddings = torch.cat(all_target_embeddings, dim=0)\n",
    "    total_samples = all_source_embeddings.size(0)\n",
    "    \n",
    "    print(f\"\\nComputing global rankings for {total_samples} samples...\")\n",
    "    \n",
    "    # Define k values for top-k accuracy\n",
    "    k_values = [1, 3, 5, 10, 50]\n",
    "    \n",
    "    # Process in chunks to avoid OOM\n",
    "    chunk_size = 512\n",
    "    similarity_rankings = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, total_samples, chunk_size), desc='Computing rankings'):\n",
    "        chunk_end = min(i + chunk_size, total_samples)\n",
    "        source_chunk = all_source_embeddings[i:chunk_end].to(model.device)\n",
    "        \n",
    "        # Calculate similarity for this chunk\n",
    "        chunk_similarity = torch.matmul(source_chunk, all_target_embeddings.to(model.device).t()) / temperature\n",
    "        \n",
    "        # Get rankings for this chunk\n",
    "        chunk_rankings = torch.argsort(chunk_similarity, dim=-1, descending=True)\n",
    "        similarity_rankings.append(chunk_rankings.cpu())\n",
    "        \n",
    "        del chunk_similarity, source_chunk\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all rankings\n",
    "    all_rankings = torch.cat(similarity_rankings, dim=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    correct_at_k = {k: 0 for k in k_values}\n",
    "    reciprocal_ranks = []\n",
    "    all_ranks = []\n",
    "    \n",
    "    print(\"Computing final metrics...\")\n",
    "    for i in range(total_samples):\n",
    "        rank = (all_rankings[i] == i).nonzero().item() + 1\n",
    "        \n",
    "        # Update top-k accuracy counters\n",
    "        for k in k_values:\n",
    "            if rank <= k:\n",
    "                correct_at_k[k] += 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        all_ranks.append(rank)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    metrics = {\n",
    "        f'top_{k}_accuracy': correct_at_k[k] / total_samples for k in k_values\n",
    "    }\n",
    "    metrics.update({\n",
    "        'mrr': sum(reciprocal_ranks) / len(reciprocal_ranks),\n",
    "        'median_rank': float(np.median(all_ranks)),\n",
    "        'mean_rank': float(np.mean(all_ranks)),\n",
    "        'val_size': len(all_ranks),\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return val_loss, metrics\n",
    "    \n",
    "\n",
    "# Usage example:\n",
    "# Split data into train and validation sets\n",
    "def split_data(sources: List[str], targets: List[str], val_ratio: float = 0.2) -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"Split data into training and validation sets.\"\"\"\n",
    "    val_size = int(len(sources) * val_ratio)\n",
    "    indices = torch.randperm(len(sources))\n",
    "    \n",
    "    train_indices = indices[val_size:]\n",
    "    val_indices = indices[:val_size]\n",
    "    \n",
    "    train_sources = [sources[i] for i in train_indices]\n",
    "    train_targets = [targets[i] for i in train_indices]\n",
    "    val_sources = [sources[i] for i in val_indices]\n",
    "    val_targets = [targets[i] for i in val_indices]\n",
    "    \n",
    "    return train_sources, train_targets, val_sources, val_targets\n",
    "\n",
    "# wiki_parser = WikiDumpParser('./data/wiki/simplewiki-latest-pages-articles.xml')\n",
    "# wiki_parser.save_to_jsonl('./wiki_articles.jsonl')\n",
    "\n",
    "# articles_dict = create_article_dict(jsonl_path='./wiki_articles.jsonl')\n",
    "train_sources, train_targets = prepare_sources_targets(articles_dict, sample_size = 2000, cite_sample=1)\n",
    "val_sources, val_targets = prepare_sources_targets(articles_dict, sample_size = 300, cite_sample=10)\n",
    "\n",
    "# Split the data\n",
    "# train_sources, train_targets, val_sources, val_targets = split_data(sources, targets)\n",
    "\n",
    "# Initialize model (assuming it's already defined)\n",
    "model = AutoregressiveCitationMatcher(model_name=\"bert-base-uncased\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Train the modelk\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_sources=train_sources,\n",
    "    train_targets=train_targets,\n",
    "    val_sources=val_sources,\n",
    "    val_targets=val_targets,\n",
    "    batch_size=32,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1.5e-4,\n",
    "    temperature=0.1,\n",
    "    num_workers=4,\n",
    "    save_path='best_model.pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e45919-7df5-473e-991b-8c2f6adfcedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ba1cc-d1d3-4cf5-9fbb-e82f0e4186a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb302bc-5802-4f5d-959c-4bdf4c9acdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f27a2-64ab-4277-ad92-4dc3b610b132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
