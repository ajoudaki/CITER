{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64717a5c-3b9b-41e8-81a0-ace988f60d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 11186.99it/s]\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pre-processing dataset...\n",
      "Initial samples: 1342\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import bz2\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Union, Optional\n",
    "import re\n",
    "import os \n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class WikiDumpParser:\n",
    "    def __init__(self, dump_path: str):\n",
    "        self.dump_path = dump_path\n",
    "        self.ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "\n",
    "    def _open_dump(self) -> Iterator:\n",
    "        if self.dump_path.endswith('.bz2'):\n",
    "            return bz2.BZ2File(self.dump_path)\n",
    "        return open(self.dump_path, 'rb')\n",
    "\n",
    "    def iter_pages(self) -> Iterator[Dict]:\n",
    "        context = ET.iterparse(self._open_dump(), events=('end',))\n",
    "        \n",
    "        for event, elem in context:\n",
    "            if elem.tag.endswith('page'):\n",
    "                title_elem = elem.find('.//mw:title', self.ns)\n",
    "                revision = elem.find('.//mw:revision', self.ns)\n",
    "                \n",
    "                if revision is not None:\n",
    "                    text_elem = revision.find('mw:text', self.ns)\n",
    "                    timestamp_elem = revision.find('mw:timestamp', self.ns)\n",
    "                    text = text_elem.text if text_elem is not None else ''\n",
    "                    timestamp = timestamp_elem.text if timestamp_elem is not None else ''\n",
    "                else:\n",
    "                    text = ''\n",
    "                    timestamp = ''\n",
    "                \n",
    "                title = title_elem.text if title_elem is not None else ''\n",
    "                \n",
    "                yield {\n",
    "                    'title': title,\n",
    "                    'text': text,\n",
    "                    'timestamp': timestamp,\n",
    "                    'is_redirect': bool(re.match(r'#REDIRECT', text or '', re.IGNORECASE))\n",
    "                }\n",
    "                \n",
    "                elem.clear()\n",
    "\n",
    "    def iter_articles(self, skip_redirects: bool = True) -> Iterator[Dict]:\n",
    "        for page in self.iter_pages():\n",
    "            title = page['title']\n",
    "            \n",
    "            if any(title.startswith(prefix) for prefix in [\n",
    "                'Wikipedia:', 'Template:', 'Category:', 'Portal:',\n",
    "                'File:', 'MediaWiki:', 'Help:', 'Book:', 'Draft:',\n",
    "                'TimedText:', 'Module:', 'Special:'\n",
    "            ]):\n",
    "                continue\n",
    "                \n",
    "            if skip_redirects and page['is_redirect']:\n",
    "                continue\n",
    "                \n",
    "            yield page\n",
    "\n",
    "    def save_to_jsonl(self, output_path: Union[str, Path], sample_size: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        Save articles to a JSONL file (one JSON object per line).\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to the output JSONL file\n",
    "            sample_size: Optional number of articles to save\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of articles saved\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for i, article in enumerate(self.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                json.dump(article, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def save_to_sqlite(self, db_path: Union[str, Path], sample_size: Optional[int] = None,\n",
    "                      batch_size: int = 1000) -> int:\n",
    "        \"\"\"\n",
    "        Save articles to a SQLite database.\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to the SQLite database file\n",
    "            sample_size: Optional number of articles to save\n",
    "            batch_size: Number of articles to insert in each batch\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of articles saved\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        # Create table if it doesn't exist\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                    (title TEXT PRIMARY KEY,\n",
    "                     text TEXT,\n",
    "                     timestamp TEXT,\n",
    "                     is_redirect INTEGER)''')\n",
    "        \n",
    "        # Create index on title\n",
    "        c.execute('CREATE INDEX IF NOT EXISTS idx_title ON articles(title)')\n",
    "        \n",
    "        count = 0\n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for i, article in enumerate(self.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                batch.append((\n",
    "                    article['title'],\n",
    "                    article['text'],\n",
    "                    article['timestamp'],\n",
    "                    1 if article['is_redirect'] else 0\n",
    "                ))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    c.executemany(\n",
    "                        'INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)',\n",
    "                        batch\n",
    "                    )\n",
    "                    conn.commit()\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "            \n",
    "            # Insert remaining articles\n",
    "            if batch:\n",
    "                c.executemany(\n",
    "                    'INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)',\n",
    "                    batch\n",
    "                )\n",
    "                conn.commit()\n",
    "                count += len(batch)\n",
    "                \n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "        return count\n",
    "\n",
    "def read_from_jsonl(file_path: Union[str, Path], limit: Optional[int] = None) -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Read articles from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSONL file\n",
    "        limit: Optional maximum number of articles to read\n",
    "        \n",
    "    Yields:\n",
    "        Dict: Article information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            yield json.loads(line)\n",
    "\n",
    "def read_from_sqlite(db_path: Union[str, Path], limit: Optional[int] = None) -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Read articles from a SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database\n",
    "        limit: Optional maximum number of articles to read\n",
    "        \n",
    "    Yields:\n",
    "        Dict: Article information\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        query = 'SELECT title, text, timestamp, is_redirect FROM articles'\n",
    "        if limit is not None:\n",
    "            query += f' LIMIT {limit}'\n",
    "            \n",
    "        for row in c.execute(query):\n",
    "            yield {\n",
    "                'title': row[0],\n",
    "                'text': row[1],\n",
    "                'timestamp': row[2],\n",
    "                'is_redirect': bool(row[3])\n",
    "            }\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def extract_citations(text: str) -> List[str]:\n",
    "    \"\"\"Extract all citations from a text and clean them.\"\"\"\n",
    "    citations = re.findall(r'\\[\\[(.*?)\\]\\]', text)\n",
    "    # Clean up citations (take first part if pipe character exists)\n",
    "    return [c.split('|')[0]for c in citations]\n",
    "\n",
    "def clean_wiki_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean wiki content by:\n",
    "    1. Finding the start of the actual article (from triple quotes)\n",
    "    2. Removing category tags, file links, and other metadata\n",
    "    \n",
    "    Args:\n",
    "        text: Raw wiki text\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned article content starting from title\n",
    "    \"\"\"\n",
    "    # Find the main article content starting with triple quotes\n",
    "    title_match = re.search(r\"'''([^']+?)'''\", text)\n",
    "    if not title_match:\n",
    "        return text  # Return original if no title found\n",
    "        \n",
    "    # Get the position where the title starts\n",
    "    start_pos = title_match.start()\n",
    "    content = text[start_pos:]\n",
    "    \n",
    "    # Remove category tags\n",
    "    content = re.sub(r'\\[\\[Category:.*?\\]\\]', '', content)\n",
    "    \n",
    "    # Remove file/image links\n",
    "    content = re.sub(r'\\[\\[File:.*?\\]\\]', '', content)\n",
    "    \n",
    "    # Remove stub templates\n",
    "    content = re.sub(r'\\{\\{stub\\}\\}', '', content)\n",
    "    \n",
    "    # Remove empty lines that might remain\n",
    "    content = '\\n'.join(line for line in content.split('\\n') if line.strip())\n",
    "    \n",
    "    return content.strip()\n",
    "\n",
    "def create_article_dict(jsonl_path, sample_size: Optional[int] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create a dictionary of articles from the wiki parser, with cleaned content.\n",
    "    \n",
    "    Args:\n",
    "        jsonl: the path to the saved wiki pages as [Title] -> [Text] key value format. \n",
    "        sample_size: Optional number of articles to include\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary with article titles as keys and cleaned content as values\n",
    "    \"\"\"\n",
    "    articles = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            article = json.loads(line)\n",
    "            if sample_size is not None and i >= sample_size:\n",
    "                break\n",
    "                \n",
    "            # Clean the content to start from title\n",
    "            cleaned_content = clean_wiki_content(article['text'])\n",
    "            \n",
    "            if cleaned_content:  # Only include if content remains after cleaning\n",
    "                articles[article['title'].lower()] = cleaned_content\n",
    "            \n",
    "    return articles\n",
    "\n",
    "def prepare_sources_targets(articles_dict, sample_size=1000, cite_sample=1):\n",
    "    articles = np.random.permutation(list(articles_dict.keys()))[:sample_size]\n",
    "    # Initialize your model (assuming it's defined elsewhere)\n",
    "    model = AutoregressiveCitationMatcher()\n",
    "    sources = []\n",
    "    targets = []\n",
    "    for article in tqdm.tqdm(articles):\n",
    "        source_text = articles_dict[article]\n",
    "        # Get all citations from this source article\n",
    "        citations = extract_citations(source_text)\n",
    "        valid_citations = [c for c in citations if c.lower() in articles_dict]\n",
    "    \n",
    "        for citation in np.random.choice(valid_citations,min(cite_sample,len(valid_citations))):\n",
    "            try:\n",
    "                source_content = model.prepare_source_context(source_text, citation)\n",
    "                target_content = model.prepare_target_page(articles_dict[citation.lower()])\n",
    "                sources.append(source_content)\n",
    "                targets.append(target_content)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return sources, targets \n",
    "\n",
    "\n",
    "class AutoregressiveCitationMatcher(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        max_length: int = 512,\n",
    "        cite_token: str = \"<CITE>\",\n",
    "        ref_token: str = \"<REF>\",\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Add special tokens for citation and reference\n",
    "        special_tokens = {\n",
    "            'additional_special_tokens': [cite_token, ref_token]\n",
    "        }\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # For GPT models, we might need to set pad token if it's not set\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize encoder for both source and target texts\n",
    "        # We use the same model for both since autoregressive models maintain context\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # Resize token embeddings\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.cite_token = cite_token\n",
    "        self.ref_token = ref_token\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def prepare_source_context(\n",
    "        self,\n",
    "        text: str,\n",
    "        target_citation: str,\n",
    "        window_size: int = 100\n",
    "    ) -> str:\n",
    "        \"\"\"Prepare source context with citation and reference tokens.\"\"\"\n",
    "        citation_pattern = re.escape(f\"[[{target_citation}]]\")\n",
    "        # Add both <CITE> and <REF> tokens\n",
    "        modified_text = re.sub(citation_pattern, f\"{self.cite_token}\", text)\n",
    "        # modified_text = f\"{modified_text} {self.ref_token}\"\n",
    "\n",
    "        if self.cite_token not in modified_text:\n",
    "            raise ValueError(f\"There is no cite token\")\n",
    "        \n",
    "        return modified_text\n",
    "\n",
    "    def prepare_target_page(\n",
    "        self,\n",
    "        page_content: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Prepare target page with reference token.\"\"\"\n",
    "        # Take first paragraph as summary and add <REF> token\n",
    "            \n",
    "        return f\"{page_content} {self.ref_token}\"\n",
    "\n",
    "    def get_token_embedding(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        token_id: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Extract embedding for specific token for each item in the batch.\"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        embeddings = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            # Find token positions for this batch item\n",
    "            token_positions = (input_ids[batch_idx] == token_id).nonzero()\n",
    "            \n",
    "            if len(token_positions) == 0:\n",
    "                raise ValueError(f\"Token id {token_id} not found in sequence for batch item {batch_idx}\")\n",
    "                    \n",
    "            if len(token_positions) > 1:\n",
    "                # If multiple occurrences, take the last one\n",
    "                position = token_positions[-1].item()\n",
    "            else:\n",
    "                position = token_positions[0].item()\n",
    "                    \n",
    "            # Extract embedding for this batch item\n",
    "            embedding = hidden_states[batch_idx, position, :]  # Explicitly select all hidden dimensions\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        # Stack all embeddings into a single tensor\n",
    "        return torch.stack(embeddings)  # Shape will be [batch_size, hidden_size]\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        is_source: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode text and extract reference token embedding.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Get hidden states from the model\n",
    "        outputs = self.model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get [REF] token embedding from the last hidden state\n",
    "        ref_token_id = self.tokenizer.convert_tokens_to_ids(self.ref_token)\n",
    "        ref_embeddings = self.get_token_embedding(\n",
    "            outputs.hidden_states[-1],\n",
    "            inputs['input_ids'],\n",
    "            ref_token_id\n",
    "        )\n",
    "        return ref_embeddings\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source_contexts: List[str],\n",
    "        target_pages: List[str],\n",
    "        temperature: float = 0.07\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute similarity between source and target [REF] embeddings.\"\"\"\n",
    "        # Encode all contexts and references\n",
    "        source_embeddings = []\n",
    "        target_embeddings = []\n",
    "        \n",
    "        for context in source_contexts:\n",
    "            source_emb = self.encode_text(context, is_source=True)\n",
    "            source_embeddings.append(source_emb)\n",
    "            \n",
    "        for page in target_pages:\n",
    "            target_emb = self.encode_text(page, is_source=False)\n",
    "            target_embeddings.append(target_emb)\n",
    "        \n",
    "        # Stack embeddings\n",
    "        source_embeddings = torch.cat(source_embeddings, dim=0)\n",
    "        target_embeddings = torch.cat(target_embeddings, dim=0)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        source_embeddings = nn.functional.normalize(source_embeddings, dim=-1)\n",
    "        target_embeddings = nn.functional.normalize(target_embeddings, dim=-1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity = torch.matmul(\n",
    "            source_embeddings,\n",
    "            target_embeddings.transpose(0, 1)\n",
    "        ) / temperature\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    def train_step(\n",
    "        self,\n",
    "        source_contexts: List[str],\n",
    "        target_pages: List[str],\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        temperature: float = 0.07\n",
    "    ) -> float:\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        similarity = self(source_contexts, target_pages, temperature)\n",
    "        \n",
    "        # The diagonal elements should be the positive pairs\n",
    "        labels = torch.arange(len(source_contexts)).to(self.device)\n",
    "        # print('#'*20)\n",
    "        # print('similarity = ', similarity, ', labels = ', labels)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.CrossEntropyLoss()(similarity, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "class CitationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sources: List[str],\n",
    "        targets: List[str],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset with preprocessing of all samples.\n",
    "        \n",
    "        Args:\n",
    "            sources: List of source texts\n",
    "            targets: List of target texts\n",
    "            tokenizer: Tokenizer to use\n",
    "            max_length: Maximum sequence length\n",
    "            verbose: Whether to print processing statistics\n",
    "        \"\"\"\n",
    "        assert len(sources) == len(targets), \"Sources and targets must have same length\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.ref_token = \"<REF>\"\n",
    "        self.cite_token = \"<CITE>\"\n",
    "        \n",
    "        # Get token IDs\n",
    "        self.ref_token_id = self.tokenizer.convert_tokens_to_ids(self.ref_token)\n",
    "        self.cite_token_id = self.tokenizer.convert_tokens_to_ids(self.cite_token)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Pre-processing dataset...\")\n",
    "            print(f\"Initial samples: {len(sources)}\")\n",
    "        \n",
    "        # Pre-process and store all valid samples\n",
    "        self.processed_samples = self._preprocess_samples(sources, targets, verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Final processed samples: {len(self.processed_samples)}\")\n",
    "            print(\"Dataset preprocessing completed.\")\n",
    "\n",
    "    def _preprocess_samples(\n",
    "        self,\n",
    "        sources: List[str],\n",
    "        targets: List[str],\n",
    "        verbose: bool\n",
    "    ) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Pre-process all samples and filter invalid ones.\n",
    "        Returns list of valid processed samples.\n",
    "        \"\"\"\n",
    "        processed_samples = []\n",
    "        skipped_no_cite = 0\n",
    "        skipped_errors = 0\n",
    "        \n",
    "        for idx, (source, target) in enumerate(zip(sources, targets)):\n",
    "            try:\n",
    "                # Tokenize source and target\n",
    "                source_tokens = self.tokenizer(\n",
    "                    source,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                target_tokens = self.tokenizer(\n",
    "                    target,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                \n",
    "                # Ensure REF token in both source and target\n",
    "                # source_tokens['input_ids'] = self.ensure_ref_token(source_tokens['input_ids'])\n",
    "                target_tokens['input_ids'] = self.ensure_ref_token(target_tokens['input_ids'])\n",
    "                \n",
    "                # Store processed tensors\n",
    "                processed_sample = {\n",
    "                    'source_input_ids': source_tokens['input_ids'].squeeze(0),\n",
    "                    'source_attention_mask': source_tokens['attention_mask'].squeeze(0),\n",
    "                    'target_input_ids': target_tokens['input_ids'].squeeze(0),\n",
    "                    'target_attention_mask': target_tokens['attention_mask'].squeeze(0)\n",
    "                }\n",
    "                \n",
    "                # Check if CITE token exists in source after tokenization\n",
    "                if self.cite_token_id not in source_tokens['input_ids'][0]:\n",
    "                    skipped_no_cite += 1\n",
    "                    continue\n",
    "                \n",
    "                processed_samples.append(processed_sample)\n",
    "                \n",
    "            except Exception as e:\n",
    "                skipped_errors += 1\n",
    "                if verbose:\n",
    "                    print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if verbose and (idx + 1) % 10000 == 0:\n",
    "                print(f\"Processed {idx + 1} samples...\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Skipped {skipped_no_cite} samples due to missing CITE token\")\n",
    "            print(f\"Skipped {skipped_errors} samples due to processing errors\")\n",
    "        \n",
    "        return processed_samples\n",
    "\n",
    "    def ensure_ref_token(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Ensure the REF token is present in the sequence, replacing last non-pad token if needed.\"\"\"\n",
    "        batch_size = tokens.size(0)\n",
    "        \n",
    "        # For each sequence in the batch\n",
    "        for i in range(batch_size):\n",
    "            sequence = tokens[i]\n",
    "            # Find positions of REF token\n",
    "            ref_positions = (sequence == self.ref_token_id).nonzero()\n",
    "            \n",
    "            if len(ref_positions) == 0:\n",
    "                # If no REF token found, replace the last non-pad token\n",
    "                pad_token_id = self.tokenizer.pad_token_id\n",
    "                # Find the last non-pad token position\n",
    "                non_pad_positions = (sequence != pad_token_id).nonzero()\n",
    "                if len(non_pad_positions) > 0:\n",
    "                    last_non_pad_pos = non_pad_positions[-1]\n",
    "                    sequence[last_non_pad_pos] = self.ref_token_id\n",
    "            elif len(ref_positions) > 1:\n",
    "                # If multiple REF tokens, keep only the last one\n",
    "                for pos in ref_positions[:-1]:\n",
    "                    sequence[pos] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.unk_token)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_samples)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Return pre-processed sample.\"\"\"\n",
    "        return self.processed_samples[idx]\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Custom collate function to handle batching of tokenized inputs.\"\"\"\n",
    "    return {\n",
    "        key: torch.stack([item[key] for item in batch])\n",
    "        for key in batch[0].keys()\n",
    "    }\n",
    "\n",
    "def train_model(\n",
    "    model: AutoregressiveCitationMatcher,\n",
    "    train_sources: List[str],\n",
    "    train_targets: List[str],\n",
    "    val_sources: List[str] = None,\n",
    "    val_targets: List[str] = None,\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    temperature: float = 0.07,\n",
    "    num_workers: int = 4,\n",
    "    validation_steps: int = 100,\n",
    "    save_path: str = 'checkpoint.pt'\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-optimized training function that only keeps the last hidden layer.\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = CitationDataset(\n",
    "        train_sources,\n",
    "        train_targets,\n",
    "        model.tokenizer,\n",
    "        model.max_length\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    if val_sources and val_targets:\n",
    "        val_dataset = CitationDataset(\n",
    "            val_sources,\n",
    "            val_targets,\n",
    "            model.tokenizer,\n",
    "            model.max_length\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criteria = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass - only get last hidden layer\n",
    "            source_outputs = model.model(\n",
    "                input_ids=batch['source_input_ids'],\n",
    "                attention_mask=batch['source_attention_mask'],\n",
    "                output_hidden_states=False,  # Only get last layer\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            target_outputs = model.model(\n",
    "                input_ids=batch['target_input_ids'],\n",
    "                attention_mask=batch['target_attention_mask'],\n",
    "                output_hidden_states=False,  # Only get last layer\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Get token positions efficiently\n",
    "            cite_token_id = model.tokenizer.convert_tokens_to_ids(model.cite_token)\n",
    "            ref_token_id = model.tokenizer.convert_tokens_to_ids(model.ref_token)\n",
    "\n",
    "            source_embeddings = model.get_token_embedding(\n",
    "                source_outputs.last_hidden_state,\n",
    "                batch['source_input_ids'],\n",
    "                cite_token_id\n",
    "            )\n",
    "            \n",
    "            target_embeddings = model.get_token_embedding(\n",
    "                target_outputs.last_hidden_state,\n",
    "                batch['target_input_ids'],\n",
    "                ref_token_id\n",
    "            )\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            source_embeddings = torch.nn.functional.normalize(source_embeddings, dim=-1)\n",
    "            target_embeddings = torch.nn.functional.normalize(target_embeddings, dim=-1)\n",
    "            \n",
    "            # Compute similarity matrix\n",
    "            similarity = torch.matmul(source_embeddings, target_embeddings.transpose(0, 1)) / temperature\n",
    "            \n",
    "            # Compute loss\n",
    "            labels = torch.arange(similarity.size(0)).to(model.device)\n",
    "            loss = criteria(similarity, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss': f'{total_loss / num_batches:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Free memory\n",
    "            del source_outputs, target_outputs, source_embeddings, target_embeddings, similarity\n",
    "            torch.cuda.empty_cache()  # Optional, can help with memory fragmentation\n",
    "            \n",
    "            \n",
    "        # Validation\n",
    "        if val_sources and val_targets:\n",
    "            val_loss = validate_model(model, val_loader, temperature)\n",
    "            scheduler.step(val_loss)\n",
    "    \n",
    "            print(f\"Validation loss: {val_loss:0.4f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"\\nSaved new best model with validation loss: {val_loss:.4f}\")\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "                \n",
    "        epoch_loss = total_loss / num_batches\n",
    "        print(f\"\\nEpoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "def validate_model(\n",
    "    model: AutoregressiveCitationMatcher,\n",
    "    val_loader: DataLoader,\n",
    "    temperature: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Memory-optimized validation function that only keeps the last hidden layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(val_loader, desc='Validation'):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass - only get last hidden layer\n",
    "            source_outputs = model.model(\n",
    "                input_ids=batch['source_input_ids'],\n",
    "                attention_mask=batch['source_attention_mask'],\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            target_outputs = model.model(\n",
    "                input_ids=batch['target_input_ids'],\n",
    "                attention_mask=batch['target_attention_mask'],\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Get token positions efficiently\n",
    "                cite_token_id = model.tokenizer.convert_tokens_to_ids(model.cite_token)\n",
    "                ref_token_id = model.tokenizer.convert_tokens_to_ids(model.ref_token)\n",
    "                \n",
    "                source_embeddings = model.get_token_embedding(\n",
    "                    source_outputs.last_hidden_state,\n",
    "                    batch['source_input_ids'],\n",
    "                    cite_token_id\n",
    "                )\n",
    "                \n",
    "                target_embeddings = model.get_token_embedding(\n",
    "                    target_outputs.last_hidden_state,\n",
    "                    batch['target_input_ids'],\n",
    "                    ref_token_id\n",
    "                )\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                source_embeddings = torch.nn.functional.normalize(source_embeddings, dim=-1)\n",
    "                target_embeddings = torch.nn.functional.normalize(target_embeddings, dim=-1)\n",
    "                \n",
    "                # Compute similarity and loss\n",
    "                similarity = torch.matmul(source_embeddings, target_embeddings.transpose(0, 1)) / temperature\n",
    "                \n",
    "                labels = torch.arange(similarity.size(0)).to(model.device)\n",
    "                loss = torch.nn.CrossEntropyLoss()(similarity, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Free memory\n",
    "                del source_outputs, target_outputs, source_embeddings, target_embeddings, similarity\n",
    "                \n",
    "            except ValueError as e:\n",
    "                continue\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "# Usage example:\n",
    "# Split data into train and validation sets\n",
    "def split_data(sources: List[str], targets: List[str], val_ratio: float = 0.1) -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"Split data into training and validation sets.\"\"\"\n",
    "    val_size = int(len(sources) * val_ratio)\n",
    "    indices = torch.randperm(len(sources))\n",
    "    \n",
    "    train_indices = indices[val_size:]\n",
    "    val_indices = indices[:val_size]\n",
    "    \n",
    "    train_sources = [sources[i] for i in train_indices]\n",
    "    train_targets = [targets[i] for i in train_indices]\n",
    "    val_sources = [sources[i] for i in val_indices]\n",
    "    val_targets = [targets[i] for i in val_indices]\n",
    "    \n",
    "    return train_sources, train_targets, val_sources, val_targets\n",
    "\n",
    "\n",
    "\n",
    "# wiki_parser = WikiDumpParser('./data/wiki/simplewiki-latest-pages-articles.xml')\n",
    "# wiki_parser.save_to_jsonl('./wiki_articles.jsonl')\n",
    "\n",
    "articles_dict = create_article_dict(jsonl_path='./wiki_articles.jsonl')\n",
    "sources, targets = prepare_sources_targets(articles_dict, sample_size = 2000)\n",
    "\n",
    "# Split the data\n",
    "train_sources, train_targets, val_sources, val_targets = split_data(sources, targets)\n",
    "\n",
    "# Initialize model (assuming it's already defined)\n",
    "model = AutoregressiveCitationMatcher()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Train the model\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_sources=train_sources,\n",
    "    train_targets=train_targets,\n",
    "    val_sources=val_sources,\n",
    "    val_targets=val_targets,\n",
    "    batch_size=16,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1.5e-4,\n",
    "    temperature=0.1,\n",
    "    num_workers=4,\n",
    "    validation_steps=50,\n",
    "    save_path='best_model.pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e45919-7df5-473e-991b-8c2f6adfcedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051ba1cc-d1d3-4cf5-9fbb-e82f0e4186a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_752646/2296136602.py:228: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Memory allocated: 3.99 GB\n",
      "Max memory allocated: 12.00 GB\n",
      "Memory cached: 12.49 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                     | 0/11028 [00:00<?, ?it/s]/tmp/ipykernel_752646/2296136602.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10:   0%|                           | 1/11028 [00:01<3:07:07,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                           | 2/11028 [00:01<1:44:26,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                           | 3/11028 [00:01<1:18:49,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                           | 4/11028 [00:01<1:06:40,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                             | 5/11028 [00:02<58:22,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                             | 6/11028 [00:02<54:14,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                             | 7/11028 [00:02<51:24,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                             | 8/11028 [00:02<48:02,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                             | 9/11028 [00:02<46:52,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 10/11028 [00:03<45:38,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 11/11028 [00:03<43:51,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 12/11028 [00:03<44:01,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 14/11028 [00:04<40:54,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 15/11028 [00:04<41:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 16/11028 [00:04<40:02,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 17/11028 [00:04<39:15,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 19/11028 [00:05<38:43,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 20/11028 [00:05<37:58,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 22/11028 [00:05<38:18,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 23/11028 [00:05<37:37,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 25/11028 [00:06<37:42,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 26/11028 [00:06<37:02,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 28/11028 [00:06<36:42,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 30/11028 [00:07<36:25,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 31/11028 [00:07<36:29,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 32/11028 [00:07<36:37,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 34/11028 [00:08<36:47,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 36/11028 [00:08<36:24,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 38/11028 [00:08<36:18,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 40/11028 [00:09<36:00,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 42/11028 [00:09<36:28,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 44/11028 [00:10<35:59,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 46/11028 [00:10<35:34,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                            | 48/11028 [00:10<35:53,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|▏                           | 50/11028 [00:11<36:10,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|▏                           | 52/11028 [00:11<35:50,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|▏                           | 54/11028 [00:12<35:59,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 56/11028 [00:12<36:09,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 57/11028 [00:12<36:21,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 59/11028 [00:13<36:19,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 61/11028 [00:13<36:14,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 63/11028 [00:13<35:56,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 64/11028 [00:14<35:40,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 66/11028 [00:14<36:54,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 68/11028 [00:14<36:39,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 70/11028 [00:15<36:44,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 72/11028 [00:15<36:10,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 74/11028 [00:16<36:14,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 76/11028 [00:16<36:16,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 78/11028 [00:16<35:56,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 80/11028 [00:17<35:38,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 82/11028 [00:17<35:26,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 84/11028 [00:18<35:28,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 86/11028 [00:18<35:58,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 87/11028 [00:18<35:55,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 88/11028 [00:18<36:08,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 90/11028 [00:19<36:11,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 92/11028 [00:19<36:05,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 93/11028 [00:19<36:56,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 95/11028 [00:20<36:22,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 97/11028 [00:20<35:50,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                           | 98/11028 [00:20<35:53,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                           | 99/11028 [00:21<36:14,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                          | 101/11028 [00:21<36:29,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▏                          | 102/11028 [00:21<36:14,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 104/11028 [00:22<36:25,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 106/11028 [00:22<35:56,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 108/11028 [00:22<35:26,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 110/11028 [00:23<35:20,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 112/11028 [00:23<35:15,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 114/11028 [00:24<35:03,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 116/11028 [00:24<35:08,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 118/11028 [00:24<35:31,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 120/11028 [00:25<35:10,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 122/11028 [00:25<34:52,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 124/11028 [00:25<34:56,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 126/11028 [00:26<35:07,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 128/11028 [00:26<35:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 130/11028 [00:27<34:48,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 132/11028 [00:27<34:48,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 134/11028 [00:27<35:09,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 136/11028 [00:28<35:11,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 138/11028 [00:28<35:21,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 140/11028 [00:29<34:38,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 142/11028 [00:29<35:16,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 144/11028 [00:29<35:06,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 146/11028 [00:30<35:05,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 148/11028 [00:30<34:43,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 150/11028 [00:30<34:41,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▎                          | 152/11028 [00:31<35:04,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▍                          | 154/11028 [00:31<35:02,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▍                          | 156/11028 [00:32<34:44,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▍                          | 158/11028 [00:32<34:45,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▍                          | 160/11028 [00:32<34:23,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▍                          | 162/11028 [00:33<34:57,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|▍                          | 164/11028 [00:33<35:04,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 166/11028 [00:34<35:17,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 168/11028 [00:34<35:01,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 169/11028 [00:34<34:54,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 171/11028 [00:35<35:32,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 173/11028 [00:35<35:39,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 175/11028 [00:35<35:18,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 176/11028 [00:36<35:12,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 178/11028 [00:36<35:42,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 180/11028 [00:36<35:21,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 182/11028 [00:37<35:44,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 184/11028 [00:37<35:43,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 186/11028 [00:37<35:55,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 188/11028 [00:38<35:07,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 190/11028 [00:38<35:07,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 192/11028 [00:39<35:13,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 193/11028 [00:39<35:13,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 195/11028 [00:39<35:40,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 196/11028 [00:39<35:42,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 198/11028 [00:40<35:24,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 200/11028 [00:40<35:24,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 202/11028 [00:41<35:22,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▍                          | 204/11028 [00:41<35:07,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 206/11028 [00:41<34:54,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 208/11028 [00:42<34:38,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 210/11028 [00:42<34:55,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 212/11028 [00:43<34:29,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 214/11028 [00:43<34:40,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 216/11028 [00:43<34:47,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 218/11028 [00:44<35:10,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 220/11028 [00:44<34:56,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 222/11028 [00:45<35:18,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 224/11028 [00:45<35:29,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 225/11028 [00:45<35:26,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 227/11028 [00:45<35:24,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 228/11028 [00:46<35:05,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 230/11028 [00:46<35:40,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 232/11028 [00:46<35:19,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 233/11028 [00:47<36:15,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 235/11028 [00:47<35:44,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 236/11028 [00:47<35:07,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 237/11028 [00:47<35:36,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 239/11028 [00:48<35:36,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 241/11028 [00:48<35:53,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 243/11028 [00:49<35:33,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 244/11028 [00:49<35:31,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 246/11028 [00:49<35:44,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 247/11028 [00:49<35:28,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 250/11028 [00:50<35:16,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 252/11028 [00:50<35:05,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▌                          | 254/11028 [00:51<35:12,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 256/11028 [00:51<34:58,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 257/11028 [00:51<35:02,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 258/11028 [00:52<35:22,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 259/11028 [00:52<35:36,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 261/11028 [00:52<35:50,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 263/11028 [00:53<35:06,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 265/11028 [00:53<35:02,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 267/11028 [00:53<35:10,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 269/11028 [00:54<35:17,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 271/11028 [00:54<35:03,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 273/11028 [00:55<35:08,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▋                          | 275/11028 [00:55<34:49,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 277/11028 [00:55<35:25,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 279/11028 [00:56<35:21,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 281/11028 [00:56<35:27,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 283/11028 [00:57<35:24,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 284/11028 [00:57<35:36,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 286/11028 [00:57<35:48,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 288/11028 [00:58<35:52,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 290/11028 [00:58<35:38,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 292/11028 [00:58<35:05,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 294/11028 [00:59<35:06,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 296/11028 [00:59<35:17,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 298/11028 [01:00<35:11,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 299/11028 [01:00<35:17,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 301/11028 [01:00<35:16,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 303/11028 [01:01<35:16,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▋                          | 305/11028 [01:01<35:32,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 307/11028 [01:01<35:28,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 309/11028 [01:02<35:24,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 311/11028 [01:02<35:15,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 313/11028 [01:03<35:05,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 314/11028 [01:03<35:22,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 315/11028 [01:03<35:45,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 317/11028 [01:03<35:52,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 319/11028 [01:04<35:25,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 321/11028 [01:04<35:27,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 323/11028 [01:04<35:05,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 325/11028 [01:05<35:01,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 327/11028 [01:05<35:02,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 328/11028 [01:05<35:26,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 331/11028 [01:06<34:49,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 333/11028 [01:06<35:04,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 335/11028 [01:07<35:14,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 337/11028 [01:07<35:04,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 339/11028 [01:08<34:46,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 341/11028 [01:08<34:41,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 342/11028 [01:08<34:50,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 344/11028 [01:09<35:16,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 345/11028 [01:09<35:05,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   3%|▊                          | 347/11028 [01:09<35:03,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch due to error: There should only be one occurance of REF token\n",
      "Skipping batch due to error: There should only be one occurance of REF token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/amir/miniconda3/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Epoch 1/10:   3%|▊                          | 347/11028 [01:10<35:55,  4.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 398\u001b[0m\n\u001b[1;32m    395\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoregressiveCitationMatcher()\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# Train with memory optimizations\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced batch size\u001b[39;49;00m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Effective batch size = 8 * 4 = 32\u001b[39;49;00m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.07\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your GPU memory\u001b[39;49;00m\n\u001b[1;32m    413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 264\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_sources, train_targets, val_sources, val_targets, batch_size, gradient_accumulation_steps, num_epochs, learning_rate, temperature, num_workers, validation_steps, save_path, max_length)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# Process source sequences\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     source_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    258\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    259\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    260\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     )\n\u001b[0;32m--> 264\u001b[0m     \u001b[43mclean_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# Process target sequences\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     target_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    268\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    269\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    270\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mclean_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_memory\u001b[39m():\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clean up GPU memory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "class CitationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sources: List[str],\n",
    "        targets: List[str],\n",
    "        tokenizer,\n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        assert len(sources) == len(targets), \"Sources and targets must have same length\"\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        source = self.sources[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Tokenize source and target texts\n",
    "        source_tokens = self.tokenizer(\n",
    "            source,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        target_tokens = self.tokenizer(\n",
    "            target,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove the batch dimension added by tokenizer\n",
    "        return {\n",
    "            'source_input_ids': source_tokens['input_ids'].squeeze(0),\n",
    "            'source_attention_mask': source_tokens['attention_mask'].squeeze(0),\n",
    "            'target_input_ids': target_tokens['input_ids'].squeeze(0),\n",
    "            'target_attention_mask': target_tokens['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Custom collate function to handle batching of tokenized inputs.\"\"\"\n",
    "    return {\n",
    "        key: torch.stack([item[key] for item in batch])\n",
    "        for key in batch[0].keys()\n",
    "    }\n",
    "\n",
    "def print_gpu_memory_status():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Clean up GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def validate_model(\n",
    "    model: AutoregressiveCitationMatcher,\n",
    "    val_loader: DataLoader,\n",
    "    temperature: float,\n",
    "    scaler: GradScaler\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Validate the model on validation set.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            with autocast():\n",
    "                # Process source sequences\n",
    "                source_outputs = model.model(\n",
    "                    input_ids=batch['source_input_ids'],\n",
    "                    attention_mask=batch['source_attention_mask'],\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                clean_memory()\n",
    "                \n",
    "                # Process target sequences\n",
    "                target_outputs = model.model(\n",
    "                    input_ids=batch['target_input_ids'],\n",
    "                    attention_mask=batch['target_attention_mask'],\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    # Get embeddings and compute loss\n",
    "                    ref_token_id = model.tokenizer.convert_tokens_to_ids(model.ref_token)\n",
    "                    \n",
    "                    source_embeddings = model.get_token_embedding(\n",
    "                        source_outputs.hidden_states[-1],\n",
    "                        batch['source_input_ids'],\n",
    "                        ref_token_id\n",
    "                    )\n",
    "                    \n",
    "                    target_embeddings = model.get_token_embedding(\n",
    "                        target_outputs.hidden_states[-1],\n",
    "                        batch['target_input_ids'],\n",
    "                        ref_token_id\n",
    "                    )\n",
    "                    \n",
    "                    # Normalize embeddings\n",
    "                    source_embeddings = torch.nn.functional.normalize(source_embeddings, dim=-1)\n",
    "                    target_embeddings = torch.nn.functional.normalize(target_embeddings, dim=-1)\n",
    "                    \n",
    "                    # Compute similarity and loss\n",
    "                    similarity = torch.matmul(\n",
    "                        source_embeddings,\n",
    "                        target_embeddings.transpose(0, 1)\n",
    "                    ) / temperature\n",
    "                    \n",
    "                    labels = torch.arange(similarity.size(0)).to(model.device)\n",
    "                    loss = torch.nn.CrossEntropyLoss()(similarity, labels)\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping validation batch due to error: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                clean_memory()\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "def train_model(\n",
    "    model: AutoregressiveCitationMatcher,\n",
    "    train_sources: List[str],\n",
    "    train_targets: List[str],\n",
    "    val_sources: List[str] = None,\n",
    "    val_targets: List[str] = None,\n",
    "    batch_size: int = 8,\n",
    "    gradient_accumulation_steps: int = 4,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    temperature: float = 0.07,\n",
    "    num_workers: int = 4,\n",
    "    validation_steps: int = 100,\n",
    "    save_path: str = 'checkpoint.pt',\n",
    "    max_length: int = 512\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the citation matcher model with memory optimizations.\n",
    "    \n",
    "    Args:\n",
    "        model: The AutoregressiveCitationMatcher model\n",
    "        train_sources: List of source contexts\n",
    "        train_targets: List of target pages\n",
    "        val_sources: Optional validation source contexts\n",
    "        val_targets: Optional validation target pages\n",
    "        batch_size: Batch size for training\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        temperature: Temperature for similarity computation\n",
    "        num_workers: Number of workers for data loading\n",
    "        validation_steps: Number of steps between validations\n",
    "        save_path: Path to save the best model\n",
    "        max_length: Maximum sequence length\n",
    "    \"\"\"\n",
    "    print(f\"Using device: {model.device}\")\n",
    "    print_gpu_memory_status()\n",
    "    \n",
    "    # Create datasets with specified max_length\n",
    "    train_dataset = CitationDataset(\n",
    "        train_sources,\n",
    "        train_targets,\n",
    "        model.tokenizer,\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    if val_sources and val_targets:\n",
    "        val_dataset = CitationDataset(\n",
    "            val_sources,\n",
    "            val_targets,\n",
    "            model.tokenizer,\n",
    "            max_length\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    # Initialize optimizer and scaler for mixed precision training\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast():\n",
    "                try:\n",
    "                    # Process source sequences\n",
    "                    source_outputs = model.model(\n",
    "                        input_ids=batch['source_input_ids'],\n",
    "                        attention_mask=batch['source_attention_mask'],\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    \n",
    "                    clean_memory()\n",
    "                    \n",
    "                    # Process target sequences\n",
    "                    target_outputs = model.model(\n",
    "                        input_ids=batch['target_input_ids'],\n",
    "                        attention_mask=batch['target_attention_mask'],\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    \n",
    "                    # Get [REF] token embeddings\n",
    "                    ref_token_id = model.tokenizer.convert_tokens_to_ids(model.ref_token)\n",
    "                    \n",
    "                    source_embeddings = model.get_token_embedding(\n",
    "                        source_outputs.hidden_states[-1],\n",
    "                        batch['source_input_ids'],\n",
    "                        ref_token_id\n",
    "                    )\n",
    "                    \n",
    "                    target_embeddings = model.get_token_embedding(\n",
    "                        target_outputs.hidden_states[-1],\n",
    "                        batch['target_input_ids'],\n",
    "                        ref_token_id\n",
    "                    )\n",
    "                    \n",
    "                    # Normalize embeddings\n",
    "                    source_embeddings = torch.nn.functional.normalize(source_embeddings, dim=-1)\n",
    "                    target_embeddings = torch.nn.functional.normalize(target_embeddings, dim=-1)\n",
    "                    \n",
    "                    # Compute similarity matrix\n",
    "                    similarity = torch.matmul(\n",
    "                        source_embeddings,\n",
    "                        target_embeddings.transpose(0, 1)\n",
    "                    ) / temperature\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    labels = torch.arange(similarity.size(0)).to(model.device)\n",
    "                    loss = torch.nn.CrossEntropyLoss()(similarity, labels)\n",
    "                    \n",
    "                    # Scale loss by gradient accumulation steps\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                        # Unscale gradients and clip\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        \n",
    "                        # Step optimizer and scaler\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item() * gradient_accumulation_steps\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',\n",
    "                        'avg_loss': f'{total_loss / num_batches:.4f}'\n",
    "                    })\n",
    "                    \n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # Validation\n",
    "                    if val_sources and val_targets and global_step % validation_steps == 0:\n",
    "                        val_loss = validate_model(\n",
    "                            model,\n",
    "                            val_loader,\n",
    "                            temperature,\n",
    "                            scaler\n",
    "                        )\n",
    "                        \n",
    "                        # Update learning rate scheduler\n",
    "                        scheduler.step(val_loss)\n",
    "                        \n",
    "                        # Save best model\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            torch.save({\n",
    "                                'epoch': epoch,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'scaler_state_dict': scaler.state_dict(),\n",
    "                                'loss': best_val_loss,\n",
    "                            }, save_path)\n",
    "                            print(f\"\\nSaved new best model with validation loss: {val_loss:.4f}\")\n",
    "                        \n",
    "                        model.train()  # Switch back to training mode\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping batch due to error: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Clean memory periodically\n",
    "                if batch_idx % 10 == 0:\n",
    "                    clean_memory()\n",
    "                \n",
    "        # End of epoch\n",
    "        epoch_loss = total_loss / num_batches\n",
    "        print(f\"\\nEpoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        print_gpu_memory_status()\n",
    "\n",
    "def split_data(\n",
    "    sources: List[str],\n",
    "    targets: List[str],\n",
    "    val_ratio: float = 0.1\n",
    ") -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"Split data into training and validation sets.\"\"\"\n",
    "    val_size = int(len(sources) * val_ratio)\n",
    "    indices = torch.randperm(len(sources))\n",
    "    \n",
    "    train_indices = indices[val_size:]\n",
    "    val_indices = indices[:val_size]\n",
    "    \n",
    "    train_sources = [sources[i] for i in train_indices]\n",
    "    train_targets = [targets[i] for i in train_indices]\n",
    "    val_sources = [sources[i] for i in val_indices]\n",
    "    val_targets = [targets[i] for i in val_indices]\n",
    "    \n",
    "    return train_sources, train_targets, val_sources, val_targets\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Split the data\n",
    "    train_sources, train_targets, val_sources, val_targets = split_data(sources, targets)\n",
    "\n",
    "    # Initialize model\n",
    "    model = AutoregressiveCitationMatcher()\n",
    "\n",
    "    # Train with memory optimizations\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_sources=train_sources,\n",
    "        train_targets=train_targets,\n",
    "        val_sources=val_sources,\n",
    "        val_targets=val_targets,\n",
    "        batch_size=8,  # Reduced batch size\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8 * 4 = 32\n",
    "        num_epochs=10,\n",
    "        learning_rate=1e-4,\n",
    "        temperature=0.07,\n",
    "        num_workers=4,\n",
    "        validation_steps=100,\n",
    "        save_path='best_model.pt',\n",
    "        max_length=512  # Adjust based on your GPU memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb302bc-5802-4f5d-959c-4bdf4c9acdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f27a2-64ab-4277-ad92-4dc3b610b132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
