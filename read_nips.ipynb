{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a3c8888-1396-41bf-ae6f-a9151f65990c",
   "metadata": {},
   "source": [
    "# get nips paper links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e57043b-5bc7-46ec-b8b4-4815b7bb5bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>URL</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Federated Submodel Optimization for Hot and Co...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tan...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On Kernelized Multi-Armed Bandits with Constra...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>Xingyu Zhou, Bo Ji</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geometric Order Learning for Rank Estimation</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>Seon-Ho Lee, Nyeong Ho Shin, Chang-Su Kim</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Structured Recognition for Generative Models w...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>Changmin Yu, Hugo Soulat, Neil Burgess, Manees...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NAS-Bench-Graph: Benchmarking Graph Neural Arc...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhan...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Connecting to the Past</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>Bruce MacDonald</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>PARTITIONING OF SENSORY DATA BY A CORTICAL NET...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>Richard Granger, Jose Ambros-Ingerson, Howard ...</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>A Dynamical Approach to Temporal Pattern Proce...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>W. Stornetta, Tad Hogg, Bernardo Huberman</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Minkowski-r Back-Propagation: Learning in Conn...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>Stephen Hanson, David Burr</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Analysis and Comparison of Different Learning ...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/1987/...</td>\n",
       "      <td>J. Bernasconi</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16746 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Federated Submodel Optimization for Hot and Co...   \n",
       "1   On Kernelized Multi-Armed Bandits with Constra...   \n",
       "2        Geometric Order Learning for Rank Estimation   \n",
       "3   Structured Recognition for Generative Models w...   \n",
       "4   NAS-Bench-Graph: Benchmarking Graph Neural Arc...   \n",
       "..                                                ...   \n",
       "85                             Connecting to the Past   \n",
       "86  PARTITIONING OF SENSORY DATA BY A CORTICAL NET...   \n",
       "87  A Dynamical Approach to Temporal Pattern Proce...   \n",
       "88  Minkowski-r Back-Propagation: Learning in Conn...   \n",
       "89  Analysis and Comparison of Different Learning ...   \n",
       "\n",
       "                                                  URL  \\\n",
       "0   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "1   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "2   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "3   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "4   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "..                                                ...   \n",
       "85  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "86  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "87  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "88  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "89  https://papers.nips.cc/paper_files/paper/1987/...   \n",
       "\n",
       "                                              authors  year  \n",
       "0   Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tan...  2022  \n",
       "1                                  Xingyu Zhou, Bo Ji  2022  \n",
       "2           Seon-Ho Lee, Nyeong Ho Shin, Chang-Su Kim  2022  \n",
       "3   Changmin Yu, Hugo Soulat, Neil Burgess, Manees...  2022  \n",
       "4   Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhan...  2022  \n",
       "..                                                ...   ...  \n",
       "85                                    Bruce MacDonald  1987  \n",
       "86  Richard Granger, Jose Ambros-Ingerson, Howard ...  1987  \n",
       "87          W. Stornetta, Tad Hogg, Bernardo Huberman  1987  \n",
       "88                         Stephen Hanson, David Burr  1987  \n",
       "89                                      J. Bernasconi  1987  \n",
       "\n",
       "[16746 rows x 4 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_proceding_links(base_url, attrs={}):\n",
    "    # Send a GET request to retrieve the HTML content\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all <a> tags with the title and href attributes\n",
    "    paper_links = soup.find_all(\"a\", attrs={\"href\": True})\n",
    "\n",
    "    # Initialize empty lists to store the titles and URLs\n",
    "    titles = []\n",
    "    urls = []\n",
    "\n",
    "    # Extract the title and URL from each <a> tag and store them in the respective lists\n",
    "    for link in paper_links:\n",
    "        title = link.get_text(strip=True)\n",
    "        url = base_url + link[\"href\"]\n",
    "        if 'paper_files' not in url:\n",
    "            continue\n",
    "        titles.append(title)\n",
    "        urls.append(url)\n",
    "\n",
    "    # Create a pandas DataFrame from the collected data\n",
    "    df = pd.DataFrame({\"Title\": titles, \"URL\": urls})\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_paper_links(url, attrs={}):\n",
    "    # Send a GET request to retrieve the HTML content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all <a> tags with the title and href attributes\n",
    "    paper_links = soup.find_all(\"a\", attrs={\"title\": True, \"href\": True})\n",
    "\n",
    "    # Initialize empty lists to store the titles and URLs\n",
    "    titles = []\n",
    "    urls = []\n",
    "    authors = []\n",
    "    parsed_url = urlparse(url)\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "    base_url\n",
    "\n",
    "    # Extract the title and URL from each <a> tag and store them in the respective lists\n",
    "    for link in paper_links:\n",
    "        title = link.get_text(strip=True)\n",
    "        url = base_url + link[\"href\"]\n",
    "        author = link.find_next_sibling(\"i\").get_text(strip=True)\n",
    "\n",
    "        titles.append(title)\n",
    "        urls.append(url)\n",
    "        authors.append(author)\n",
    "\n",
    "    # Create a pandas DataFrame from the collected data\n",
    "    df = pd.DataFrame({\"title\": titles, \"URL\": urls, \"authors\": authors})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_metadata():\n",
    "    proceedings = parse_proceding_links('https://papers.nips.cc/')\n",
    "    proceedings['year'] = proceedings.URL.apply(lambda x:x.split('/')[-1])\n",
    "    dfs = []\n",
    "    for _,r in tqdm(proceedings.iterrows()):\n",
    "        df = parse_paper_links(r.URL)\n",
    "        df['year'] = r.year\n",
    "        dfs.append(df)\n",
    "    papers_metadata = pd.concat(dfs)\n",
    "    return papers_metadata\n",
    "\n",
    "# get more detailed data \n",
    "\n",
    "\n",
    "def paper_detailed_metadata(url):\n",
    "    # Send a GET request to retrieve the HTML content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the article title\n",
    "    title = soup.find(\"h4\").text.strip()\n",
    "\n",
    "    # Extract the authors\n",
    "    authors = soup.find(\"h4\", string=\"Authors\").find_next(\"p\").text.strip()\n",
    "\n",
    "    # Extract the abstract\n",
    "    abstract = soup.find(\"h4\", string=\"Abstract\").find_next(\"p\").text.strip()\n",
    "\n",
    "    # Extract the paper link\n",
    "    paper_link = soup.find(\"a\", string=\"Paper\")[\"href\"]\n",
    "\n",
    "    # Extract the supplementary link\n",
    "    supplementary_link = soup.find(\"a\", string=\"Supplemental\")[\"href\"]\n",
    "\n",
    "    # Extract the bibtex link\n",
    "    bibtex_link = soup.find(\"a\", string=\"Bibtex\")[\"href\"]\n",
    "\n",
    "    # Return the extracted information as a dictionary\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"author\": authors,\n",
    "        \"abstract\": abstract,\n",
    "        \"link\": paper_link,\n",
    "        \"supp_link\": supplementary_link,\n",
    "        \"bib_link\": bibtex_link,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Example usage \n",
    "# papers_meta = []\n",
    "# for url in tqdm(paper_links.iloc[:10].URL.unique()):\n",
    "#     try:\n",
    "#         papers_meta.append(paper_detailed_metadata(url))\n",
    "#     except:\n",
    "#         pass\n",
    "# papers_meta = pd.DataFrame(papers_meta)\n",
    "# papers_meta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "papers_metadata = get_metadata()\n",
    "papers_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f80b3-7e2e-4160-b673-c921ef7b84db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00953221321105957,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 16746,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16440dadbfb4192b81a1e490111eeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded ./sources/2109.07704v4.tar.gz\n",
      "Downloaded ./sources/2203.15589v1.tar.gz\n",
      "Downloaded ./sources/2209.05212v2.tar.gz\n",
      "Downloaded ./sources/2203.09675v3.tar.gz\n",
      "Downloaded ./sources/2205.11266v2.tar.gz\n",
      "Downloaded ./sources/2206.08155v2.tar.gz\n",
      "Downloaded ./sources/2210.08367v1.tar.gz\n",
      "Downloaded ./sources/2301.00355v2.tar.gz\n",
      "Downloaded ./sources/2201.13097v2.tar.gz\n",
      "Downloaded ./sources/2206.10870v1.tar.gz\n",
      "Downloaded ./sources/2103.16277v1.tar.gz\n",
      "Downloaded ./sources/2210.07702v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import math\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to search arXiv for a paper\n",
    "def search_arxiv(main_paper, start = 0, max_results = 10):\n",
    "    # filter items\n",
    "    paper_dict = main_paper.to_dict()\n",
    "    paper_dict = {k:v for k,v in paper_dict.items() if str(v)!='nan'}\n",
    "\n",
    "    # Define the base URL and query parameters\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    title = paper_dict.get('title', '')\n",
    "    authors = paper_dict.get('authors', '')\n",
    "    year = paper_dict.get('year', '')\n",
    "    abstract = paper_dict.get('abstract', '')\n",
    "\n",
    "    \n",
    "    # Construct the search query\n",
    "    search_query = f\"all:{title} all:{authors} all:{year}\"\n",
    "\n",
    "    # Construct the full URL\n",
    "    full_url = f\"{base_url}?search_query={search_query}&start={start}&max_results={max_results}\"\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(full_url)\n",
    "\n",
    "    # Parse the response\n",
    "    feed = feedparser.parse(response.content)\n",
    "\n",
    "    papers = []\n",
    "    for paper in feed.entries:\n",
    "        # print(paper)\n",
    "        paper.authors = ', '.join(author.name for author in paper.authors)\n",
    "        sim = paper_sim(main_paper,paper)\n",
    "        if sim >= .7:\n",
    "            papers.append(paper)\n",
    "    # Return the feed entries (i.e., the papers)\n",
    "    return papers\n",
    "\n",
    "def paper_sim(query_paper, paper):\n",
    "    # Convert titles and authors to lowercase\n",
    "    title1 = query_paper.title.lower()\n",
    "    authors1 = query_paper.authors.lower()\n",
    "    title2 = paper.title.lower()\n",
    "    authors2 = paper.authors.lower()\n",
    "\n",
    "    # Calculate Jaccard similarity for titles\n",
    "    title_set1 = set(title1.split())\n",
    "    title_set2 = set(title2.split())\n",
    "    title_similarity = len(title_set1.intersection(title_set2)) / len(title_set1.union(title_set2))\n",
    "\n",
    "    # Calculate Jaccard similarity for authors\n",
    "    authors_set1 = set(authors1.split(', '))\n",
    "    authors_set2 = set(authors2.split(', '))\n",
    "    authors_similarity = len(authors_set1.intersection(authors_set2)) / len(authors_set1.union(authors_set2))\n",
    "\n",
    "    # Calculate the overall similarity score as the average of title and authors similarity\n",
    "    similarity_score = (title_similarity + authors_similarity) / 2\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Function to download a file from a paper record\n",
    "def download_arxiv_source(paper):\n",
    "    arxiv_id = paper.id.split('/')[-1]\n",
    "    url = f\"https://arxiv.org/e-print/{arxiv_id}\"\n",
    "    local_filename = f\"./sources/{arxiv_id}.tar.gz\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    if local_filename:\n",
    "        print(f\"Downloaded {local_filename}\")\n",
    "    return local_filename\n",
    "\n",
    "bar = tqdm(papers_metadata.iloc[:20].iterrows(),total=len(papers_metadata))\n",
    "for _,main_paper in bar:\n",
    "    bar.set_postfix(title=main_paper.title)\n",
    "    papers = search_arxiv(main_paper, max_results=4)\n",
    "\n",
    "    for paper in papers:\n",
    "        download_arxiv_source(paper);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ace05ad-0f74-4b31-bd35-586665d82f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd310d78d8a44089b472ca841ac59d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Specify the directory containing the tar.gz files\n",
    "directory = './sources/'\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.gz'):\n",
    "        try:\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Create a directory name without the '.tar' extension\n",
    "            output_dir = os.path.splitext(filepath)[0]\n",
    "            output_dir = output_dir.replace('.tar', '')\n",
    "            output_dir = output_dir.replace('.gz', '')\n",
    "\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Extract the tar.gz file into the output directory\n",
    "            with tarfile.open(filepath, 'r:gz') as tar:\n",
    "                tar.extractall(output_dir)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b38eb2-21ff-4b96-9027-b54c2ff4837b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c378ccc-528b-4d1d-93cc-2bf3256093b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362a7dc-1e0f-467b-a8fb-adba1fb38fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
