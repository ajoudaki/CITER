{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:Loading articles from JSONL file...\n",
      "INFO:root:Loaded 237381 articles.\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:root:Loading cached tokenized results from cache/tokenized_1caf5def_895012ad817559b15b42e1d366769a67.pt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "from utils import *\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Tuple, Iterator, Union, Optional\n",
    "import tqdm\n",
    "import hashlib \n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[Category:.?\\]\\]|\\[\\[File:.?\\]\\]|\\{\\{stub\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources, citation_data, tokenizer, batch_size=1000, cache_dir=\"cache\"):\n",
    "    # Generate cache path\n",
    "    cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "    })\n",
    "\n",
    "    source_len = 512\n",
    "    summary_len = 100\n",
    "    overlap = 0.5\n",
    "    sample_size = 1\n",
    "\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "\n",
    "    all_ids = []\n",
    "    all_citation = []\n",
    "    for i in tqdm.trange(len(results)):\n",
    "        result = results[i]\n",
    "        for s in range(0, len(result['input_ids']), int((1-overlap)*source_len)):\n",
    "            e = s + source_len\n",
    "            input_ids = result['input_ids'][s:e]\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "\n",
    "            present_citations = cite_tokens[cite_tokens > 0]\n",
    "            N = min(sample_size, len(present_citations))\n",
    "            sample_citations = np.random.choice(present_citations, N, replace=False)\n",
    "            \n",
    "            cite_tokens = np.where(np.isin(cite_tokens, sample_citations), cite_tokens, 0)\n",
    "            mask_tokens = np.where(np.isin(mask_tokens, sample_citations), mask_tokens, 0)\n",
    "            mask_tokens[cite_tokens > 0] = 0\n",
    "\n",
    "            input_ids[cite_tokens > 0] = cite_token\n",
    "            input_ids = input_ids[mask_tokens == 0]\n",
    "            cite_tokens = cite_tokens[mask_tokens == 0]\n",
    "            all_ids.append(input_ids)\n",
    "            all_citation.append(sample_citations)\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load articles\n",
    "preprocessor = WikiProcessor()\n",
    "sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "config = ModelConfig()\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "\n",
    "# This will now use caching\n",
    "results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\")\n",
    "\n",
    "\n",
    "# collate \n",
    "# collate(results[:], tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/237381 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 407 but corresponding boolean dimension is 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 152\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: source_ids,\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcite_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: cite_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: citation_ids\n\u001b[1;32m    148\u001b[0m     }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Usage example:\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Collate the data\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m collated_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CitationDataset(collated_data)\n",
      "Cell \u001b[0;32mIn[52], line 66\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(results, tokenizer, config, max_targets, source_len, target_len, overlap)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Process each target\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, citation_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(present_citations):\n\u001b[0;32m---> 66\u001b[0m     cite_token_loc[cite_tokens \u001b[38;5;241m==\u001b[39m citation_id] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Get pre-tokenized target content\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     target_data \u001b[38;5;241m=\u001b[39m id_to_tokenized[citation_id \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 407 but corresponding boolean dimension is 7"
     ]
    }
   ],
   "source": [
    "def collate(results, tokenizer, config, max_targets=5, source_len = 512, target_len = 100, overlap = 0.5):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.trange(len(results)):\n",
    "        result = results[i]\n",
    "\n",
    "        if len(collated_data) > 1000:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-overlap)*source_len)):\n",
    "            e = s + source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, target_len), dtype=np.int64)\n",
    "            citation_ids = np.zeros(max_targets, dtype=np.int64)\n",
    "            \n",
    "            # Sample citations if we have more than max_targets\n",
    "            if len(present_citations) > max_targets:\n",
    "                present_citations = np.random.choice(present_citations, max_targets, replace=False)\n",
    "            \n",
    "            # Prepare source\n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), 0, mask_tokens)\n",
    "            mask_tokens[cite_tokens_mask > 0] = 0\n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "            cite_tokens = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > source_len:\n",
    "                source_ids = source_ids[:source_len]\n",
    "            elif len(source_ids) < source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                target_data = id_to_tokenized[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= target_len - 1:\n",
    "                    target_tokens = target_tokens[:target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                citation_ids[idx] = citation_id\n",
    "            \n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cite_tokens': torch.tensor(cite_tokens, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "                'target_count': len(present_citations),\n",
    "                'citation_ids': torch.tensor(citation_ids, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader that properly handles the stacked format.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dictionaries from CitationDataset\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with batched tensors where:\n",
    "        - source_ids: [batch_size, source_len]\n",
    "        - target_ids: [batch_size * max_targets, target_len]\n",
    "        - attention_mask: [batch_size, source_len]\n",
    "        - target_attention_mask: [batch_size * max_targets, target_len]\n",
    "        - target_counts: [batch_size]\n",
    "        - citation_ids: [batch_size * max_targets]\n",
    "    \"\"\"\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cite_tokens = torch.cat([item['cite_tokens'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_ids = torch.cat([item['target_ids'][:item['target_count']] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'][:item['target_count']] for item in batch])\n",
    "    citation_ids = torch.cat([item['citation_ids'][:item['target_count']] for item in batch])\n",
    "    target_counts = torch.tensor([item['target_count'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cite_tokens': cite_tokens,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'target_counts': target_counts,\n",
    "        'citation_ids': citation_ids\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "# Collate the data\n",
    "collated_data = collate(results, tokenizer, config, max_targets=5)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(83)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "(batch['source_ids']==cite_token).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['cite_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['source_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
