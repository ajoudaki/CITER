{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c927b80-0d8f-4a2c-81aa-4dab56aaa2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c4ca0f-3f5e-4254-8ad4-ddb87efe9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3151667/64198566.py:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./experiments/best_citation_model_4.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig\n",
    ")\n",
    "\n",
    "# library here \n",
    "from utils import * \n",
    "\n",
    "def retrieve_citations(\n",
    "    model,\n",
    "    query_text: str,\n",
    "    target_texts: list,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    k: int = 5,\n",
    "    device = None\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # First, process the query document similar to WikiProcessor._find_citations\n",
    "    citations = []\n",
    "    for match in re.finditer(r'\\[\\[(.*?)\\]\\]', query_text):\n",
    "        citations.append((match.start(), match.end(), match.group(1)))\n",
    "    \n",
    "    # Process query text similar to tokenize_sources\n",
    "    query_encoded = tokenizer.encode_plus(\n",
    "        query_text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Create offset to index mapping\n",
    "    offset_mapping = query_encoded[\"offset_mapping\"]\n",
    "    off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "    off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "    \n",
    "    # Create citation tokens array similar to tokenize_sources\n",
    "    input_ids = query_encoded[\"input_ids\"]\n",
    "    cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "    mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "    \n",
    "    # Fill in citations\n",
    "    citation_indices = []\n",
    "    for i, j, _ in citations:\n",
    "        s, e = off2i[i], off2i[j]\n",
    "        cite_tokens[s] = 1  # Using 1 as a placeholder\n",
    "        mask_tokens[s:e] = 1\n",
    "        citation_indices.append(s)\n",
    "    \n",
    "    # Prepare source similar to collate function\n",
    "    mask_tokens = np.where(np.isin(input_ids, tokenizer.convert_tokens_to_ids(['[',']'])), 1, mask_tokens)\n",
    "    mask_tokens[cite_tokens == 1] = 0\n",
    "    input_ids = np.array(input_ids)\n",
    "    input_ids[cite_tokens == 1] = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    source_ids = input_ids[mask_tokens == 0]\n",
    "    \n",
    "    # Pad or truncate source\n",
    "    if len(source_ids) > config.source_len:\n",
    "        source_ids = source_ids[:config.source_len]\n",
    "    else:\n",
    "        source_ids = np.pad(source_ids, \n",
    "                           (0, config.source_len - len(source_ids)),\n",
    "                           'constant', \n",
    "                           constant_values=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Process target texts\n",
    "    target_encoded = []\n",
    "    for target in target_texts:\n",
    "        tokens = tokenizer.encode_plus(\n",
    "            target,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        if len(tokens) >= config.target_len - 1:\n",
    "            tokens = tokens[:config.target_len-1]\n",
    "        tokens = np.append(tokens, tokenizer.convert_tokens_to_ids(config.ref_token))\n",
    "        \n",
    "        if len(tokens) < config.target_len:\n",
    "            tokens = np.pad(tokens,\n",
    "                          (0, config.target_len - len(tokens)),\n",
    "                          'constant',\n",
    "                          constant_values=tokenizer.pad_token_id)\n",
    "        \n",
    "        target_encoded.append(tokens)\n",
    "    \n",
    "    target_ids = torch.tensor(target_encoded, dtype=torch.long).to(device)\n",
    "    source_ids = torch.tensor(source_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = (source_ids != tokenizer.pad_token_id).to(device)\n",
    "    target_attention_mask = (target_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "    logit_scale = torch.clamp(model.logit_scale, 0, torch.log(torch.tensor(100.0)))\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get source embeddings\n",
    "        source_outputs = model.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get target embeddings\n",
    "        target_outputs = model.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Extract citation and reference embeddings\n",
    "        cite_mask = model.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        ref_mask = model.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) * logit_scale\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top k for each citation\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=min(k, len(target_texts)), dim=1)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for i, (_, _, citation_text) in enumerate(citations):\n",
    "        top_matches = []\n",
    "        for j, idx in enumerate(top_k_indices[i]):\n",
    "            top_matches.append({\n",
    "                'text': target_texts[idx],\n",
    "                'score': float(top_k_scores[i][j])\n",
    "            })\n",
    "        results.append({\n",
    "            'citation_text': citation_text,\n",
    "            'matches': top_matches\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "# Create model config\n",
    "config = ExperimentConfig(collate_sample_size=50000,)\n",
    "config.device = 'cuda:1'\n",
    "\n",
    "experiment = Experiment(config)\n",
    "\n",
    "tokenizer = experiment.get_tokenizer()\n",
    "\n",
    "model = experiment.get_model()\n",
    "# Initialize model\n",
    "checkpoint = torch.load('./experiments/best_citation_model_4.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f70aacc-83c4-4fb0-a0df-b45e3c478330",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation_metrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_metrics'"
     ]
    }
   ],
   "source": [
    "checkpoint['validation_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8749856c-8a76-4271-ace9-87d91ddd2c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[REF]] A breakthrough in image synthesis came with a two-stage approach using a low-dimensional latent space combined with an autoencoder, making high-quality image generation computationally feasible on consumer hardware.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.6845\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "2. Score: 0.1323\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "3. Score: 0.1080\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The discovery that asking large language models to break down their reasoning process into steps before providing a final answer dramatically improved their problem-solving abilities.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3214\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "2. Score: 0.3166\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "3. Score: 0.0837\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "4. Score: 0.0810\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "5. Score: 0.0665\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "6. Score: 0.0405\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] By treating images as sequences of patches and applying transformer architectures directly to these sequences, researchers demonstrated that convolutional neural networks weren't necessary for computer vision.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3799\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "2. Score: 0.1421\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "3. Score: 0.1204\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "4. Score: 0.0762\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "5. Score: 0.0662\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "6. Score: 0.0464\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "7. Score: 0.0393\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "8. Score: 0.0317\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] Through careful optimization of training procedures, including longer training times, larger batches, and dynamic masking patterns, researchers significantly enhanced the performance of bidirectional transformer models.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.2534\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "2. Score: 0.1418\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "3. Score: 0.1309\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "4. Score: 0.0982\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "5. Score: 0.0840\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "6. Score: 0.0687\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "7. Score: 0.0461\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "8. Score: 0.0317\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "9. Score: 0.0299\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "10. Score: 0.0242\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A major advance in computer vision came through training models to match images with natural language descriptions, creating visual representations that could generalize to any visual classification task.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.4194\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "2. Score: 0.2094\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "3. Score: 0.1046\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "4. Score: 0.0464\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "5. Score: 0.0370\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "6. Score: 0.0366\n",
      "Preview: Wav2Vec: Unsupervised Pre-training for Speech Recognition presents a self-supervised approach to speech representation learning, which can be fine-tuned with limited labeled data to achieve strong per...\n",
      "\n",
      "7. Score: 0.0295\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "8. Score: 0.0228\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The 540-billion parameter model trained on the Pathways system marked a significant milestone in language model capabilities, approaching human-level performance across hundreds of tasks.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.4154\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "2. Score: 0.3934\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "3. Score: 0.0408\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "4. Score: 0.0349\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "5. Score: 0.0299\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] Self-supervised learning revolutionized speech recognition by creating robust representations from unlabeled audio data that could be fine-tuned with minimal labeled data.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3924\n",
      "Preview: Wav2Vec: Unsupervised Pre-training for Speech Recognition presents a self-supervised approach to speech representation learning, which can be fine-tuned with limited labeled data to achieve strong per...\n",
      "\n",
      "2. Score: 0.1246\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "3. Score: 0.1097\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "4. Score: 0.0859\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "5. Score: 0.0629\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "6. Score: 0.0615\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "7. Score: 0.0486\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "8. Score: 0.0364\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The development of specialized language models trained on programming data demonstrated that AI could understand, generate, and translate between different programming languages.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3292\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "2. Score: 0.2631\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "3. Score: 0.1736\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "4. Score: 0.0454\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "5. Score: 0.0379\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "6. Score: 0.0354\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "7. Score: 0.0326\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A breakthrough in computational biology arrived with deep learning systems capable of predicting protein structures with atomic accuracy, even for previously unstudied proteins.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.8825\n",
      "Preview: AlphaFold 2: Highly accurate protein structure prediction with AlphaFold describes a deep learning system that can predict protein structures with atomic accuracy, even for proteins whose structures h...\n",
      "\n",
      "2. Score: 0.0496\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The development of foundation models ranging from 7B to 65B parameters showed that careful architecture design and training procedures could achieve state-of-the-art results with significantly reduced computational requirements.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.8047\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "2. Score: 0.0328\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "3. Score: 0.0290\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "4. Score: 0.0271\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "5. Score: 0.0253\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The introduction of attention mechanisms that completely replaced recurrent and convolutional operations transformed the field of natural language processing.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3095\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "2. Score: 0.2758\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "3. Score: 0.1498\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "4. Score: 0.1303\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "5. Score: 0.0373\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A new approach to object detection reframed the problem as direct regression of bounding boxes and class probabilities, enabling real-time performance.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.1316\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "2. Score: 0.1219\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "3. Score: 0.0998\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "4. Score: 0.0975\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "5. Score: 0.0876\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "6. Score: 0.0732\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "7. Score: 0.0703\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "8. Score: 0.0656\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "9. Score: 0.0645\n",
      "Preview: RoBERTa: A Robustly Optimized BERT Pretraining Approach presents key modifications to BERT training procedure, including training the model longer, with bigger batches, over more data; removing the ne...\n",
      "\n",
      "10. Score: 0.0592\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "11. Score: 0.0370\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] By training two models in opposition - one generating fake data and another detecting fakes - researchers created a framework for learning complex data distributions.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.2074\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "2. Score: 0.1658\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "3. Score: 0.1506\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "4. Score: 0.1235\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "5. Score: 0.0845\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "6. Score: 0.0581\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "7. Score: 0.0356\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "8. Score: 0.0332\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "9. Score: 0.0276\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "10. Score: 0.0245\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The discovery that randomly dropping out neurons during training could prevent neural networks from becoming overly dependent on specific features greatly improved generalization.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3240\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "2. Score: 0.2270\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "3. Score: 0.2033\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "4. Score: 0.0749\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "5. Score: 0.0503\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "6. Score: 0.0372\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A method for efficient stochastic optimization that adapts learning rates based on first and second moments of the gradients became the de facto standard for training neural networks.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.4357\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "2. Score: 0.2484\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "3. Score: 0.1748\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "4. Score: 0.0695\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The introduction of skip connections allowed for the successful training of extremely deep neural networks, revolutionizing computer vision architectures.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3721\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "2. Score: 0.2584\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "3. Score: 0.1014\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "4. Score: 0.0708\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "5. Score: 0.0508\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "6. Score: 0.0333\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "7. Score: 0.0228\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A technique for normalizing layer inputs in neural networks dramatically accelerated training by reducing internal covariate shift.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3021\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "2. Score: 0.2915\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "3. Score: 0.2724\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "4. Score: 0.0498\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The development of a pure reinforcement learning system that mastered Go without human knowledge demonstrated the potential of learning complex strategies from first principles.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.2279\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "2. Score: 0.2046\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "3. Score: 0.0976\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "4. Score: 0.0937\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "5. Score: 0.0864\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "6. Score: 0.0857\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "7. Score: 0.0723\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "8. Score: 0.0557\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A framework for examining the behavior of large language models as knowledge bases revealed their capacity to store and retrieve factual information.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.5792\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "2. Score: 0.1362\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "3. Score: 0.1268\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "4. Score: 0.0553\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "5. Score: 0.0371\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The creation of a system that could solve competitive programming problems at a human-competitive level marked a significant advance in automated software development.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.2365\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "2. Score: 0.1948\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "3. Score: 0.0990\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "4. Score: 0.0900\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "5. Score: 0.0830\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "6. Score: 0.0726\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "7. Score: 0.0511\n",
      "Preview: Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normal...\n",
      "\n",
      "8. Score: 0.0372\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "9. Score: 0.0308\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "10. Score: 0.0304\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extended database of target texts and example queries for citation matching\n",
    "\n",
    "def print_citation_results(results, max_preview_length=200):\n",
    "    \"\"\"\n",
    "    Print the citation retrieval results in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        results: List of results from retrieve_citations\n",
    "        max_preview_length: Maximum length of text preview to show\n",
    "    \"\"\"\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nCitation {i}: [[{result['citation_text']}]]\")\n",
    "        print(\"\\nTop matches:\")\n",
    "        total_score = 0\n",
    "        for j, match in enumerate(result['matches'], 1):\n",
    "            preview = match['text'][:max_preview_length]\n",
    "            if len(match['text']) > max_preview_length:\n",
    "                preview += \"...\"\n",
    "            print(f\"\\n{j}. Score: {match['score']:.4f}\")\n",
    "            print(f\"Preview: {preview}\")\n",
    "            total_score += match['score']\n",
    "            if total_score > .9:\n",
    "                break\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "target_texts = [\n",
    "    # Original papers from input (keeping the most significant ones)\n",
    "    \"\"\"Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The proposed model, called the Transformer, applies self-attention to compute representations of its input and output without using sequence-aligned recurrent neural networks (RNNs) or convolution. Experiments on translation tasks demonstrate superior quality while being more parallelizable and requiring significantly less time to train.\"\"\",\n",
    "    \n",
    "    \"\"\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model, to pre-train deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art performance on eleven natural language processing tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parameters, GPT-3 achieves strong performance on many NLP tasks and benchmarks without any fine-tuning, sometimes matching or exceeding state-of-the-art performance.\"\"\",\n",
    "\n",
    "    # Additional NLP/Transformer papers\n",
    "    \"\"\"RoBERTa: A Robustly Optimized BERT Pretraining Approach presents key modifications to BERT training procedure, including training the model longer, with bigger batches, over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data.\"\"\",\n",
    "    \n",
    "    \"\"\"PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model demonstrates breakthrough performance on hundreds of language tasks and exhibits reasoning capabilities that approach human-level performance.\"\"\",\n",
    "    \n",
    "    \"\"\"LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using significantly less training compute, demonstrating the effectiveness of focused architectural choices and training procedures.\"\"\",\n",
    "\n",
    "    # Vision Transformers and Multi-modal\n",
    "    \"\"\"ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data, Vision Transformers (ViT) attain excellent results compared to state-of-the-art convolutional networks.\"\"\",\n",
    "    \n",
    "    \"\"\"DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single stream of data. The model can create images of objects in novel combinations not explicitly present in the training data.\"\"\",\n",
    "    \n",
    "    \"\"\"Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffusion model and an autoencoder for high-resolution image synthesis, enabling both conditional and unconditional image generation.\"\"\",\n",
    "\n",
    "    # Reinforcement Learning and Games\n",
    "    \"\"\"MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range of challenging domains without requiring knowledge of their underlying dynamics.\"\"\",\n",
    "    \n",
    "    \"\"\"AlphaFold 2: Highly accurate protein structure prediction with AlphaFold describes a deep learning system that can predict protein structures with atomic accuracy, even for proteins whose structures had not been previously determined experimentally, representing a major advance in protein structure prediction.\"\"\",\n",
    "\n",
    "    # Foundational ML/DL Papers\n",
    "    \"\"\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as a regularizer. This significantly reduces the number of training steps required to train deep networks.\"\"\",\n",
    "\n",
    "    \"\"\"Layer Normalization:  the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.\"\"\",\n",
    "    \n",
    "    \"\"\"Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decomposing problems, generating multiple candidate responses, and selecting the best ones.\"\"\",\n",
    "    \n",
    "    \"\"\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves their performance on complex reasoning tasks.\"\"\",\n",
    "\n",
    "    # Additional ML Applications\n",
    "    \"\"\"Wav2Vec: Unsupervised Pre-training for Speech Recognition presents a self-supervised approach to speech representation learning, which can be fine-tuned with limited labeled data to achieve strong performance on speech recognition tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual classification benchmark by providing the names of the visual categories in natural language.\"\"\",\n",
    "    \n",
    "    \"\"\"Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, and complete partial code snippets with reasonable accuracy.\"\"\"\n",
    "]\n",
    "\n",
    "# Example queries with citation placeholders\n",
    "example_queries_2 = [\n",
    "    \"\"\"The [[Stable Diffusion]] model revolutionized image generation by making it computationally efficient and accessible to the masses.\"\"\",\n",
    "    \"\"\"[[Chain-of-Thought Prompting]] demonstrated a crucial technique for improving language model reasoning capabilities.\"\"\",\n",
    "    \"\"\"The introduction of [[ViT]] showed that transformers could be effectively applied to computer vision tasks.\"\"\",\n",
    "    \"\"\"[[RoBERTa]] significantly improved upon BERT's performance by modifying its training procedure.\"\"\",\n",
    "    \"\"\"[[CLIP]] demonstrated how natural language supervision could be used to create versatile visual models.\"\"\",\n",
    "    \"\"\"[[PaLM]] showed remarkable reasoning capabilities approaching human-level performance in various tasks.\"\"\",\n",
    "    \"\"\"[[Wav2Vec]] introduced innovative self-supervised learning techniques for speech recognition.\"\"\",\n",
    "    \"\"\"[[Codex]] demonstrated the potential of large language models for code generation and understanding.\"\"\",\n",
    "    \"\"\"[[AlphaFold 2]] revolutionized protein structure prediction with unprecedented accuracy.\"\"\",\n",
    "    \"\"\"[[LLaMA]] proved that efficient training procedures could match larger models' performance with fewer parameters.\"\"\",\n",
    "    \"\"\"[[BatchNorm]] is a technique for accelerating training of neural networks that aims to prevent covariate shifts. Newer variants of it, such as [[LayerNorm]] try to achieve the same thing but attempt to avoid batchwise dependencies  \"\"\",\n",
    "]\n",
    "\n",
    "# Example queries with citation placeholders - conceptual descriptions\n",
    "example_queries = [\n",
    "    \"\"\"[[REF]] A breakthrough in image synthesis came with a two-stage approach using a low-dimensional latent space combined with an autoencoder, making high-quality image generation computationally feasible on consumer hardware.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The discovery that asking large language models to break down their reasoning process into steps before providing a final answer dramatically improved their problem-solving abilities.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] By treating images as sequences of patches and applying transformer architectures directly to these sequences, researchers demonstrated that convolutional neural networks weren't necessary for computer vision.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Through careful optimization of training procedures, including longer training times, larger batches, and dynamic masking patterns, researchers significantly enhanced the performance of bidirectional transformer models.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A major advance in computer vision came through training models to match images with natural language descriptions, creating visual representations that could generalize to any visual classification task.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The 540-billion parameter model trained on the Pathways system marked a significant milestone in language model capabilities, approaching human-level performance across hundreds of tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Self-supervised learning revolutionized speech recognition by creating robust representations from unlabeled audio data that could be fine-tuned with minimal labeled data.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The development of specialized language models trained on programming data demonstrated that AI could understand, generate, and translate between different programming languages.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A breakthrough in computational biology arrived with deep learning systems capable of predicting protein structures with atomic accuracy, even for previously unstudied proteins.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The development of foundation models ranging from 7B to 65B parameters showed that careful architecture design and training procedures could achieve state-of-the-art results with significantly reduced computational requirements.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The introduction of attention mechanisms that completely replaced recurrent and convolutional operations transformed the field of natural language processing.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A new approach to object detection reframed the problem as direct regression of bounding boxes and class probabilities, enabling real-time performance.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] By training two models in opposition - one generating fake data and another detecting fakes - researchers created a framework for learning complex data distributions.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The discovery that randomly dropping out neurons during training could prevent neural networks from becoming overly dependent on specific features greatly improved generalization.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A method for efficient stochastic optimization that adapts learning rates based on first and second moments of the gradients became the de facto standard for training neural networks.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The introduction of skip connections allowed for the successful training of extremely deep neural networks, revolutionizing computer vision architectures.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A technique for normalizing layer inputs in neural networks dramatically accelerated training by reducing internal covariate shift.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The development of a pure reinforcement learning system that mastered Go without human knowledge demonstrated the potential of learning complex strategies from first principles.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A framework for examining the behavior of large language models as knowledge bases revealed their capacity to store and retrieve factual information.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The creation of a system that could solve competitive programming problems at a human-competitive level marked a significant advance in automated software development.\"\"\"\n",
    "]\n",
    "\n",
    "# The rest of the code (target_texts and functions) remains the same\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    for query in example_queries:\n",
    "    # query = example_queries[1]  # Use first example query\n",
    "        print(query)\n",
    "        results = retrieve_citations(\n",
    "            model=model,\n",
    "            query_text=query,\n",
    "            target_texts=target_texts,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            k=30\n",
    "        )\n",
    "        print_citation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3dded39-9a6f-42fa-8eb8-0d2781555a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [[Double Helix Structure]] paper fundamentally changed our understanding of genetics.\n",
      "\n",
      "Citation 1: [[Double Helix Structure]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.7859\n",
      "Preview: CRISPR Gene Editing: A Revolution in Biotechnology presents a groundbreaking technique for precise DNA modification, enabling targeted genetic changes with unprecedented accuracy and efficiency.\n",
      "\n",
      "2. Score: 0.0959\n",
      "Preview: Epigenetic Inheritance Mechanisms shows how environmental factors can influence gene expression across generations without changing DNA sequences, revolutionizing our understanding of heredity.\n",
      "\n",
      "3. Score: 0.0529\n",
      "Preview: The Double Helix Structure reveals that DNA consists of two helical chains coiled around the same axis, forming a double helix. Each chain contains nucleotide units, with phosphates and sugars forming...\n",
      "\n",
      "================================================================================\n",
      "[[Quantum Tunneling in Biological Systems]] revealed the quantum nature of life processes.\n",
      "\n",
      "Citation 1: [[Quantum Tunneling in Biological Systems]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.3838\n",
      "Preview: Quantum Computing Applications in Drug Discovery demonstrates how quantum algorithms can accelerate the identification and optimization of new pharmaceutical compounds.\n",
      "\n",
      "2. Score: 0.3611\n",
      "Preview: Quantum Tunneling in Biological Systems demonstrates how quantum mechanical tunneling enables crucial biological processes like photosynthesis and enzyme catalysis, challenging classical models of bio...\n",
      "\n",
      "3. Score: 0.1252\n",
      "Preview: Neuroplasticity: Rewiring the Brain demonstrates the brain's ability to form new neural connections throughout life, challenging previous beliefs about brain development and recovery.\n",
      "\n",
      "4. Score: 0.0340\n",
      "Preview: The Double Helix Structure reveals that DNA consists of two helical chains coiled around the same axis, forming a double helix. Each chain contains nucleotide units, with phosphates and sugars forming...\n",
      "\n",
      "================================================================================\n",
      "[[Cellular Autophagy Mechanisms]] explained how cells maintain their health.\n",
      "\n",
      "Citation 1: [[Cellular Autophagy Mechanisms]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.7130\n",
      "Preview: Cellular Autophagy Mechanisms describes the process by which cells break down and recycle their components, revealing its crucial role in maintaining cellular health and preventing diseases.\n",
      "\n",
      "2. Score: 0.1655\n",
      "Preview: Microbiome Impact on Human Health explores how gut bacteria influence everything from metabolism to immune response, revealing the complex relationship between microorganisms and human health.\n",
      "\n",
      "3. Score: 0.0421\n",
      "Preview: Epigenetic Inheritance Mechanisms shows how environmental factors can influence gene expression across generations without changing DNA sequences, revolutionizing our understanding of heredity.\n",
      "\n",
      "================================================================================\n",
      "[[CRISPR Gene Editing]] transformed our ability to modify genetic material.\n",
      "\n",
      "Citation 1: [[CRISPR Gene Editing]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.6324\n",
      "Preview: Epigenetic Inheritance Mechanisms shows how environmental factors can influence gene expression across generations without changing DNA sequences, revolutionizing our understanding of heredity.\n",
      "\n",
      "2. Score: 0.2763\n",
      "Preview: CRISPR Gene Editing: A Revolution in Biotechnology presents a groundbreaking technique for precise DNA modification, enabling targeted genetic changes with unprecedented accuracy and efficiency.\n",
      "\n",
      "================================================================================\n",
      "[[Microbiome Impact]] demonstrated the importance of gut bacteria.\n",
      "\n",
      "Citation 1: [[Microbiome Impact]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.9639\n",
      "Preview: Microbiome Impact on Human Health explores how gut bacteria influence everything from metabolism to immune response, revealing the complex relationship between microorganisms and human health.\n",
      "\n",
      "================================================================================\n",
      "[[Neuroplasticity]] changed our view of brain development.\n",
      "\n",
      "Citation 1: [[Neuroplasticity]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.9652\n",
      "Preview: Neuroplasticity: Rewiring the Brain demonstrates the brain's ability to form new neural connections throughout life, challenging previous beliefs about brain development and recovery.\n",
      "\n",
      "================================================================================\n",
      "[[Epigenetic Inheritance]] showed how environment affects heredity.\n",
      "\n",
      "Citation 1: [[Epigenetic Inheritance]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.6301\n",
      "Preview: Microbiome Impact on Human Health explores how gut bacteria influence everything from metabolism to immune response, revealing the complex relationship between microorganisms and human health.\n",
      "\n",
      "2. Score: 0.1933\n",
      "Preview: Epigenetic Inheritance Mechanisms shows how environmental factors can influence gene expression across generations without changing DNA sequences, revolutionizing our understanding of heredity.\n",
      "\n",
      "3. Score: 0.0679\n",
      "Preview: Climate Change Impact on Marine Ecosystems documents the effects of rising ocean temperatures and acidification on marine life, predicting cascading effects through food webs.\n",
      "\n",
      "4. Score: 0.0495\n",
      "Preview: Cellular Autophagy Mechanisms describes the process by which cells break down and recycle their components, revealing its crucial role in maintaining cellular health and preventing diseases.\n",
      "\n",
      "================================================================================\n",
      "[[Climate Change Impact]] documented oceanic ecosystem changes.\n",
      "\n",
      "Citation 1: [[Climate Change Impact]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.9934\n",
      "Preview: Climate Change Impact on Marine Ecosystems documents the effects of rising ocean temperatures and acidification on marine life, predicting cascading effects through food webs.\n",
      "\n",
      "================================================================================\n",
      "[[Sustainable Energy Storage]] presented new battery technologies.\n",
      "\n",
      "Citation 1: [[Sustainable Energy Storage]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.9992\n",
      "Preview: Sustainable Energy Storage Solutions presents breakthrough technologies in battery design, focusing on improved capacity and reduced environmental impact.\n",
      "\n",
      "================================================================================\n",
      "[[Quantum Computing Applications]] showed promise in pharmaceutical research.\n",
      "\n",
      "Citation 1: [[Quantum Computing Applications]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.6830\n",
      "Preview: Quantum Computing Applications in Drug Discovery demonstrates how quantum algorithms can accelerate the identification and optimization of new pharmaceutical compounds.\n",
      "\n",
      "2. Score: 0.1595\n",
      "Preview: Microbiome Impact on Human Health explores how gut bacteria influence everything from metabolism to immune response, revealing the complex relationship between microorganisms and human health.\n",
      "\n",
      "3. Score: 0.1262\n",
      "Preview: CRISPR Gene Editing: A Revolution in Biotechnology presents a groundbreaking technique for precise DNA modification, enabling targeted genetic changes with unprecedented accuracy and efficiency.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "target_texts = [\n",
    "    \"\"\"The Double Helix Structure reveals that DNA consists of two helical chains coiled around the same axis, forming a double helix. Each chain contains nucleotide units, with phosphates and sugars forming the backbone. The bases are on the inside of the helix, and the complementary pairs are joined by hydrogen bonds.\"\"\",\n",
    "    \n",
    "    \"\"\"Quantum Tunneling in Biological Systems demonstrates how quantum mechanical tunneling enables crucial biological processes like photosynthesis and enzyme catalysis, challenging classical models of biochemical reactions.\"\"\",\n",
    "    \n",
    "    \"\"\"Cellular Autophagy Mechanisms describes the process by which cells break down and recycle their components, revealing its crucial role in maintaining cellular health and preventing diseases.\"\"\",\n",
    "    \n",
    "    \"\"\"CRISPR Gene Editing: A Revolution in Biotechnology presents a groundbreaking technique for precise DNA modification, enabling targeted genetic changes with unprecedented accuracy and efficiency.\"\"\",\n",
    "    \n",
    "    \"\"\"Microbiome Impact on Human Health explores how gut bacteria influence everything from metabolism to immune response, revealing the complex relationship between microorganisms and human health.\"\"\",\n",
    "    \n",
    "    \"\"\"Neuroplasticity: Rewiring the Brain demonstrates the brain's ability to form new neural connections throughout life, challenging previous beliefs about brain development and recovery.\"\"\",\n",
    "    \n",
    "    \"\"\"Epigenetic Inheritance Mechanisms shows how environmental factors can influence gene expression across generations without changing DNA sequences, revolutionizing our understanding of heredity.\"\"\",\n",
    "    \n",
    "    \"\"\"Climate Change Impact on Marine Ecosystems documents the effects of rising ocean temperatures and acidification on marine life, predicting cascading effects through food webs.\"\"\",\n",
    "    \n",
    "    \"\"\"Sustainable Energy Storage Solutions presents breakthrough technologies in battery design, focusing on improved capacity and reduced environmental impact.\"\"\",\n",
    "    \n",
    "    \"\"\"Quantum Computing Applications in Drug Discovery demonstrates how quantum algorithms can accelerate the identification and optimization of new pharmaceutical compounds.\"\"\"\n",
    "]\n",
    "\n",
    "example_queries = [\n",
    "    \"\"\"[[REF]] The discovery of DNA's structure with its complementary base pairs revolutionized our understanding of genetic inheritance.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Research revealed that quantum effects play a significant role in biological processes at the molecular level.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Scientists discovered a cellular recycling system essential for maintaining cellular health.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A revolutionary method for precisely editing genes has transformed biotechnology and medicine.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Studies showed that intestinal bacteria significantly influence human health and disease.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Research demonstrated that the brain can create new neural pathways throughout life.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Evidence showed that environmental factors can affect gene expression across generations.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Studies documented how climate change affects ocean ecosystems through multiple mechanisms.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] New developments in energy storage technology promise to revolutionize renewable energy adoption.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Quantum computing showed potential to accelerate drug development processes.\"\"\"\n",
    "]\n",
    "\n",
    "example_queries_2 = [\n",
    "    \"\"\"The [[Double Helix Structure]] paper fundamentally changed our understanding of genetics.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Quantum Tunneling in Biological Systems]] revealed the quantum nature of life processes.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Cellular Autophagy Mechanisms]] explained how cells maintain their health.\"\"\",\n",
    "    \n",
    "    \"\"\"[[CRISPR Gene Editing]] transformed our ability to modify genetic material.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Microbiome Impact]] demonstrated the importance of gut bacteria.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Neuroplasticity]] changed our view of brain development.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Epigenetic Inheritance]] showed how environment affects heredity.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Climate Change Impact]] documented oceanic ecosystem changes.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Sustainable Energy Storage]] presented new battery technologies.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Quantum Computing Applications]] showed promise in pharmaceutical research.\"\"\"\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    for query in example_queries_2:\n",
    "    # query = example_queries[1]  # Use first example query\n",
    "        print(query)\n",
    "        results = retrieve_citations(\n",
    "            model=model,\n",
    "            query_text=query,\n",
    "            target_texts=target_texts,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            k=30\n",
    "        )\n",
    "        print_citation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc477886-38ef-45ce-86fc-ddc6a62c0095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
