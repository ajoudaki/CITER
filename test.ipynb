{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c927b80-0d8f-4a2c-81aa-4dab56aaa2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c4ca0f-3f5e-4254-8ad4-ddb87efe9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2204102/4288513006.py:231: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./experiments/best_citation_model_backup.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 128\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    collate_sample_size: int = 5000\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def retrieve_citations(\n",
    "    model,\n",
    "    query_text: str,\n",
    "    target_texts: list,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    k: int = 5,\n",
    "    device = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve top k similar articles for each citation in a query document.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CitationModel\n",
    "        query_text: Source text with citations marked with [[citation]]\n",
    "        target_texts: List of potential target articles\n",
    "        tokenizer: Tokenizer used by the model\n",
    "        config: Configuration object with model parameters\n",
    "        k: Number of top articles to retrieve per citation\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples containing (citation_span, list of top k articles with scores)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # First, process the query document similar to WikiProcessor._find_citations\n",
    "    citations = []\n",
    "    for match in re.finditer(r'\\[\\[(.*?)\\]\\]', query_text):\n",
    "        citations.append((match.start(), match.end(), match.group(1)))\n",
    "    \n",
    "    # Process query text similar to tokenize_sources\n",
    "    query_encoded = tokenizer.encode_plus(\n",
    "        query_text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Create offset to index mapping\n",
    "    offset_mapping = query_encoded[\"offset_mapping\"]\n",
    "    off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "    off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "    \n",
    "    # Create citation tokens array similar to tokenize_sources\n",
    "    input_ids = query_encoded[\"input_ids\"]\n",
    "    cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "    mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "    \n",
    "    # Fill in citations\n",
    "    citation_indices = []\n",
    "    for i, j, _ in citations:\n",
    "        s, e = off2i[i], off2i[j]\n",
    "        cite_tokens[s] = 1  # Using 1 as a placeholder\n",
    "        mask_tokens[s:e] = 1\n",
    "        citation_indices.append(s)\n",
    "    \n",
    "    # Prepare source similar to collate function\n",
    "    mask_tokens = np.where(np.isin(input_ids, tokenizer.convert_tokens_to_ids(['[',']'])), 1, mask_tokens)\n",
    "    mask_tokens[cite_tokens == 1] = 0\n",
    "    input_ids = np.array(input_ids)\n",
    "    input_ids[cite_tokens == 1] = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    source_ids = input_ids[mask_tokens == 0]\n",
    "    \n",
    "    # Pad or truncate source\n",
    "    if len(source_ids) > config.source_len:\n",
    "        source_ids = source_ids[:config.source_len]\n",
    "    else:\n",
    "        source_ids = np.pad(source_ids, \n",
    "                           (0, config.source_len - len(source_ids)),\n",
    "                           'constant', \n",
    "                           constant_values=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Process target texts\n",
    "    target_encoded = []\n",
    "    for target in target_texts:\n",
    "        tokens = tokenizer.encode_plus(\n",
    "            target,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        if len(tokens) >= config.target_len - 1:\n",
    "            tokens = tokens[:config.target_len-1]\n",
    "        tokens = np.append(tokens, tokenizer.convert_tokens_to_ids(config.ref_token))\n",
    "        \n",
    "        if len(tokens) < config.target_len:\n",
    "            tokens = np.pad(tokens,\n",
    "                          (0, config.target_len - len(tokens)),\n",
    "                          'constant',\n",
    "                          constant_values=tokenizer.pad_token_id)\n",
    "        \n",
    "        target_encoded.append(tokens)\n",
    "    \n",
    "    target_ids = torch.tensor(target_encoded, dtype=torch.long).to(device)\n",
    "    source_ids = torch.tensor(source_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = (source_ids != tokenizer.pad_token_id).to(device)\n",
    "    target_attention_mask = (target_ids != tokenizer.pad_token_id).to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get source embeddings\n",
    "        source_outputs = model.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get target embeddings\n",
    "        target_outputs = model.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Extract citation and reference embeddings\n",
    "        cite_mask = model.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        ref_mask = model.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) / model.config.temperature\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top k for each citation\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=min(k, len(target_texts)), dim=1)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for i, (_, _, citation_text) in enumerate(citations):\n",
    "        top_matches = []\n",
    "        for j, idx in enumerate(top_k_indices[i]):\n",
    "            top_matches.append({\n",
    "                'text': target_texts[idx],\n",
    "                'score': float(top_k_scores[i][j])\n",
    "            })\n",
    "        results.append({\n",
    "            'citation_text': citation_text,\n",
    "            'matches': top_matches\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_citation_results(results, max_preview_length=200):\n",
    "    \"\"\"\n",
    "    Print the citation retrieval results in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        results: List of results from retrieve_citations\n",
    "        max_preview_length: Maximum length of text preview to show\n",
    "    \"\"\"\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nCitation {i}: [[{result['citation_text']}]]\")\n",
    "        print(\"\\nTop matches:\")\n",
    "        for j, match in enumerate(result['matches'], 1):\n",
    "            preview = match['text'][:max_preview_length]\n",
    "            if len(match['text']) > max_preview_length:\n",
    "                preview += \"...\"\n",
    "            print(f\"\\n{j}. Score: {match['score']:.4f}\")\n",
    "            print(f\"Preview: {preview}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "# Create model config\n",
    "config = ExperimentConfig(collate_sample_size=50000,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "model_config = CitationConfig(\n",
    "    base_model_name=config.model_name,\n",
    "    vocab_size=len(tokenizer),\n",
    "    cite_token_id=tokenizer.convert_tokens_to_ids(config.cite_token),\n",
    "    ref_token_id=tokenizer.convert_tokens_to_ids(config.ref_token),\n",
    "    temperature=config.temperature,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = CitationModel(model_config)\n",
    "checkpoint = torch.load('./experiments/best_citation_model_backup.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8749856c-8a76-4271-ace9-87d91ddd2c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Citation 1: [[Attention is All You Need]]\n",
      "\n",
      "Top matches:\n",
      "\n",
      "1. Score: 0.2209\n",
      "Preview: ImageNet Classification with Deep Convolutional Neural Networks introduces AlexNet, a large, deep convolutional neural network that achieved record-breaking results in the ImageNet Large Scale Visual ...\n",
      "\n",
      "2. Score: 0.1451\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "3. Score: 0.1305\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "4. Score: 0.1077\n",
      "Preview: Generative Adversarial Nets presents an adversarial process for estimating generative models via a framework where two models are trained simultaneously: a generative model G that captures the data di...\n",
      "\n",
      "5. Score: 0.0816\n",
      "Preview: Dropout: A Simple Way to Prevent Neural Networks from Overfitting presents dropout, a technique where randomly selected neurons are ignored during training. This prevents units from co-adapting too mu...\n",
      "\n",
      "6. Score: 0.0570\n",
      "Preview: T5: Exploring the Limits of Transfer Learning presents a unified framework that converts all text-based language problems into a text-to-text format. Using this framework, we study different pre-train...\n",
      "\n",
      "7. Score: 0.0565\n",
      "Preview: Deep Residual Learning for Image Recognition introduces ResNet, which explicitly reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unrefere...\n",
      "\n",
      "8. Score: 0.0473\n",
      "Preview: XLNet: Generalized Autoregressive Pretraining for Language Understanding proposes a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expecte...\n",
      "\n",
      "9. Score: 0.0439\n",
      "Preview: Adam: A Method for Stochastic Optimization presents a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method computes individu...\n",
      "\n",
      "10. Score: 0.0419\n",
      "Preview: YOLO: Real-Time Object Detection explains the You Only Look Once (YOLO) system, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection, but YOLO...\n",
      "\n",
      "11. Score: 0.0325\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "12. Score: 0.0178\n",
      "Preview: Neural Machine Translation by Jointly Learning to Align and Translate introduces an attention mechanism that allows a model to automatically (soft-)search for parts of a source sentence that are relev...\n",
      "\n",
      "13. Score: 0.0167\n",
      "Preview: AlphaGo: Mastering the Game of Go with Deep Neural Networks and Tree Search combines Monte Carlo tree search with deep neural networks that have been trained by supervised learning, followed by reinfo...\n",
      "\n",
      "14. Score: 0.0006\n",
      "Preview: Bitcoin: A Peer-to-Peer Electronic Cash System presents the original design for Bitcoin, a purely peer-to-peer version of electronic cash that allows online payments to be sent directly from one party...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example texts with varying degrees of relevance\n",
    "\n",
    "target_texts = [\n",
    "    # Directly related (original paper and closest variants)\n",
    "    \"\"\"Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The proposed model, called the Transformer, applies self-attention to compute representations of its input and output without using sequence-aligned recurrent neural networks (RNNs) or convolution. Experiments on translation tasks demonstrate superior quality while being more parallelizable and requiring significantly less time to train.\"\"\",\n",
    "    \n",
    "    \"\"\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model, to pre-train deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art performance on eleven natural language processing tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parameters, GPT-3 achieves strong performance on many NLP tasks and benchmarks without any fine-tuning, sometimes matching or exceeding state-of-the-art performance.\"\"\",\n",
    "    \n",
    "    # Loosely related (discussing attention or transformers in different contexts)\n",
    "    \"\"\"Neural Machine Translation by Jointly Learning to Align and Translate introduces an attention mechanism that allows a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. This approach achieves significant improvements in translation performance.\"\"\",\n",
    "    \n",
    "    \"\"\"XLNet: Generalized Autoregressive Pretraining for Language Understanding proposes a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order. Additionally, XLNet integrates ideas from Transformer-XL into pretraining.\"\"\",\n",
    "    \n",
    "    \"\"\"T5: Exploring the Limits of Transfer Learning presents a unified framework that converts all text-based language problems into a text-to-text format. Using this framework, we study different pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.\"\"\",\n",
    "    \n",
    "    # Remotely related (general ML/DL papers)\n",
    "    \"\"\"Deep Residual Learning for Image Recognition introduces ResNet, which explicitly reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. These networks can be substantially deeper, leading to improved performance on visual recognition tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"Adam: A Method for Stochastic Optimization presents a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\"\"\",\n",
    "    \n",
    "    \"\"\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting presents dropout, a technique where randomly selected neurons are ignored during training. This prevents units from co-adapting too much by randomly dropping out a proportion of the hidden units on each presentation of each training case.\"\"\",\n",
    "    \n",
    "    # Not directly related (different domain or focus)\n",
    "    \"\"\"AlphaGo: Mastering the Game of Go with Deep Neural Networks and Tree Search combines Monte Carlo tree search with deep neural networks that have been trained by supervised learning, followed by reinforcement learning. This approach achieves a high winning rate against other Go programs and defeated a human professional player.\"\"\",\n",
    "    \n",
    "    \"\"\"Bitcoin: A Peer-to-Peer Electronic Cash System presents the original design for Bitcoin, a purely peer-to-peer version of electronic cash that allows online payments to be sent directly from one party to another without going through a financial institution.\"\"\",\n",
    "    \n",
    "    \"\"\"ImageNet Classification with Deep Convolutional Neural Networks introduces AlexNet, a large, deep convolutional neural network that achieved record-breaking results in the ImageNet Large Scale Visual Recognition Challenge. The network was trained on two GPUs and incorporated several novel features.\"\"\",\n",
    "    \n",
    "    \"\"\"YOLO: Real-Time Object Detection explains the You Only Look Once (YOLO) system, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection, but YOLO frames object detection as a regression problem to spatially separated bounding boxes and associated class probabilities.\"\"\",\n",
    "    \n",
    "    \"\"\"Generative Adversarial Nets presents an adversarial process for estimating generative models via a framework where two models are trained simultaneously: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.\"\"\"\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "query_text = \"The transformer architecture has revolutionized NLP by introducing a model based entirely on attention mechanisms and outperformed the existing benchmarks on a variety of tasks, this was studied in [[Attention is All You Need]],\"\n",
    "\n",
    "# Retrieve citations\n",
    "results = retrieve_citations(\n",
    "    model=model,\n",
    "    query_text=query_text,\n",
    "    target_texts=target_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_citation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dded39-9a6f-42fa-8eb8-0d2781555a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
