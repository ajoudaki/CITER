{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c927b80-0d8f-4a2c-81aa-4dab56aaa2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c4ca0f-3f5e-4254-8ad4-ddb87efe9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2226208/3407509931.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./experiments/best_citation_model_4.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig\n",
    ")\n",
    "\n",
    "# library here \n",
    "from utils import * \n",
    "\n",
    "def retrieve_citations(\n",
    "    model,\n",
    "    query_text: str,\n",
    "    target_texts: list,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    k: int = 5,\n",
    "    device = None\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # First, process the query document similar to WikiProcessor._find_citations\n",
    "    citations = []\n",
    "    for match in re.finditer(r'\\[\\[(.*?)\\]\\]', query_text):\n",
    "        citations.append((match.start(), match.end(), match.group(1)))\n",
    "    \n",
    "    # Process query text similar to tokenize_sources\n",
    "    query_encoded = tokenizer.encode_plus(\n",
    "        query_text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Create offset to index mapping\n",
    "    offset_mapping = query_encoded[\"offset_mapping\"]\n",
    "    off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "    off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "    \n",
    "    # Create citation tokens array similar to tokenize_sources\n",
    "    input_ids = query_encoded[\"input_ids\"]\n",
    "    cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "    mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "    \n",
    "    # Fill in citations\n",
    "    citation_indices = []\n",
    "    for i, j, _ in citations:\n",
    "        s, e = off2i[i], off2i[j]\n",
    "        cite_tokens[s] = 1  # Using 1 as a placeholder\n",
    "        mask_tokens[s:e] = 1\n",
    "        citation_indices.append(s)\n",
    "    \n",
    "    # Prepare source similar to collate function\n",
    "    mask_tokens = np.where(np.isin(input_ids, tokenizer.convert_tokens_to_ids(['[',']'])), 1, mask_tokens)\n",
    "    mask_tokens[cite_tokens == 1] = 0\n",
    "    input_ids = np.array(input_ids)\n",
    "    input_ids[cite_tokens == 1] = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    source_ids = input_ids[mask_tokens == 0]\n",
    "    \n",
    "    # Pad or truncate source\n",
    "    if len(source_ids) > config.source_len:\n",
    "        source_ids = source_ids[:config.source_len]\n",
    "    else:\n",
    "        source_ids = np.pad(source_ids, \n",
    "                           (0, config.source_len - len(source_ids)),\n",
    "                           'constant', \n",
    "                           constant_values=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Process target texts\n",
    "    target_encoded = []\n",
    "    for target in target_texts:\n",
    "        tokens = tokenizer.encode_plus(\n",
    "            target,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        if len(tokens) >= config.target_len - 1:\n",
    "            tokens = tokens[:config.target_len-1]\n",
    "        tokens = np.append(tokens, tokenizer.convert_tokens_to_ids(config.ref_token))\n",
    "        \n",
    "        if len(tokens) < config.target_len:\n",
    "            tokens = np.pad(tokens,\n",
    "                          (0, config.target_len - len(tokens)),\n",
    "                          'constant',\n",
    "                          constant_values=tokenizer.pad_token_id)\n",
    "        \n",
    "        target_encoded.append(tokens)\n",
    "    \n",
    "    target_ids = torch.tensor(target_encoded, dtype=torch.long).to(device)\n",
    "    source_ids = torch.tensor(source_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = (source_ids != tokenizer.pad_token_id).to(device)\n",
    "    target_attention_mask = (target_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "    logit_scale = torch.clamp(model.logit_scale, 0, torch.log(torch.tensor(100.0)))\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get source embeddings\n",
    "        source_outputs = model.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get target embeddings\n",
    "        target_outputs = model.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Extract citation and reference embeddings\n",
    "        cite_mask = model.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        ref_mask = model.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) * logit_scale\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top k for each citation\n",
    "        top_k_scores, top_k_indices = torch.topk(scores, k=min(k, len(target_texts)), dim=1)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for i, (_, _, citation_text) in enumerate(citations):\n",
    "        top_matches = []\n",
    "        for j, idx in enumerate(top_k_indices[i]):\n",
    "            top_matches.append({\n",
    "                'text': target_texts[idx],\n",
    "                'score': float(top_k_scores[i][j])\n",
    "            })\n",
    "        results.append({\n",
    "            'citation_text': citation_text,\n",
    "            'matches': top_matches\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "# Create model config\n",
    "config = ExperimentConfig(collate_sample_size=50000,)\n",
    "config.device = 'cuda:1'\n",
    "\n",
    "experiment = Experiment(config)\n",
    "\n",
    "tokenizer = experiment.get_tokenizer()\n",
    "\n",
    "model = experiment.get_model()\n",
    "# Initialize model\n",
    "checkpoint = torch.load('./experiments/best_citation_model_4.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f70aacc-83c4-4fb0-a0df-b45e3c478330",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation_metrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_metrics'"
     ]
    }
   ],
   "source": [
    "checkpoint['validation_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8749856c-8a76-4271-ace9-87d91ddd2c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[REF]] A breakthrough in image synthesis came with a two-stage diffusion approach using a low-dimensional latent space combined with an autoencoder, making high-quality image generation computationally feasible on consumer hardware.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The discovery that asking large language models to break down their reasoning process into steps before providing a final answer dramatically improved their problem-solving abilities.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] By treating images as sequences of patches and applying transformer architectures directly to these sequences, researchers demonstrated that convolutional neural networks weren't necessary for computer vision.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] Through careful optimization of training procedures, including longer training times, larger batches, and dynamic masking patterns, researchers significantly enhanced the performance of bidirectional transformer models.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A major advance in computer vision came through training models to match images with natural language descriptions, creating visual representations that could generalize to any visual classification task.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "Preview: DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single str...\n",
      "\n",
      "Preview: ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classificati...\n",
      "\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The 540-billion parameter model trained on the Pathways system marked a significant milestone in language model capabilities, approaching human-level performance across hundreds of tasks.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] Self-supervised learning revolutionized speech recognition by creating robust representations from unlabeled audio data that could be fine-tuned with minimal labeled data.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Wav2Vec: Unsupervised Pre-training for Speech Recognition presents a self-supervised approach to speech representation learning, which can be fine-tuned with limited labeled data to achieve strong per...\n",
      "\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The development of specialized language models trained on programming data demonstrated that AI could understand, generate, and translate between different programming languages.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A breakthrough in computational biology arrived with deep learning systems capable of predicting protein structures with atomic accuracy, even for previously unstudied proteins.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: AlphaFold 2: Highly accurate protein structure prediction with AlphaFold describes a deep learning system that can predict protein structures with atomic accuracy, even for proteins whose structures h...\n",
      "\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffu...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The development of foundation models ranging from 7B to 65B parameters showed that careful architecture design and training procedures could achieve state-of-the-art results with significantly reduced computational requirements.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The introduction of attention mechanisms that completely replaced recurrent and convolutional operations transformed the field of natural language processing.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A new approach to object detection reframed the problem as direct regression of bounding boxes and class probabilities, enabling real-time performance.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] By training two models in opposition - one generating fake data and another detecting fakes - researchers created a framework for learning complex data distributions.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The discovery that randomly dropping out neurons during training could prevent neural networks from becoming overly dependent on specific features greatly improved generalization.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A method for efficient stochastic optimization that adapts learning rates based on first and second moments of the gradients became the de facto standard for training neural networks.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The introduction of skip connections allowed for the successful training of extremely deep neural networks, revolutionizing computer vision architectures.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A technique for normalizing layer inputs in neural networks dramatically accelerated training by reducing internal covariate shift.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The development of a pure reinforcement learning system that mastered Go without human knowledge demonstrated the potential of learning complex strategies from first principles.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The...\n",
      "\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] A framework for examining the behavior of large language models as knowledge bases revealed their capacity to store and retrieve factual information.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decompos...\n",
      "\n",
      "Preview: CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual...\n",
      "\n",
      "Preview: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves t...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "[[REF]] The creation of a system that could solve competitive programming problems at a human-competitive level marked a significant advance in automated software development.\n",
      "\n",
      "Citation 1: [[REF]]\n",
      "\n",
      "Top matches:\n",
      "Preview: Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, a...\n",
      "\n",
      "Preview: MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range...\n",
      "\n",
      "Preview: PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model ...\n",
      "\n",
      "Preview: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as ...\n",
      "\n",
      "Preview: GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parame...\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extended database of target texts and example queries for citation matching\n",
    "\n",
    "def print_citation_results(results, max_preview_length=200):\n",
    "    \"\"\"\n",
    "    Print the citation retrieval results in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        results: List of results from retrieve_citations\n",
    "        max_preview_length: Maximum length of text preview to show\n",
    "    \"\"\"\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nCitation {i}: [[{result['citation_text']}]]\")\n",
    "        print(\"\\nTop matches:\")\n",
    "        for j, match in enumerate(result['matches'], 1):\n",
    "            preview = match['text'][:max_preview_length]\n",
    "            if len(match['text']) > max_preview_length:\n",
    "                preview += \"...\\n\"\n",
    "            # print(f\"\\n{j}. Score: {match['score']:.4f}\")\n",
    "            print(f\"Preview: {preview}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "target_texts = [\n",
    "    # Original papers from input (keeping the most significant ones)\n",
    "    \"\"\"Attention Is All You Need introduces the transformer architecture, a novel sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely. The proposed model, called the Transformer, applies self-attention to compute representations of its input and output without using sequence-aligned recurrent neural networks (RNNs) or convolution. Experiments on translation tasks demonstrate superior quality while being more parallelizable and requiring significantly less time to train.\"\"\",\n",
    "    \n",
    "    \"\"\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding presents a new language representation model that uses bidirectional training of Transformer, a popular attention model, to pre-train deep bidirectional representations from unlabeled text. BERT achieves state-of-the-art performance on eleven natural language processing tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"GPT-3: Language Models are Few-Shot Learners demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. Using a transformer architecture with 175 billion parameters, GPT-3 achieves strong performance on many NLP tasks and benchmarks without any fine-tuning, sometimes matching or exceeding state-of-the-art performance.\"\"\",\n",
    "\n",
    "    # Additional NLP/Transformer papers\n",
    "    \"\"\"RoBERTa: A Robustly Optimized BERT Pretraining Approach presents key modifications to BERT training procedure, including training the model longer, with bigger batches, over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data.\"\"\",\n",
    "    \n",
    "    \"\"\"PaLM: Scaling Language Modeling with Pathways introduces a 540-billion parameter language model trained using the Pathways system, which enables efficient training across multiple TPU pods. The model demonstrates breakthrough performance on hundreds of language tasks and exhibits reasoning capabilities that approach human-level performance.\"\"\",\n",
    "    \n",
    "    \"\"\"LLaMA: Open and Efficient Foundation Language Models presents a collection of foundation language models ranging from 7B to 65B parameters. These models outperform larger models like GPT-3 while using significantly less training compute, demonstrating the effectiveness of focused architectural choices and training procedures.\"\"\",\n",
    "\n",
    "    # Vision Transformers and Multi-modal\n",
    "    \"\"\"ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale shows that a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data, Vision Transformers (ViT) attain excellent results compared to state-of-the-art convolutional networks.\"\"\",\n",
    "    \n",
    "    \"\"\"DALL·E: Creating Images from Text demonstrates the capability to generate images from text descriptions, leveraging a transformer that autoregressively models the text and image tokens as a single stream of data. The model can create images of objects in novel combinations not explicitly present in the training data.\"\"\",\n",
    "    \n",
    "    \"\"\"Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models presents a computationally efficient approach to image generation using a two-stage model: a low-dimensional latent diffusion model and an autoencoder for high-resolution image synthesis, enabling both conditional and unconditional image generation.\"\"\",\n",
    "\n",
    "    # Reinforcement Learning and Games\n",
    "    \"\"\"MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model presents a new algorithm that combines tree-based search with a learned model to achieve superhuman performance in a range of challenging domains without requiring knowledge of their underlying dynamics.\"\"\",\n",
    "    \n",
    "    \"\"\"AlphaFold 2: Highly accurate protein structure prediction with AlphaFold describes a deep learning system that can predict protein structures with atomic accuracy, even for proteins whose structures had not been previously determined experimentally, representing a major advance in protein structure prediction.\"\"\",\n",
    "\n",
    "    # Foundational ML/DL Papers\n",
    "    \"\"\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift introduces a technique to normalize each layer's inputs, allowing much higher learning rates and acting as a regularizer. This significantly reduces the number of training steps required to train deep networks.\"\"\",\n",
    "    \n",
    "    \"\"\"Large Language Models Can Self-Improve explores how language models can improve their own responses through self-reflection and iteration, showing that models can generate better responses by decomposing problems, generating multiple candidate responses, and selecting the best ones.\"\"\",\n",
    "    \n",
    "    \"\"\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models demonstrates that prompting language models to generate step-by-step reasoning before producing an answer significantly improves their performance on complex reasoning tasks.\"\"\",\n",
    "\n",
    "    # Additional ML Applications\n",
    "    \"\"\"Wav2Vec: Unsupervised Pre-training for Speech Recognition presents a self-supervised approach to speech representation learning, which can be fine-tuned with limited labeled data to achieve strong performance on speech recognition tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"CLIP: Learning Transferable Visual Models From Natural Language Supervision demonstrates efficient learning of visual concepts from natural language supervision. The model can be applied to any visual classification benchmark by providing the names of the visual categories in natural language.\"\"\",\n",
    "    \n",
    "    \"\"\"Codex: Evaluating Large Language Models Trained on Code explores the capabilities of language models trained on code, showing they can translate natural language to code, explain complex algorithms, and complete partial code snippets with reasonable accuracy.\"\"\"\n",
    "]\n",
    "\n",
    "# Example queries with citation placeholders\n",
    "example_queries_2 = [\n",
    "    \"\"\"The [[Stable Diffusion]] model revolutionized image generation by making it computationally efficient and accessible to the masses.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Chain-of-Thought Prompting]] demonstrated a crucial technique for improving language model reasoning capabilities.\"\"\",\n",
    "    \n",
    "    \"\"\"The introduction of [[ViT]] showed that transformers could be effectively applied to computer vision tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"[[RoBERTa]] significantly improved upon BERT's performance by modifying its training procedure.\"\"\",\n",
    "    \n",
    "    \"\"\"[[CLIP]] demonstrated how natural language supervision could be used to create versatile visual models.\"\"\",\n",
    "    \n",
    "    \"\"\"[[PaLM]] showed remarkable reasoning capabilities approaching human-level performance in various tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Wav2Vec]] introduced innovative self-supervised learning techniques for speech recognition.\"\"\",\n",
    "    \n",
    "    \"\"\"[[Codex]] demonstrated the potential of large language models for code generation and understanding.\"\"\",\n",
    "    \n",
    "    \"\"\"[[AlphaFold 2]] revolutionized protein structure prediction with unprecedented accuracy.\"\"\",\n",
    "    \n",
    "    \"\"\"[[LLaMA]] proved that efficient training procedures could match larger models' performance with fewer parameters.\"\"\"\n",
    "]\n",
    "\n",
    "# Example queries with citation placeholders - conceptual descriptions\n",
    "example_queries = [\n",
    "    \"\"\"[[REF]] A breakthrough in image synthesis came with a two-stage diffusion approach using a low-dimensional latent space combined with an autoencoder, making high-quality image generation computationally feasible on consumer hardware.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The discovery that asking large language models to break down their reasoning process into steps before providing a final answer dramatically improved their problem-solving abilities.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] By treating images as sequences of patches and applying transformer architectures directly to these sequences, researchers demonstrated that convolutional neural networks weren't necessary for computer vision.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Through careful optimization of training procedures, including longer training times, larger batches, and dynamic masking patterns, researchers significantly enhanced the performance of bidirectional transformer models.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A major advance in computer vision came through training models to match images with natural language descriptions, creating visual representations that could generalize to any visual classification task.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The 540-billion parameter model trained on the Pathways system marked a significant milestone in language model capabilities, approaching human-level performance across hundreds of tasks.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] Self-supervised learning revolutionized speech recognition by creating robust representations from unlabeled audio data that could be fine-tuned with minimal labeled data.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The development of specialized language models trained on programming data demonstrated that AI could understand, generate, and translate between different programming languages.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A breakthrough in computational biology arrived with deep learning systems capable of predicting protein structures with atomic accuracy, even for previously unstudied proteins.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The development of foundation models ranging from 7B to 65B parameters showed that careful architecture design and training procedures could achieve state-of-the-art results with significantly reduced computational requirements.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The introduction of attention mechanisms that completely replaced recurrent and convolutional operations transformed the field of natural language processing.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A new approach to object detection reframed the problem as direct regression of bounding boxes and class probabilities, enabling real-time performance.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] By training two models in opposition - one generating fake data and another detecting fakes - researchers created a framework for learning complex data distributions.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The discovery that randomly dropping out neurons during training could prevent neural networks from becoming overly dependent on specific features greatly improved generalization.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A method for efficient stochastic optimization that adapts learning rates based on first and second moments of the gradients became the de facto standard for training neural networks.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The introduction of skip connections allowed for the successful training of extremely deep neural networks, revolutionizing computer vision architectures.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A technique for normalizing layer inputs in neural networks dramatically accelerated training by reducing internal covariate shift.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The development of a pure reinforcement learning system that mastered Go without human knowledge demonstrated the potential of learning complex strategies from first principles.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] A framework for examining the behavior of large language models as knowledge bases revealed their capacity to store and retrieve factual information.\"\"\",\n",
    "    \n",
    "    \"\"\"[[REF]] The creation of a system that could solve competitive programming problems at a human-competitive level marked a significant advance in automated software development.\"\"\"\n",
    "]\n",
    "\n",
    "# The rest of the code (target_texts and functions) remains the same\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    for query in example_queries:\n",
    "    # query = example_queries[1]  # Use first example query\n",
    "        print(query)\n",
    "        results = retrieve_citations(\n",
    "            model=model,\n",
    "            query_text=query,\n",
    "            target_texts=target_texts,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            k=5\n",
    "        )\n",
    "        print_citation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dded39-9a6f-42fa-8eb8-0d2781555a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc477886-38ef-45ce-86fc-ddc6a62c0095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
