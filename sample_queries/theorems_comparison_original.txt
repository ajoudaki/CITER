Problem (1) can be solved with $\mathcal{L}(\gamma^{(1)},\gamma^{(2)})=0$ if the matrices $W^{(1)}$ and W(2) have full rank and $c_{1}\ge c_{2}c_{0}.$ The parameters $\gamma_{i}^{(2)}=1$ and $\gamma^{(1)}=M^{+}v$ define a solution, where $M^{+}$ denotes the Moore-Penrose inverse and $v_{(ij)}=w_{(ij)}^{(t)}$ is a flattened vector representation of the target. If we utilize $\gamma^{(2)}$ and $\gamma^{(1)}$ solves $M\gamma^{(1)}=0$ with $m_{(ij)k}=w_{i^{\prime}k}^{(2)}w_{kj}^{(1)}-\frac{w_{ij}^{(t)}}{w_{ij_{i}}^{(t)}}w_{i^{\prime}k}^{(2)}w_{kj_{i}}^{(1)}$ , where i corresponds to a pivotal element of the target row, then $c_{1}\ge c_{2}(c_{0}-1)+1$ features suffice.
The singular values of the matrix $M=(m_{(ij)k})$ with $m_{(ij)k}=w_{ik}^{(2)}w_{kj}^{(1)}$ where the factors $w_{ik}^{(2)},w_{kj}^{(1)}$ are independent, have uniformly bounded fourth moments, and the variance $Var(m_{(ij)k})=1/(c_{0}c_{2}).$ are asymptotically (for $c_{0}c_{2}\rightarrow\infty)$ distributed with probability density $p(x)=\frac{1}{\lambda\pi x}\sqrt{(x^{2}-\lambda_{-}^{2})(\lambda_{+}^{2}-x^{2})},$ where $\lambda=max((c_{2}c_{0})/c_{1},c_{1}/(c_{2}c_{0}))$, $\lambda_{-}=1-\sqrt{\lambda},$ and $\lambda_{+}^{2}=1+\sqrt{\lambda}and~x\in[\lambda_{-},\lambda_{+}]$.
Learning with ID cond features (2) is equivalent to learning a neural network in its original parameterization with $w_{ij}^{(t)}=\gamma_{k_{ij}}^{(1)}$ . The corresponding feature matrix $M=I$ is maximally sparse and perfectly conditioned.
Assume that M has rank r and singular value decomposition $M=USV^{T}$ with singular values sorted in decreasing order. Let P be a permutation of the rows of the target $W^{(t)}$ and $v^{(t)}(P)=vec(PW^{(t)})$ be the flattened vector representation of the permuted target. Then, the approximation error is minimized by minp min, $||M\hat{\gamma}-v^{(t)}(P)||^{2}=$ minp $||P_{r}U^{T}v^{(t)}(P)||^{2},$ where $P_{r}=[\begin{matrix}0&0\\ 0&I_{r}\end{matrix}]$ projects a vector to its last $c_0 c_2 - r$ components.
Let the target matrix $W^{(t)}$ be a sparse matrix with in-degrees $d_{i}=\sum_{j}\tilde{\delta}_{w_{ij}^{(t)}\ne0}$ and the feature matrix M correspond to a random $Ber(p)$ matrix (i.e. sparse Id cond features). Then, the probability that the (permuted) target can be accurately matched is upper bounded by $\mathbb{P}(W^{(t)}\subset M)\le\Pi_{i}(1-(1-p^{d_{i}})^{c_{2}})$.
A random target with iid $Ber(q)$ entries and $c_{2}^{(t)}$ output neurons can be perfectly matched with probability 1-δ with random ID cond features with expected density p if $c_{2}=kc_{2}^{(t)}$ with $k\ge\frac{log(1-(1-\delta)^{1/\epsilon_{2}^{(t)}})}{log(1-(1-q(1-p))^{c_{0}}}$.
Let $X=X^{(p)}$, $p=\sum_{k}d_{k}$ be a sequence of $p\times m$ random matrices, whose columns are independent and follow the block-independent model with blocks of sizes $d_{k}=d_{k}(p)$, the aspect ratio $p/m$ converging to a number $\lambda\in(0,\infty)$ and maxk $d_{k}=o(p)$ as $p\rightarrow\infty$. Assume that all entries of the random matrix X have uniformly bounded fourth moments. Then with probability 1 the empirical spectral distribution of the sample covariance matrix $W=\frac{1}{m}XX^{T}$ converges weakly in distribution to the Marchenko-Pastur distribution with parameter λ.
