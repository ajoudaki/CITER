[Assumption 1 Exponential $\beta$-mixing from architecture aware] The training sequence $\{Z_{t}\}_{t\ge1}$ is strictly stationary and satisfies $\beta(k)\le C_{０}e^{-c_{０}k}$ for some constants $C_{０},c_{０}>0$ and all $k\ge1$.
[Lemma 1 Blocking Lemma from architecture aware] Under Assumption 1, the first elements of each block $\{Z_{I_{j}}^{(1)}\}_{j=1}^{B}$ (where block j contains indices $I_{j}=\{(j-1)(d+1)+1,...,j(d+1)\}$ and $B=\lfloor N/(d+1)\rfloor$) are nearly independent in the sense that $||P_{Z_{I_{１}}^{(1)}},...,Z_{I_{B}}^{(1)}-P_{Z_{I_{１}}^{(1)}}\otimes...\otimes P_{Z_{I_{B}}^{(1)}}||_{TV}\le B~\beta(d)$, where $d$ is the separation between the first elements.
[Proposition 1 Delayed-Feedback Generalization from architecture aware] Under Assumption 1 (Exponential $\beta$-mixing), for any $\delta\in(0,1)$, the average predictor $\overline{f}=\frac{1}{N}\sum_{t=1}^{N}h_{t}$ produced by a delayed-feedback online learning algorithm with delay $d$ satisfies: $|\mathcal{L}(\overline{f})-\hat{\mathcal{L}}_{N}(\overline{f})|\le\frac{R_{N}}{N}+N\beta(d)+\sqrt{\frac{log(1/\delta)}{N}},$ with probability $1-\delta$, where $R_N$ is the online regret over $N$ steps.
[Lemma 2 TCN Rademacher Complexity from architecture aware] For a Temporal Convolutional Network (TCN) hypothesis class $\mathcal{F}_{D,p,R}$ with depth D, kernel size p, and $l_{２,１}$ weight norm bound R per layer, the Rademacher complexity for any i.i.d. sample of size m is bounded by $\mathfrak{R}_{m}(\mathcal{F}_{D,p,R}) \le 4R\sqrt{\frac{Dpn \log(2m)}{m}}$, where $n$ is the input dimension.
[Theorem 1 Architecture-Aware Generalization from architecture aware] Under Assumption 1 (Exponential $\beta$-mixing), for any $\delta\in(0,1)$, every predictor $f\in\mathcal{F}_{D,p,R}$ (TCN with depth D, kernel size p, input dimension n, and $l_{２,１}$ norm R) produced by the delayed-feedback learner with optimal delay $d=\lceil \log N/c_{０}\rceil$ satisfies: $|\mathcal{L}(f)-\hat{\mathcal{L}}_{N}(f)|\le C_{１}R\sqrt{\frac{D~p~n~log~N}{N}}+C_{０}+\sqrt{\frac{log(1/\delta)}{N}},$ with probability $1-\delta$, where $C_1$ is a universal constant related to the online learning algorithm and $C_0, c_0$ are from the mixing assumption.
[Frobenius Norm Rademacher Bound from Golowich et al.] For networks with d layers, where each layer j has a parameter matrix with Frobenius norm at most $M_{F}(j)$, and m i.i.d. training examples, one can prove a generalization bound of $\mathcal{O}(\sqrt{d}(\prod_{j=1}^{d}M_{F}(j))/\sqrt{m})$ using Rademacher complexity analysis.
