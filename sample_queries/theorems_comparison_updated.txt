Consider approximating a target linear layer defined by weights $W^{(t)} \in \mathbb{R}^{c_{2}\times c_{0}}$ using two source linear layers with frozen random weights $W^{(1)} \in \mathbb{R}^{c_{1}\times c_{0}}$ and $W^{(2)} \in \mathbb{R}^{c_{2}\times c_{1}}$. Let the output of the first source layer be element-wise multiplied by a trainable vector $\gamma^{(1)} \in \mathbb{R}^{c_{1}}$ and the output of the second source layer be element-wise multiplied by a trainable vector $\gamma^{(2)} \in \mathbb{R}^{c_{2}}$. The target layer $W^{(t)}$ can be exactly represented, meaning $\sum_{k} (W^{(2)}_{ik} \gamma^{(1)}_{k} W^{(1)}_{kj}) \gamma^{(2)}_{i} = W^{(t)}_{ij}$ for all $i, j$, if $W^{(1)}$ and $W^{(2)}$ have full rank and the intermediate dimension $c_{1}$ satisfies $c_{1}\ge c_{2}c_{0}$. One solution uses $\gamma^{(2)}_{i}=1$ for all $i$ and $\gamma^{(1)} = M^{+}v$, where $M$ is a $(c_{0}c_{2}) \times c_{1}$ matrix with entries $m_{(ij)k} = W^{(2)}_{ik}W^{(1)}_{kj}$, $v$ is the flattened vector of $W^{(t)}$, and $M^{+}$ is the Moore-Penrose inverse. Alternatively, if both $\gamma^{(1)}$ and $\gamma^{(2)}$ are utilized, the target layer can be represented if $c_{1}\ge c_{2}(c_{0}-1)+1$ features suffice, provided a related linear system involving $W^{(t)}$, $W^{(1)}$, $W^{(2)}$, and $\gamma^{(1)}$ has a non-trivial solution.
Consider a $(c_{0}c_{2}) \times c_{1}$ matrix $M$ where each entry $m_{(ij)k}$ is formed by the product of independent random variables $w_{ik}^{(2)}$ and $w_{kj}^{(1)}$, such that $m_{(ij)k}=w_{ik}^{(2)}w_{kj}^{(1)}$. Assume these factors $w^{(2)}$ and $w^{(1)}$ are independent, have zero mean, uniformly bounded fourth moments, and variances such that the variance of the product $m_{(ij)k}$ is $Var(m_{(ij)k})=1/(c_{0}c_{2})$. As the dimensions grow such that $c_{0}c_{2}\rightarrow\infty$ and the ratio $\lambda = \max((c_{2}c_{0})/c_{1}, c_{1}/(c_{2}c_{0}))$ is fixed, the distribution of the singular values $x$ of $M$ asymptotically follows the probability density function $p(x)=\frac{1}{\lambda\pi x}\sqrt{(x^{2}-\lambda_{-}^{2})(\lambda_{+}^{2}-x^{2})}$ for $x$ in the interval $[\lambda_{-},\lambda_{+}]$, where $\lambda_{-}=1-\sqrt{\lambda}$ and $\lambda_{+}=1+\sqrt{\lambda}$.
Consider a specific construction of the source weights $W^{(1)}$ and $W^{(2)}$ (called "ID cond features") such that the resulting feature matrix $M$ (where $m_{(ij)k} = W^{(2)}_{ik} W^{(1)}_{kj}$) is the identity matrix $I$. Learning only the trainable normalization parameters $\gamma^{(1)}$ associated with this $M=I$ structure is equivalent to learning the weights $W^{(t)}$ of a standard neural network layer directly, where $w_{ij}^{(t)}$ corresponds to $\gamma_{k_{ij}}^{(1)}$ (the element of $\gamma^{(1)}$ associated with the $(i,j)$ output index). This feature matrix $M=I$ is maximally sparse (contains the minimum number of non-zero elements required) and perfectly conditioned (condition number is 1).
Let $M$ be a $(c_{0}c_{2}) \times c_{1}$ feature matrix with rank $r$ and singular value decomposition $M=USV^{T}$, where singular values in $S$ are sorted in decreasing order. Let $W^{(t)} \in \mathbb{R}^{c_{2} \times c_{0}}$ be the target weight matrix. Let $P$ be a permutation matrix acting on the rows of $W^{(t)}$, and let $v^{(t)}(P)=vec(PW^{(t)})$ be the flattened vector representation of the permuted target. The minimum approximation error achievable by finding the best parameters $\hat{\gamma}$ for the linear system $M\gamma \approx v^{(t)}(P)$, optimized over all possible permutations $P$, is given by $\min_{P} \min_{\gamma} ||M\gamma-v^{(t)}(P)||^{2} = \min_{P} ||P_{r}U^{T}v^{(t)}(P)||^{2}$. Here, $P_{r}$ is a projection matrix $P_{r}=[\begin{smallmatrix}0&0\\ 0&I_{c_0 c_2 - r}\end{smallmatrix}]$ that projects onto the last $c_0 c_2 - r$ components, corresponding to the null space of $M^T$.
Let the target weight matrix $W^{(t)} \in \mathbb{R}^{c_{2} \times c_{0}}$ be sparse, with $d_i$ non-zero entries in the $i$-th row ($d_{i}=\sum_{j}\delta_{w_{ij}^{(t)}\ne0}$). Let the feature matrix $M$ correspond to sparse "ID cond features", meaning $M$ is effectively a random $c_2 c_0 \times c_1$ matrix where entries are independently sampled from a Bernoulli distribution with parameter $p$ ($Ber(p)$), representing the probability of a feature existing. The probability that there exists a permutation $P$ of the target rows such that all non-zero entries required by $PW^{(t)}$ exist in the feature matrix $M$ (denoted $W^{(t)} \subset M$ after permutation) is upper bounded by $\mathbb{P}(W^{(t)}\subset M)\le\Pi_{i=1}^{c_2}(1-(1-p^{d_{i}})^{c_{2}})$.
Consider a random target weight matrix $W^{(t)} \in \mathbb{R}^{c_{2}^{(t)} \times c_{0}}$ where entries are independent Bernoulli variables with parameter $q$ ($Ber(q)$). Consider constructing this target using random "ID cond features" (a sparse identity-like structure) defined over a source network with $c_2$ output channels and $c_0$ input channels, where the features exist with probability $p$. The target matrix can be perfectly matched (i.e., represented exactly by selecting appropriate existing features via permutation and scaling) with probability at least $1-\delta$, if the number of source output channels $c_2$ is $k$ times the number of target output channels $c_{2}^{(t)}$ ($c_{2}=kc_{2}^{(t)}$), where the factor $k$ satisfies $k\ge\frac{\log(1-(1-\delta)^{1/c_{2}^{(t)}})}{\log(1-(1-q(1-p))^{c_{0}})}$.
Let $X^{(p)}$ be a sequence of $p \times m$ random matrices as $p \to \infty$. Assume the columns of $X^{(p)}$ are independent random vectors. Further assume each column vector $x \in \mathbb{R}^p$ follows a "block-independent model": its $p$ coordinates can be partitioned into blocks $I_1, I_2, ...$ with sizes $d_1, d_2, ...$ such that the random variables corresponding to coordinates in different blocks are independent (variables within the same block may be dependent). Assume the vectors are isotropic ($Exx^T = I$). Let the aspect ratio $p/m$ converge to a constant $\lambda \in (0,\infty)$. Assume the size of the largest block is asymptotically smaller than $p$, i.e., $\max_k d_k = o(p)$. Also, assume all entries of the matrix $X^{(p)}$ have uniformly bounded fourth moments. Then, with probability 1, the empirical spectral distribution (ESD) of the sample covariance matrix $W = \frac{1}{m} X^{(p)} (X^{(p)})^T$ converges weakly in distribution to the Marchenko-Pastur distribution with parameter $\lambda$.
