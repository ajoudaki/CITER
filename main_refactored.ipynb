{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading cached tokenized results from ./cache/tokenized_1caf5def_895012ad817559b15b42e1d366769a67.pt\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Iterator, Union, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.cuda.amp  # For automatic mixed precision\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[Category:.?\\]\\]|\\[\\[File:.?\\]\\]|\\{\\{stub\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if len(collated_data) > 1000:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            citation_ids = np.zeros(max_targets, dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source\n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "            \n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "                'target_count': len(present_citations),\n",
    "                # 'citation_ids': torch.tensor(citation_ids, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    target_art_ids = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_ids = torch.cat([item['target_ids'][:item['target_count']] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'][:item['target_count']] for item in batch])\n",
    "    # citation_ids = torch.cat([item['citation_ids'][:item['target_count']] for item in batch])\n",
    "    target_counts = torch.tensor([item['target_count'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'target_counts': target_counts,\n",
    "        # 'citation_ids': citation_ids\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration   the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 100\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Load articles\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "config = ExperimentConfig()\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "\n",
    "\n",
    "# results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "\n",
    "# # This will now use caching directly \n",
    "results = tokenize_sources(cache_path='./cache/tokenized_1caf5def_895012ad817559b15b42e1d366769a67.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Collate the data\n",
    "collated_data = collate(results, tokenizer, config)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 150/237381 [00:00<05:42, 692.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([77, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237381, 237381, 237381, 1056)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources), len(citation_data), len(results), len(collated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading articles from JSONL file...\n",
      "INFO:root:Loaded 237381 articles.\n"
     ]
    }
   ],
   "source": [
    "sources = tokenizer.batch_decode(batch['source_ids'])\n",
    "# cited_articles = [preprocessor.id2ref[id] for id in batch['cited_art_ids']]\n",
    "# target_articles = [preprocessor.id2ref[id] for id in batch['target_art_ids']]\n",
    "targets = tokenizer.batch_decode(batch['target_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512]), torch.Size([77, 100]), tensor(81), 81, 77)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "\n",
    "batch['source_ids'].shape, batch['target_ids'].shape, (batch['source_ids']==cite_token).sum(), len(batch['cited_art_ids']), len(batch['target_art_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "[[File:Colorful spring garden.jpg|thumb|180px|right|[[Spring]] flowers in April in the [[Northern Hemisphere]].]]\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[c\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "' ' ' april ' ' ' ( apr. ) is the fourth [ [ month ] ] of the [ [ year ] ] in the [ [ julian calendar | julian ] ] and [ [ gregorian calendar ] ] s, and comes between [ [ march ] ] and <CITE> ]. it is one of four months to have 30 [ [ day ] ] s. april always begins on the same day of the week as <CITE> ], and additionally, [ [ january ] ] in leap years. april always ends on the same day of the week as <CITE> ]. = = the month = = [ [ file : colorful spring garden. jpg | thumb | 180px | right | [ [ spring ] ] flowers in april in the [ [ northern hemisphere ] ]. ] ] april comes between [ [ march ] ] and <CITE> ], making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as <CITE> ], [ [ september ] ] and [ [ november ] ] are later in the year. april begins on the same day of the week as <CITE> ] every year and on the same day of the week as [ [ january ] ] in [ [ leap year ] ] s. april ends on the same day of the week as <CITE> ] every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in <CITE> ] s, april starts on the same day of the week as [ [ october ] ] of the previous year, and in [ [ leap year ] ] s, <CITE> ] of the previous year. in common years, april finishes on the same day of the week as <CITE> ] of the previous year, and in leap years, [ [ february ] ] and [ [ october ] ] of the previous year. in common years immediately after other common years, april starts on the same day of the week as [ [ january ] ] of the previous year, and in leap years and years immediately after that, april finishes on the same day of the week as january of the previous year. in years immediately before common years, april starts on the same day of the week as [ [ september ] ] and <CITE> ] of the following year, and in years immediately before leap years, <CITE> ] of the following year. in years immediately before common years [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Target: id=244:\n",
      "'''May''' is the fifth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian c\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused239]\n",
      "\n",
      "\n",
      "Target: id=208:\n",
      "'''July''' (Jul.) is the seventh [[month]] of the [[year]] in the [[Gregorian calendar]], coming bet\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused203]\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[UNK]\n",
      "\n",
      "\n",
      "Target: id=244:\n",
      "'''May''' is the fifth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian c\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused239]\n",
      "\n",
      "\n",
      "Target: id=207:\n",
      "'''June''' (Jun.) is the sixth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gre\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused202]\n",
      "\n",
      "\n",
      "Target: id=208:\n",
      "'''July''' (Jul.) is the seventh [[month]] of the [[year]] in the [[Gregorian calendar]], coming bet\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused203]\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[UNK]\n",
      "\n",
      "\n",
      "Target: id=1044:\n",
      "'''common year''' is a [[calendar year]] with 365 [[day]]s.  It is a year that is not a [[leap year]\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "h\n",
      "\n",
      "\n",
      "Target: id=244:\n",
      "'''May''' is the fifth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian c\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused239]\n",
      "\n",
      "\n",
      "Target: id=208:\n",
      "'''July''' (Jul.) is the seventh [[month]] of the [[year]] in the [[Gregorian calendar]], coming bet\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused203]\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[UNK]\n",
      "\n",
      "\n",
      "Target: id=207:\n",
      "'''June''' (Jun.) is the sixth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gre\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused202]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "for i, target_art_id in enumerate(sample['cited_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['cited_art_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:100]}\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:100]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3933, 3934,  140,  243,  208])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target_art_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
