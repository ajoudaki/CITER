{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import tqdm \n",
    "import yaml\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    bracket_tokens = tokenizer.convert_tokens_to_ids(['[',']'])\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if config.collate_sample_size and len(collated_data)>config.collate_sample_size:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source: \n",
    "            # only keep citation tokens that are sampled to be masked \n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            # don't mask citations that are not sampled \n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            # remove brackets from the rest of the text \n",
    "            mask_tokens = np.where(np.isin(input_ids,bracket_tokens),1, mask_tokens)\n",
    "            # don't mask the citation tokens \n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            # set the citation tokens (first token of a citation range) as special token <CITE> \n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            # mask all tokens in a citation, except for the first (special) token \n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "\n",
    "            # keep the cited article ids in the text in the order they appear (with repeats)\n",
    "            # & keep the unique cited artile ids \n",
    "            # this will enable us to link each special cite token to a target via the article id\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "\n",
    "\n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_art_ids_all = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    target_ids = torch.cat([item['target_ids'] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'] for item in batch])\n",
    "\n",
    "    # Get unique indices and inverse indices\n",
    "    target_art_ids, unique_indices = np.unique(target_art_ids_all.numpy(), return_index=True)\n",
    "    target_art_ids = torch.tensor(target_art_ids)\n",
    "    unique_indices = torch.tensor(unique_indices)\n",
    "    \n",
    "    # Use unique indices to get corresponding targets\n",
    "    target_ids = target_ids[unique_indices]\n",
    "    target_attention_mask = target_attention_mask[unique_indices]\n",
    "\n",
    "    id2i = {id.item():i for i,id in enumerate(target_art_ids)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids],dtype=torch.long)\n",
    "\n",
    "      \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration   the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 100\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    collate_sample_size: int = None,\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CitationConfig(PretrainedConfig):\n",
    "    \"\"\"Configuration class for CitationModel.\"\"\"\n",
    "    model_type = \"citation\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"bert-base-uncased\",\n",
    "        vocab_size=30522,\n",
    "        cite_token_id=None,\n",
    "        ref_token_id=None,\n",
    "        temperature=0.07,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model_name = base_model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cite_token_id = cite_token_id\n",
    "        self.ref_token_id = ref_token_id\n",
    "        self.temperature = temperature\n",
    "\n",
    "@dataclass\n",
    "class CitationModelOutput:\n",
    "    \"\"\"Custom output class for the citation model.\"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    cite_embeds: Optional[torch.FloatTensor] = None\n",
    "    ref_embeds: Optional[torch.FloatTensor] = None\n",
    "\n",
    "class CitationModel(nn.Module):\n",
    "    \"\"\"Custom model for citation matching using transformer embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: CitationConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base model configuration\n",
    "        base_config = AutoConfig.from_pretrained(config.base_model_name)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # Load base transformer model\n",
    "        self.transformer = AutoModel.from_pretrained(config.base_model_name)\n",
    "        \n",
    "        # Resize token embeddings if needed\n",
    "        if config.vocab_size != self.transformer.config.vocab_size:\n",
    "            self.transformer.resize_token_embeddings(config.vocab_size)\n",
    "    \n",
    "    def get_citation_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for citation token positions.\"\"\"\n",
    "        return input_ids == self.config.cite_token_id\n",
    "    \n",
    "    def get_reference_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for reference token positions.\"\"\"\n",
    "        return input_ids == self.config.ref_token_id\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        source_ids: torch.Tensor,\n",
    "        target_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        target_attention_mask: Optional[torch.Tensor] = None,\n",
    "        cited_art_ids: Optional[torch.Tensor] = None,\n",
    "        target_art_ids: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[Tuple, CitationModelOutput]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        \n",
    "        # Process source text\n",
    "        source_outputs = self.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Process target text\n",
    "        target_outputs = self.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get citation mask and extract citation embeddings\n",
    "        cite_mask = self.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        # Get reference mask and extract reference embeddings\n",
    "        ref_mask = self.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) / self.config.temperature\n",
    "\n",
    "        # compute the loss \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        if return_dict:\n",
    "            return CitationModelOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                cite_embeds=cite_embeds,\n",
    "                ref_embeds=ref_embeds\n",
    "            )\n",
    "        \n",
    "        return (loss, logits, cite_embeds, ref_embeds)\n",
    "\n",
    "\n",
    "def train_citation_model(\n",
    "    model,\n",
    "    results,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    train_ratio: float = 0.8,\n",
    "    num_epochs: int = 5,\n",
    "    learning_rate: float = 1.5e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_steps: int = 0,\n",
    "    device: str = None,\n",
    "    save_path: str = \"citation_model.pt\",\n",
    "    batch_size: int = 128,\n",
    "    temperatures = []\n",
    "):\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable memory efficient training\n",
    "    model.transformer.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch < len(temperatures):\n",
    "            model.config.temperature = temperatures[epoch]\n",
    "            print(f\"temperature changed to {temperatures[epoch]}\")\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Create new collated data for this epoch\n",
    "        print(\"Collating training data with new random masks...\")\n",
    "        collated = collate(results, tokenizer, config)\n",
    "        dataset = CitationDataset(collated)\n",
    "        train_size = int(len(dataset) * 0.8)\n",
    "        train_dataset = dataset[:train_size]\n",
    "        val_dataset = dataset[train_size:]\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"Running validation...\")\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_steps = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(val_dataloader, desc=\"Validation\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "        print(f\"\\nValidation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, save_path)\n",
    "            print(f\"Saved new best model to {save_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading cached tokenized results from ./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2238248/2184520446.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature changed to 1\n",
      "\n",
      "Epoch 1/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<06:53, 573.52it/s]\n",
      "Training:   0%|                                                                                                                         | 0/9 [00:00<?, ?it/s]/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.09s/it, loss=3.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 6.0797\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.13it/s, loss=2.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 4.8741\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "temperature changed to 0.5\n",
      "\n",
      "Epoch 2/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<06:52, 574.60it/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:28<00:00,  3.17s/it, loss=3.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 5.3684\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.12it/s, loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 4.3607\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "temperature changed to 0.2\n",
      "\n",
      "Epoch 3/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:05, 556.91it/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.08s/it, loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 4.0308\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 3.4653\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "temperature changed to 0.1\n",
      "\n",
      "Epoch 4/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:12, 547.85it/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.08s/it, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 2.8987\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.6954\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "temperature changed to 0.07\n",
      "\n",
      "Epoch 5/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<06:58, 566.35it/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.10s/it, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 2.2807\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.4470\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "temperature changed to 0.06\n",
      "\n",
      "Epoch 6/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:02, 561.55it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.10s/it, loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.8894\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.4007\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "temperature changed to 0.05\n",
      "\n",
      "Epoch 7/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:03, 560.10it/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.09s/it, loss=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.6242\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.2822\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "\n",
      "Epoch 8/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:14, 546.29it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.10s/it, loss=0.764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.4499\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.09it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.3854\n",
      "\n",
      "Epoch 9/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:28, 528.52it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:28<00:00,  3.11s/it, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.2798\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.08it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.3354\n",
      "\n",
      "Epoch 10/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:12, 548.35it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.11s/it, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.1712\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.10it/s, loss=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.4682\n",
      "\n",
      "Epoch 11/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:10, 550.51it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.11s/it, loss=0.663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.0717\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.10it/s, loss=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.2377\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "\n",
      "Epoch 12/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:10, 550.76it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.11s/it, loss=0.457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.9803\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.2884\n",
      "\n",
      "Epoch 13/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:08, 553.24it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:28<00:00,  3.11s/it, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.8759\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.08it/s, loss=1.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.5096\n",
      "\n",
      "Epoch 14/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<06:56, 569.22it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.10s/it, loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.8347\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.09it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.4250\n",
      "\n",
      "Epoch 15/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:29, 527.97it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:27<00:00,  3.11s/it, loss=0.611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.8076\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.11it/s, loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss: 2.1802\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Load articles\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "config = ExperimentConfig(collate_sample_size=2000,target_len=128)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "\n",
    "\n",
    "# results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "\n",
    "# This will now use caching directly \n",
    "# results = tokenize_sources(cache_path='./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt')\n",
    "\n",
    "\n",
    "# Create model config\n",
    "model_config = CitationConfig(\n",
    "    base_model_name=config.model_name,\n",
    "    vocab_size=len(tokenizer),\n",
    "    cite_token_id=tokenizer.convert_tokens_to_ids(config.cite_token),\n",
    "    ref_token_id=tokenizer.convert_tokens_to_ids(config.ref_token),\n",
    "    temperature=config.temperature,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = CitationModel(model_config)\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Replace the previous training code with:\n",
    "trained_model = train_citation_model(\n",
    "    model=model,\n",
    "    results=results,  # Pass raw results instead of dataloader\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    "    num_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1.5e-4,\n",
    "    batch_size=200,\n",
    "    save_path=\"./experiments/best_citation_model.pt\",\n",
    "    temperatures = [1, 0.5, .2, .1, 0.07, 0.06, 0.05],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                  | 279/237381 [00:00<07:03, 560.42it/s]\n"
     ]
    }
   ],
   "source": [
    "collated = collate(results, tokenizer, config)\n",
    "dataset = CitationDataset(collated)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "for i,batch in enumerate(dataloader):\n",
    "    if i > 2:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(799)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([80, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break\n",
    "\n",
    "labels = [torch.where(batch['cited_art_ids']==id)[0][0].item() for id in batch['target_art_ids']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useless =  0.1\n",
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "' ' ' april ' ' ' ( apr. ) is the fourth month of the year in the julian calendar | julian and gregorian calendar s, and comes between march and may. it is one of four months to have 30 <CITE> s. april always begins on the same day of the week as july, and additionally, january in leap years. april always ends on the same day of the week as <CITE>. = = the month = = april comes between march and may, making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as june, <CITE> and november are later in the year. april begins on the same day of the week as july every year and on the same day of the week as january in <CITE> s. april ends on the same day of the week as <CITE> every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in common year s, april starts on the same day of the week as <CITE> of the previous year, and in <CITE> s, may of the previous year. in common years, april finishes on the same day of the week as july of the previous year, and in leap years, february and <CITE> of the previous year. in common years immediately after other common years, april starts on the same day of the week as january of the previous year, and in leap years and years immediately after that, april finishes on the same day of the week as january of the previous year. in years immediately before common years, april starts on the same day of the week as <CITE> and <CITE> of the following year, and in years immediately before leap years, june of the following year. in years immediately before common years, april finishes on the same day of the week as september of the following year, and in years immediately before leap years, march and june of the following year. april [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Source attention mask: 126, \n",
      "Target: id=357:\n",
      "'''September''' (Sep.) is the ninth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[August]] and [[October]]. It has 30 [[day]]s. Its name comes from the [[Latin]] word ''sep...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' september ' ' ' ( sep. ) is the ninth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ august ] ] and [ [ october ] ]. it has 30 [ [ day ] ] s. its name comes from the [ [ latin ] ] word ' ' sept ' ' for \" seven \" ( it was the seventh month of the year, before [ [ january ] ] and [ [ february <REF>\n",
      "\n",
      "\n",
      "Target: id=832:\n",
      "'''day''' is the time it takes the [[Earth]] to spin around once. It is day time on the side of the Earth that is facing the [[Sun]]. When it is [[wikt:night|night]] time, that side of the Earth is fa...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' day ' ' ' is the time it takes the [ [ earth ] ] to spin around once. it is day time on the side of the earth that is facing the [ [ sun ] ]. when it is [ [ wikt : night | night ] ] time, that side of the earth is facing away from the sun. it takes 24 [ [ hour ] ] s for the earth to spin once, so that is one day, including the day time and night time <REF>\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[November]] (of the current year) and [[January]] (of the following year). It has...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' december ' ' ' ( dec. ) is the twelfth and last [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ november ] ] ( of the current year ) and [ [ january ] ] ( of the following year ). it has 31 days. with the name of the month coming from the [ [ latin ] ] ' ' decem ' ' for \" ten \", it was the tenth month of <REF>\n",
      "\n",
      "\n",
      "Target: id=297:\n",
      "'''October''' (Oct.) is the tenth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[September]] and [[November]]. It has 31 [[day]]s. The name comes from the [[Latin]] ''octo''...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' october ' ' ' ( oct. ) is the tenth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ september ] ] and [ [ november ] ]. it has 31 [ [ day ] ] s. the name comes from the [ [ latin ] ] ' ' octo ' ' for \" eight \". it was the eighth month of the year before [ [ january ] ] and [ [ february ] <REF>\n",
      "\n",
      "\n",
      "Target: id=228:\n",
      "'''leap year''' is a [[calendar year]] in which an extra [[day]] is added to the [[Gregorian calendar]], which is used by most of the world. A [[common year]] has 365 [[day]]s, but a leap year has 366...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' leap year ' ' ' is a [ [ calendar year ] ] in which an extra [ [ day ] ] is added to the [ [ gregorian calendar ] ], which is used by most of the world. a [ [ common year ] ] has 365 [ [ day ] ] s, but a leap year has 366 days. the extra day, february 29, is added to the [ [ month ] ] of [ [ february ] ]. in a common year, february <REF>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "useless_chars = np.sum([c==']' for c in source_text])*2/len(source_text)\n",
    "print('useless = ', useless_chars)\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "print(f\"Source attention mask: {(sample['attention_mask']==0).sum()}, \")\n",
    "\n",
    "for i, target_art_id in enumerate(sample['target_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['target_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:200]}...\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3]), array([0, 1, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, first_indexes = np.unique(np.array([1,2,1, 3, 3, 3]), return_index=True)\n",
    "unique, first_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
