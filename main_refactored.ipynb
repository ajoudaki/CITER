{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 150/237381 [00:00<05:33, 710.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([79, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Iterator, Union, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tqdm \n",
    "import hashlib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.cuda.amp  # For automatic mixed precision\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if len(collated_data) > 1000:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source\n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "            \n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "                'target_count': len(present_citations),\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    target_art_ids = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_ids = torch.cat([item['target_ids'][:item['target_count']] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'][:item['target_count']] for item in batch])\n",
    "    target_counts = torch.tensor([item['target_count'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'target_counts': target_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration   the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 100\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # # Load articles\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "# config = ExperimentConfig()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# tokenizer.add_special_tokens({\n",
    "#     'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "# })\n",
    "\n",
    "\n",
    "# results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "\n",
    "# # # # This will now use caching directly \n",
    "# # results = tokenize_sources(cache_path='./cache/tokenized_1caf5def_895012ad817559b15b42e1d366769a67.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Collate the data\n",
    "collated_data = collate(results, tokenizer, config)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 150/237381 [00:00<05:42, 692.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([77, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237381, 237381, 237381, 1056)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources), len(citation_data), len(results), len(collated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading articles from JSONL file...\n",
      "INFO:root:Loaded 237381 articles.\n"
     ]
    }
   ],
   "source": [
    "sources = tokenizer.batch_decode(batch['source_ids'])\n",
    "# cited_articles = [preprocessor.id2ref[id] for id in batch['cited_art_ids']]\n",
    "# target_articles = [preprocessor.id2ref[id] for id in batch['target_art_ids']]\n",
    "targets = tokenizer.batch_decode(batch['target_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512]), torch.Size([77, 100]), tensor(81), 81, 77)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "\n",
    "batch['source_ids'].shape, batch['target_ids'].shape, (batch['source_ids']==cite_token).sum(), len(batch['cited_art_ids']), len(batch['target_art_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "' ' ' april ' ' ' ( apr. ) is the fourth [ [ month ] ] of the <CITE> in the [ [ julian calendar | julian ] ] and <CITE> s, and comes between [ [ march ] ] and [ [ may ] ]. it is one of four months to have 30 <CITE> s. april always begins on the same day of the week as [ [ july ] ], and additionally, <CITE> in leap years. april always ends on the same day of the week as [ [ december ] ]. = = the month = = april comes between [ [ march ] ] and [ [ may ] ], making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as <CITE>, [ [ september ] ] and [ [ november ] ] are later in the year. april begins on the same day of the week as [ [ july ] ] every year and on the same day of the week as <CITE> in [ [ leap year ] ] s. april ends on the same day of the week as [ [ december ] ] every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in [ [ common year ] ] s, april starts on the same day of the week as [ [ october ] ] of the previous year, and in [ [ leap year ] ] s, [ [ may ] ] of the previous year. in common years, april finishes on the same day of the week as [ [ july ] ] of the previous year, and in leap years, [ [ february ] ] and [ [ october ] ] of the previous year. in common years immediately after other common years, april starts on the same day of the week as <CITE> of the previous year, and in leap years and years immediately after that, april finishes on the same day of the week as january of the previous year. in years immediately before common years, april starts on the same day of the week as [ [ september ] ] and [ [ december ] ] of the following year, and in years immediately before leap years, <CITE> of the following year. in years immediately before common years, april finishes on the same day of the week as september of the following year, and in years immediately before leap years, [ [ march ] ] and june of the following year. april [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Target: id=206:\n",
      "'''January''' (Jan.) is the first [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, coming between [[December]] (of the previous year) and [[February]] (of the c...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' january ' ' ' ( jan. ) is the first [ [ month ] ] of the [ [ year ] ] in the [ [ julian calendar | julian ] ] and [ [ gregorian calendar ] ] s, coming between [ [ december ] ] ( of the previous year ) and [ [ february ] ] ( of the current year ). it has 31 [ [ day ] ] s. january begins on the same day of the week as [ [ october ] ] in [ [ <REF>\n",
      "\n",
      "\n",
      "Target: id=207:\n",
      "'''June''' (Jun.) is the sixth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, coming between [[May]] and [[July]]. It has 30 [[day]]s. In [[Sweden]] in [[1732...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' june ' ' ' ( jun. ) is the sixth [ [ month ] ] of the [ [ year ] ] in the [ [ julian calendar | julian ] ] and [ [ gregorian calendar ] ] s, coming between [ [ may ] ] and [ [ july ] ]. it has 30 [ [ day ] ] s. in [ [ sweden ] ] in [ [ 1732 ] ] the month had 31 days. june never begins on the same day of the week as any <REF>\n",
      "\n",
      "\n",
      "Target: id=3934:\n",
      "'''Gregorian calendar''' is the [[calendar]] that is used throughout most of the world. It began being used in 1582. It replaced the previous [[Julian calendar]] because the Julian calendar had an err...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' gregorian calendar ' ' ' is the [ [ calendar ] ] that is used throughout most of the world. it began being used in 1582. it replaced the previous [ [ julian calendar ] ] because the julian calendar had an error : it added a [ [ leap year ] ] ( with an extra day every four years ) with no exceptions. the length of the julian year was exactly 365. 25 days ( 365 days and 6 hours ), but the actual time it takes for <REF>\n",
      "\n",
      "\n",
      "Target: id=449:\n",
      "'''year''' is about 365 [[day]]s (except in a [[leap year]]). It is the time it takes the [[Earth]] to go completely around ([[orbit]]) the [[sun]] once. A year is actually almost 365.25 days long, bu...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' year ' ' ' is about 365 [ [ day ] ] s ( except in a [ [ leap year ] ] ). it is the time it takes the [ [ earth ] ] to go completely around ( [ [ orbit ] ] ) the [ [ sun ] ] once. a year is actually almost 365. 25 days long, but a [ [ calendar ] ] has 365 days, except in a [ [ leap year ] ], which has 366 days. < ref <REF>\n",
      "\n",
      "\n",
      "Target: id=832:\n",
      "'''day''' is the time it takes the [[Earth]] to spin around once. It is day time on the side of the Earth that is facing the [[Sun]]. When it is [[wikt:night|night]] time, that side of the Earth is fa...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' day ' ' ' is the time it takes the [ [ earth ] ] to spin around once. it is day time on the side of the earth that is facing the [ [ sun ] ]. when it is [ [ wikt : night | night ] ] time, that side of the earth is facing away from the sun. it takes 24 [ [ hour ] ] s for the earth to spin once, so that is one day, including the day time and night time <REF>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "for i, target_art_id in enumerate(sample['target_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['target_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:200]}...\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title = april\n",
      "title = august\n",
      "title = art\n",
      "title = a\n",
      "title = air\n",
      "title = autonomous communities of spain\n",
      "title = alan turing\n",
      "title = alanis morissette\n",
      "title = adobe illustrator\n",
      "title = andouille\n",
      "title = farming\n",
      "title = arithmetic\n",
      "title = addition\n",
      "title = australia\n",
      "title = american english\n"
     ]
    }
   ],
   "source": [
    "def batch_tokenize(batch_sources, batch_citations):\n",
    "    # batch_sources = sources[batch_start:batch_end]\n",
    "    # batch_citations = citation_data[batch_start:batch_end]\n",
    "\n",
    "    # Ba tch encode\n",
    "    batch_encoded = tokenizer.batch_encode_plus(\n",
    "        batch_sources,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    all_results = []\n",
    "    # Process each item in the batch\n",
    "    for idx in range(len(batch_sources)):\n",
    "        offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "        input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "        \n",
    "        # Create offset to index mapping\n",
    "        off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "        off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "        \n",
    "        # Create citation tokens array\n",
    "        mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "        cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "        \n",
    "        # Fill in citations\n",
    "        for i, j, art_id in batch_citations[idx]:\n",
    "            s, e = off2i[i], off2i[j]\n",
    "            cite_tokens[s] = art_id\n",
    "            mask_tokens[s:e] = art_id\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'input_ids': np.array(input_ids),\n",
    "            'cite_tokens': cite_tokens,\n",
    "            'mask_tokens': mask_tokens,\n",
    "            'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "        })\n",
    "    return all_results\n",
    "\n",
    "# preprocessor = WikiProcessor()\n",
    "sources, citation_data = preprocessor.find_source_citations()\n",
    "for i in range(15):\n",
    "    ref = preprocessor.id2ref[i+1]\n",
    "    print(f\"title = {ref}\")\n",
    "    if 'File:' not in sources[i]:\n",
    "        continue\n",
    "    result = batch_tokenize(sources[i:i+1], citation_data[i:i+1])[0]\n",
    "    print(result['input_ids'].shape, result['cite_tokens'].shape, result['mask_tokens'].shape)\n",
    "    input_ids = result['input_ids']\n",
    "    cite_tokens = result['cite_tokens']\n",
    "    mask_tokens = result['mask_tokens']\n",
    "    \n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print('original text = ', decoded_text[:1000])\n",
    "    input_ids = input_ids[mask_tokens==0]\n",
    "    print('#'*50)\n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print('masked text = ', decoded_text[:1000])\n",
    "    print('\\n'*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " [[Category:something something]]'''Iran''', officially the '''Islamic''' '''Republic of Iran''', also known as '''Persia''', is a [[country]] in [[Western Asia]]. It is part of the [[Middle East]] region. It shares [[border]]s with [[Afghanistan]], [[Armenia]], [[Azerbaijan]], [[Iraq]], [[Pakistan]], [[Turkey]], and [[Turkmenistan]].\n",
      "[[Tehran]] is the [[Capital (city)|capital]] and biggest [[city]]. Iran is the eighteenth largest country in the world. It has more than 84.9 million people. Iran has been a member of the [[United Nations]] since 1945. It is a member of the [[Organization of the Petroleum Exporting Countries]] (OPEC).<ref name=\"un.org 2009\">{{cite web | title=United Nations Member States | website=un.org | date=2009-04-30 | url=http://www.un.org/en/members/index.shtml | archive-url=https://web.archive.org/web/20140412154940/http://www.un.org/en/members/index.shtml | archive-date=2014-04-12 | url-status=dead | access-date=2022-12-18}}</ref> It is an [[Islamic republic]].\n",
      "== History ==\n",
      "{{main|History of Iran}}\n",
      "In the past, Iran was called \"Persia\" by people outside of the [[country]]. The people that lived there called the country \"Iran\". The official name was Persia, The name Persia was used when dealing with other countries and in [[government]] papers.\n",
      "In 1935, [[Reza Shāh Pahlavi]] was [[Shah]] of Iran. He officially asked foreigners to call the country \"Iran\". This was done to show that Iran belongs to all the non-Persian Iranians as well as to Persian Iranians. The name Iran means ''land of the [[Aryan]]s''. It is used in the ancient book of the [[Zoroastrian]]s, the [[Avesta (religious scripture)|Avesta]]. In the 19th and early 20th century, the name ''Aryan'' was used by [[Europe]]ans to mean ''all [[Indo-European people|Indo-European]]s''.  The \"Aryan Race\" was a term that [[Adolph Hitler|Hitler]] used to describe his \"Superior\" or \"perfect\" race, but it first meant Iranians.<ref>Norton, 2002</ref> \"Aryan\" means \"noble\" in Iranian languages.\n",
      "=== Persian Empire ===\n",
      "Around 500 BC, the area that is now Iran was the center of the [[Achaemenid Empire]]. The [[Ancient Greece|Greek]] city states fought against the Persian armies led by [[Darius the Great]] and [[Xerxes]]. Then [[Alexander the Great]] took the country by fighting the Achaemenid dynasty of [[Achaemenid Empire|Persia]]. He ruled until he died,then the Greek Seleucids ruled until they were defeated by the Parthian Empire which later fought the [[Roman Empire]].<ref name=\"BBC\">{{cite news|date=16 August 2011|title=Iran Country Profile|work=[[BBC News]]|url=https://www.bbc.co.uk/news/world-middle-east-14541327|url-status=live|archive-url=https://web.archive.org/web/20141125011109/http://www.bbc.co.uk/news/world-middle-east-14541327|archive-date=25 November 2014|accessdate=8 August 2012}}</ref><ref>{{cite web|title=\"CESWW\" – Definition of Central Eurasia|url=http://cesww.fas.harvard.edu/ces_definition.html|url-status=dead|archive-url=https://web.archive.org/web/20100805052739/http://cesww.fas.harvard.edu/ces_definition.html|archive-date=5 August 2010|publisher=Cesww.fas.harvard.edu|accessdate=1 August 2010}}</ref><ref>{{cite web|date=14 June 2013|title=Iran Guide|url=http://travel.nationalgeographic.com/places/countries/country_iran.html|url-status=live|archive-url=https://web.archive.org/web/20091212095435/http://travel.nationalgeographic.com/places/countries/country_iran.html|archive-date=12 December 2009|work=National Geographic|accessdate=21 June 2013}}</ref>\n",
      "After the Parthians, the Sassanian dynasty (224-651) took over. Other people took Persia by [[war|conquest]], like the [[Arab]]s (7th century), [[Turkic peoples|Turk]]s (10th century) and [[Mongols]] (13th century).  However, Iran has always had a different and distinct native Iranian [[culture]] which has continued to survive.\n",
      "After the Muslim Arabs conquered the Iranians, Persian women were enslaved as domestic servants while Persian men were forced to build and farm and engage in hard labour.<ref>{{cite book |editor1-last=Fisher |editor1-first=William Bayne |editor2-last=Frye |editor2-first=Richard Nelson |editor3-last=Avery |editor3-first=Peter |editor4-last=Gershevitch |editor4-first=Ilya |editor5-last=Boyle |editor5-first=John Andrew |editor6-last=Yarshater |editor6-first=Ehsan |editor7-last=Jackson |editor7-first=Peter |title=The Cambridge History of Iran, Volume 4 |volume=The Cambridge History of Iran, Volume 4: From the Arab Invasion to the Saljuqs |date=1975 |publisher=Cambridge University Press |isbn=0521200938 |page=29 |edition=illustrated, reprint, reissue |chapter=CHAPTER I THE ARAB CONQUEST OF IRAN AND ITS AFTERMATH |chapter-url=https://books.google.com/books?id=hvx9jq_2L3EC&dq=%22farming+and+building+and+the+women+for+domestic+service%22&pg=PA29 |archive-url=https://web.archive.org/web/20221218025313/https://books.google.com/books?id=hvx9jq_2L3EC&dq=%22farming+and+building+and+the+women+for+domestic+service%22&pg=PA29 |archive-date=2022-12-18 |access-date=2022\n",
      "\n",
      "\n",
      "Cleaned text: \n",
      " '''Iran''', officially the '''Islamic''' '''Republic of Iran''', also known as '''Persia''', is a [[country]] in [[Western Asia]]. It is part of the [[Middle East]] region. It shares [[border]]s with [[Afghanistan]], [[Armenia]], [[Azerbaijan]], [[Iraq]], [[Pakistan]], [[Turkey]], and [[Turkmenistan]].\n",
      "[[Tehran]] is the [[Capital (city)|capital]] and biggest [[city]]. Iran is the eighteenth largest country in the world. It has more than 84.9 million people. Iran has been a member of the [[United Nations]] since 1945. It is a member of the [[Organization of the Petroleum Exporting Countries]] (OPEC).<ref name=\"un.org 2009\">{{cite web | title=United Nations Member States | website=un.org | date=2009-04-30 | url=http://www.un.org/en/members/index.shtml | archive-url=https://web.archive.org/web/20140412154940/http://www.un.org/en/members/index.shtml | archive-date=2014-04-12 | url-status=dead | access-date=2022-12-18}}</ref> It is an [[Islamic republic]].\n",
      "== History ==\n",
      "{{main|History of Iran}}\n",
      "In the past, Iran was called \"Persia\" by people outside of the [[country]]. The people that lived there called the country \"Iran\". The official name was Persia, The name Persia was used when dealing with other countries and in [[government]] papers.\n",
      "In 1935, [[Reza Shāh Pahlavi]] was [[Shah]] of Iran. He officially asked foreigners to call the country \"Iran\". This was done to show that Iran belongs to all the non-Persian Iranians as well as to Persian Iranians. The name Iran means ''land of the [[Aryan]]s''. It is used in the ancient book of the [[Zoroastrian]]s, the [[Avesta (religious scripture)|Avesta]]. In the 19th and early 20th century, the name ''Aryan'' was used by [[Europe]]ans to mean ''all [[Indo-European people|Indo-European]]s''.  The \"Aryan Race\" was a term that [[Adolph Hitler|Hitler]] used to describe his \"Superior\" or \"perfect\" race, but it first meant Iranians.<ref>Norton, 2002</ref> \"Aryan\" means \"noble\" in Iranian languages.\n",
      "=== Persian Empire ===\n",
      "Around 500 BC, the area that is now Iran was the center of the [[Achaemenid Empire]]. The [[Ancient Greece|Greek]] city states fought against the Persian armies led by [[Darius the Great]] and [[Xerxes]]. Then [[Alexander the Great]] took the country by fighting the Achaemenid dynasty of [[Achaemenid Empire|Persia]]. He ruled until he died,then the Greek Seleucids ruled until they were defeated by the Parthian Empire which later fought the [[Roman Empire]].<ref name=\"BBC\">{{cite news|date=16 August 2011|title=Iran Country Profile|work=[[BBC News]]|url=https://www.bbc.co.uk/news/world-middle-east-14541327|url-status=live|archive-url=https://web.archive.org/web/20141125011109/http://www.bbc.co.uk/news/world-middle-east-14541327|archive-date=25 November 2014|accessdate=8 August 2012}}</ref><ref>{{cite web|title=\"CESWW\" – Definition of Central Eurasia|url=http://cesww.fas.harvard.edu/ces_definition.html|url-status=dead|archive-url=https://web.archive.org/web/20100805052739/http://cesww.fas.harvard.edu/ces_definition.html|archive-date=5 August 2010|publisher=Cesww.fas.harvard.edu|accessdate=1 August 2010}}</ref><ref>{{cite web|date=14 June 2013|title=Iran Guide|url=http://travel.nationalgeographic.com/places/countries/country_iran.html|url-status=live|archive-url=https://web.archive.org/web/20091212095435/http://travel.nationalgeographic.com/places/countries/country_iran.html|archive-date=12 December 2009|work=National Geographic|accessdate=21 June 2013}}</ref>\n",
      "After the Parthians, the Sassanian dynasty (224-651) took over. Other people took Persia by [[war|conquest]], like the [[Arab]]s (7th century), [[Turkic peoples|Turk]]s (10th century) and [[Mongols]] (13th century).  However, Iran has always had a different and distinct native Iranian [[culture]] which has continued to survive.\n",
      "After the Muslim Arabs conquered the Iranians, Persian women were enslaved as domestic servants while Persian men were forced to build and farm and engage in hard labour.<ref>{{cite book |editor1-last=Fisher |editor1-first=William Bayne |editor2-last=Frye |editor2-first=Richard Nelson |editor3-last=Avery |editor3-first=Peter |editor4-last=Gershevitch |editor4-first=Ilya |editor5-last=Boyle |editor5-first=John Andrew |editor6-last=Yarshater |editor6-first=Ehsan |editor7-last=Jackson |editor7-first=Peter |title=The Cambridge History of Iran, Volume 4 |volume=The Cambridge History of Iran, Volume 4: From the Arab Invasion to the Saljuqs |date=1975 |publisher=Cambridge University Press |isbn=0521200938 |page=29 |edition=illustrated, reprint, reissue |chapter=CHAPTER I THE ARAB CONQUEST OF IRAN AND ITS AFTERMATH |chapter-url=https://books.google.com/books?id=hvx9jq_2L3EC&dq=%22farming+and+building+and+the+women+for+domestic+service%22&pg=PA29 |archive-url=https://web.archive.org/web/20221218025313/https://books.google.com/books?id=hvx9jq_2L3EC&dq=%22farming+and+building+and+the+women+for+domestic+service%22&pg=PA29 |archive-date=2022-12-18 |access-date=2022-02-22 |url-status=bot: unknown \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(98, 109, 'country'),\n",
       " (113, 129, 'western asia'),\n",
       " (149, 164, 'middle east'),\n",
       " (183, 193, 'border'),\n",
       " (200, 215, 'afghanistan'),\n",
       " (217, 228, 'armenia'),\n",
       " (230, 244, 'azerbaijan'),\n",
       " (246, 254, 'iraq'),\n",
       " (256, 268, 'pakistan'),\n",
       " (270, 280, 'turkey'),\n",
       " (286, 302, 'turkmenistan'),\n",
       " (304, 314, 'tehran'),\n",
       " (322, 348, 'capital'),\n",
       " (361, 369, 'city'),\n",
       " (492, 510, 'united nations'),\n",
       " (944, 964, 'islamic republic')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def find_citations(text, articles_dict):\n",
    "    citations = []\n",
    "    for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "        match_text = match.group(1)\n",
    "        citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "        citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "        ref = None\n",
    "        for cit in citation:\n",
    "            if cit.lower() in articles_dict:\n",
    "                ref = cit.lower()\n",
    "                break\n",
    "        if ref:\n",
    "            citations.append((match.start(), match.end(), ref))\n",
    "    return citations\n",
    "\n",
    "\n",
    "def clean_wiki_text(text: str) -> str:\n",
    "    \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "    # Find main content starting from first bold title\n",
    "    match = re.search(r\"'''([^']+?)'''\", text)\n",
    "    if match:\n",
    "        text = text[match.start():]\n",
    "\n",
    "    # Remove wiki elements and clean up\n",
    "    text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "    # text = re.sub(r'\\[\\[Category:.*\\]\\]', '', text)\n",
    "    # text = re.sub(r'\\[\\[File:.*\\]\\]', '', text)\n",
    "    # text = re.sub(r'\\[\\[Category:.*\\]\\]', '', text)\n",
    "    # text = re.sub(r'\\{\\{stub.*\\}\\}', '', text)\n",
    "    return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "text = preprocessor.articles_dict['iran'] \n",
    "text = \"[[Category:something something]]\" + text\n",
    "print(f\"Original text: \\n {text[:5000]}\\n\\n\")\n",
    "text =clean_wiki_text(text)\n",
    "\n",
    "print(f\"Cleaned text: \\n {text[:5000]}\\n\\n\")\n",
    "\n",
    "find_citations(text[:1000], preprocessor.articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[[File:Colorful spring garden.jpg|thumb|180px|right|[[Spring]] flowers in April in the [[Northern Hemisphere]].]]',\n",
       " \"[[File:Aprilsnar 2001.png|thumb|200px|right|An [[April Fools' Day]] hoax for [[April 1]] in [[Copenhagen]].]]\",\n",
       " '[[File:Songkran in Wat Kungthapao 03.jpg|thumb|180px|right|[[Songkran]] celebration in [[Thailand]] around [[April 14]].]]',\n",
       " '[[File:Earth flag PD.jpg|thumb|200px|right|Proposed [[flag]] for [[Earth Day]] on [[April 22]].]]',\n",
       " \"[[File:St George's Day 2010 - 14.jpg|thumb|200px|right|[[Saint George]]'s Day on [[April 23]] in [[London]]'s [[Trafalgar Square]].]]\",\n",
       " '[[File:Anzac1.JPG|thumb|180px|right|[[ANZAC Day]] commemoration in [[Australia]] on [[April 25]].]]',\n",
       " \"[[File:Koninginnedag2007.jpg|thumb|180px|right|Queen's Day, [[April 30]], celebration in the [[Netherlands]]. It changed to King's Day, [[April 27]], in [[2014]].]]\",\n",
       " '[[File:Valborgsbrasa-1.jpg|thumb|210px|right|[[Walpurgis Night]] bonfire on [[April 30]] in [[Sweden]].]]',\n",
       " '[[File:Vajicka1.jpg|thumb|200px|right|Eggs celebrating [[Easter]], which often falls in April, but sometimes falls in [[March]].]]',\n",
       " '[[File:Aprilregen - Lithografie.jpg|thumb|200px|right|Image traditionally showing it as [[rain]]ing in April in the [[Northern Hemisphere]].]]',\n",
       " '[[File:Nunavut-Feierlichkeit (01-04-99).jpg|thumb|180px|right|[[Inauguration]] celebration for [[Nunavut]] on [[April 1]], [[1999]].]]',\n",
       " '[[File:Moai Rano raraku.jpg|thumb|160px|right|A statue on [[Easter Island]] - Jacob Roggeveen became the first [[Europe]]an to land there on [[April 5]], [[1722]].]]',\n",
       " '[[File:Titanic-New York Herald front page.jpeg|thumb|150px|right|[[Newspaper]] report on the sinking of the [[RMS Titanic]] on [[April 15]], [[1912]].]]',\n",
       " '[[File:San Francisco Fire Sacramento Street 1906-04-18.jpg|thumb|180px|right|[[Fire]]s after the [[San Francisco]] [[earthquake]] on [[April 18]], [[1906]].]]',\n",
       " '[[File:Anzac Beach 4th Bn landing 8am April 25 1915.jpg|thumb|180px|right|[[Australia]]n and [[New Zealand]] forces landing at Anzac Cove, [[April 25]], [[1915]].]]',\n",
       " '[[File:HMS Bounty.jpg|thumb|200px|right|Painting showing the Mutiny on the Bounty on [[April 28]], [[1789]].]]',\n",
       " '[[File:Juliana 1959.jpg|thumb|150px|right|Queen [[Juliana of the Netherlands]], who abdicated the throne on her 71st [[birthday]], [[April 30]], [[1980]].]]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\[\\[File:.*\\]\\]'\n",
    "text = preprocessor.articles_dict['april'] \n",
    "matches = re.finditer(pattern, text, re.X)\n",
    "[m.group() for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
