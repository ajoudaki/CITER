{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Iterator, Union, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.cuda.amp  # For automatic mixed precision\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[Category:.?\\]\\]|\\[\\[File:.?\\]\\]|\\{\\{stub\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources, citation_data, tokenizer, batch_size=1000, cache_dir=\"cache\"):\n",
    "    # Generate cache path\n",
    "    cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((config.max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((config.max_targets, config.target_len), dtype=np.int64)\n",
    "            citation_ids = np.zeros(config.max_targets, dtype=np.int64)\n",
    "            \n",
    "            # Sample citations if we have more than max_targets\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            \n",
    "            # Prepare source\n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), 0, mask_tokens)\n",
    "            mask_tokens[cite_tokens_mask > 0] = 0\n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "            cite_tokens = present_citations\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                target_data = id_to_tokenized[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                citation_ids[idx] = citation_id\n",
    "            \n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cite_tokens': torch.tensor(cite_tokens, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "                'target_count': len(present_citations),\n",
    "                'citation_ids': torch.tensor(citation_ids, dtype=torch.long)\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cite_tokens = torch.cat([item['cite_tokens'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_ids = torch.cat([item['target_ids'][:item['target_count']] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'][:item['target_count']] for item in batch])\n",
    "    citation_ids = torch.cat([item['citation_ids'][:item['target_count']] for item in batch])\n",
    "    target_counts = torch.tensor([item['target_count'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cite_tokens': cite_tokens,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'target_counts': target_counts,\n",
    "        'citation_ids': citation_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading articles from JSONL file...\n",
      "INFO:root:Loaded 237381 articles.\n",
      "INFO:root:Loading cached tokenized results from cache/tokenized_1caf5def_895012ad817559b15b42e1d366769a67.pt\n",
      "  0%|          | 150/237381 [00:00<13:50, 285.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([70, 100])\n",
      "Target counts: tensor([5, 5, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 100\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load articles\n",
    "preprocessor = WikiProcessor()\n",
    "sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "config = ExperimentConfig()\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "\n",
    "# # This will now use caching\n",
    "results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Collate the data\n",
    "collated_data = collate(results, tokenizer, config)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1012, 25392, 17788,  ...,     0,     0,     0],\n",
       "        [ 1033,  1012,  2087,  ...,     0,     0,     0],\n",
       "        [ 2000,  1996, 10925,  ...,  1027,  1027,  1999],\n",
       "        ...,\n",
       "        [ 2111,  2031,  1037,  ...,     0,     0,     0],\n",
       "        [ 1063, 29215,  1024,  ...,  2012,  2035,  1012],\n",
       "        [ 1004,  3720,  1027,  ...,  2024,  2763,  3182]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['source_ids']==cite_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(83)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "(batch['source_ids']==cite_token).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 82033,   1381,  15573,  57208,   7496,  82417,   7632,     90,  31926,\n",
       "          4074,  56491,  48808,   3627,  56492,  42839,  80840,  80944,  80945,\n",
       "         80942,  80843, 143770,   1329,    390,  20380,   1104,    832,   1907,\n",
       "         10207,  57174,  58992,    363,  10732,  28145,  15970,  15784,   6323,\n",
       "          1423,    403,    984,   1298,  12065,   1124,   8458,   8360,    628,\n",
       "           242,  80768,  80739,  80737,   1212,    243,   1044,    297, 129139,\n",
       "          1994,   2079,    278,  85075,    880,   1275,   1339,  20781,   2992,\n",
       "         98834,  99707,   4190,  99265,  81555,    889,   3125,   1195,   6230,\n",
       "          1886,    244,   1224,    482,    474,    292,    484,    117])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['cite_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['source_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
