{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import tqdm \n",
    "import yaml\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    bracket_tokens = tokenizer.convert_tokens_to_ids(['[',']'])\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if config.collate_sample_size and len(collated_data)>config.collate_sample_size:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source: \n",
    "            # only keep citation tokens that are sampled to be masked \n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            # don't mask citations that are not sampled \n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            # remove brackets from the rest of the text \n",
    "            mask_tokens = np.where(np.isin(input_ids,bracket_tokens),1, mask_tokens)\n",
    "            # don't mask the citation tokens \n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            # set the citation tokens (first token of a citation range) as special token <CITE> \n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            # mask all tokens in a citation, except for the first (special) token \n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "\n",
    "            # keep the cited article ids in the text in the order they appear (with repeats)\n",
    "            # & keep the unique cited artile ids \n",
    "            # this will enable us to link each special cite token to a target via the article id\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "\n",
    "\n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_art_ids_all = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    target_ids = torch.cat([item['target_ids'] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'] for item in batch])\n",
    "\n",
    "    # Get unique indices and inverse indices\n",
    "    target_art_ids, unique_indices = np.unique(target_art_ids_all.numpy(), return_index=True)\n",
    "    target_art_ids = torch.tensor(target_art_ids)\n",
    "    unique_indices = torch.tensor(unique_indices)\n",
    "    \n",
    "    # Use unique indices to get corresponding targets\n",
    "    target_ids = target_ids[unique_indices]\n",
    "    target_attention_mask = target_attention_mask[unique_indices]\n",
    "\n",
    "    id2i = {id.item():i for i,id in enumerate(target_art_ids)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids],dtype=torch.long)\n",
    "\n",
    "      \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class CitationConfig(PretrainedConfig):\n",
    "    \"\"\"Configuration class for CitationModel.\"\"\"\n",
    "    model_type = \"citation\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"bert-base-uncased\",\n",
    "        vocab_size=30522,\n",
    "        cite_token_id=None,\n",
    "        ref_token_id=None,\n",
    "        temperature=0.07,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model_name = base_model_name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cite_token_id = cite_token_id\n",
    "        self.ref_token_id = ref_token_id\n",
    "        self.temperature = temperature\n",
    "\n",
    "@dataclass\n",
    "class CitationModelOutput:\n",
    "    \"\"\"Custom output class for the citation model.\"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    cite_embeds: Optional[torch.FloatTensor] = None\n",
    "    ref_embeds: Optional[torch.FloatTensor] = None\n",
    "\n",
    "class CitationModel(nn.Module):\n",
    "    \"\"\"Custom model for citation matching using transformer embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: CitationConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base model configuration\n",
    "        base_config = AutoConfig.from_pretrained(config.base_model_name)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # Load base transformer model\n",
    "        self.transformer = AutoModel.from_pretrained(config.base_model_name)\n",
    "        \n",
    "        # Resize token embeddings if needed\n",
    "        if config.vocab_size != self.transformer.config.vocab_size:\n",
    "            self.transformer.resize_token_embeddings(config.vocab_size)\n",
    "    \n",
    "    def get_citation_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for citation token positions.\"\"\"\n",
    "        return input_ids == self.config.cite_token_id\n",
    "    \n",
    "    def get_reference_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for reference token positions.\"\"\"\n",
    "        return input_ids == self.config.ref_token_id\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        source_ids: torch.Tensor,\n",
    "        target_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        target_attention_mask: Optional[torch.Tensor] = None,\n",
    "        cited_art_ids: Optional[torch.Tensor] = None,\n",
    "        target_art_ids: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[Tuple, CitationModelOutput]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        \n",
    "        # Process source text\n",
    "        source_outputs = self.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Process target text\n",
    "        target_outputs = self.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get citation mask and extract citation embeddings\n",
    "        cite_mask = self.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        # Get reference mask and extract reference embeddings\n",
    "        ref_mask = self.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) / self.config.temperature\n",
    "\n",
    "        # compute the loss \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        if return_dict:\n",
    "            return CitationModelOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                cite_embeds=cite_embeds,\n",
    "                ref_embeds=ref_embeds\n",
    "            )\n",
    "        \n",
    "        return (loss, logits, cite_embeds, ref_embeds)\n",
    "\n",
    "def compute_retrieval_metrics(logits, labels, ks=[1, 5, 10, 50, 100, 1000]):\n",
    "    \"\"\"\n",
    "    Compute various retrieval metrics including top-k accuracy and MRR.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of similarity scores [num_queries, num_targets]\n",
    "        labels: Tensor of correct target indices [num_queries]\n",
    "        ks: List of k values for top-k accuracy\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing various retrieval metrics\n",
    "    \"\"\"\n",
    "    # Get rankings of correct targets\n",
    "    correct_scores = logits[torch.arange(logits.size(0)), labels]\n",
    "    rankings = (logits >= correct_scores.unsqueeze(1)).sum(1)\n",
    "    \n",
    "    # Compute MRR\n",
    "    mrr = (1.0 / rankings).mean().item()\n",
    "    \n",
    "    # Compute top-k accuracy for different k values\n",
    "    metrics = {'mrr': mrr}\n",
    "    for k in ks:\n",
    "        if k <= logits.size(1):  # Only compute if k is not larger than number of targets\n",
    "            top_k_acc = (rankings <= k).float().mean().item()\n",
    "            metrics[f'top_{k}_accuracy'] = top_k_acc\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate_citation_model(\n",
    "    model,\n",
    "    val_dataloader,\n",
    "    device: str = None,\n",
    "    return_embeddings: bool = False,\n",
    "    k_values: List[int] = [1, 5, 10, 50, 100, 1000]\n",
    "):\n",
    "    \"\"\"\n",
    "    Validates citation model performance by computing loss and various retrieval metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: The citation model to validate\n",
    "        val_dataloader: DataLoader containing validation data\n",
    "        device: Device to run validation on\n",
    "        return_embeddings: Whether to return computed embeddings\n",
    "        k_values: List of k values for top-k accuracy computation\n",
    "        \n",
    "    Returns:\n",
    "        dict containing validation metrics and optionally embeddings\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store accumulated embeddings and IDs\n",
    "    all_cite_embeds = []\n",
    "    all_ref_embeds = []\n",
    "    all_cited_art_ids = []\n",
    "    all_target_art_ids = []\n",
    "    \n",
    "    # Accumulate embeddings and IDs\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(val_dataloader, desc=\"Computing embeddings\"):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Process source text\n",
    "            source_outputs = model.transformer(\n",
    "                input_ids=batch['source_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Process target text\n",
    "            target_outputs = model.transformer(\n",
    "                input_ids=batch['target_ids'],\n",
    "                attention_mask=batch['target_attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Get citation mask and extract citation embeddings\n",
    "            cite_mask = model.get_citation_masks(batch['source_ids'])\n",
    "            cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "            \n",
    "            # Get reference mask and extract reference embeddings\n",
    "            ref_mask = model.get_reference_masks(batch['target_ids'])\n",
    "            ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "            ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "            \n",
    "            # Store embeddings and IDs\n",
    "            all_cite_embeds.append(cite_embeds.cpu())\n",
    "            all_ref_embeds.append(ref_embeds.cpu())\n",
    "            all_cited_art_ids.append(batch['cited_art_ids'].cpu())\n",
    "            all_target_art_ids.append(batch['target_art_ids'].cpu())\n",
    "    \n",
    "    # Concatenate all accumulated tensors\n",
    "    cite_embeds = torch.cat(all_cite_embeds)\n",
    "    ref_embeds = torch.cat(all_ref_embeds)\n",
    "    cited_art_ids = torch.cat(all_cited_art_ids)\n",
    "    target_art_ids = torch.cat(all_target_art_ids)\n",
    "    \n",
    "    # Get unique target art IDs and create mapping\n",
    "    target_art_ids_unique, unique_indices = np.unique(target_art_ids.numpy(), return_index=True)\n",
    "    target_art_ids_unique = torch.tensor(target_art_ids_unique)\n",
    "    ref_embeds_unique = ref_embeds[torch.tensor(unique_indices)]\n",
    "    \n",
    "    # Create ID to index mapping\n",
    "    id2i = {id.item(): i for i, id in enumerate(target_art_ids_unique)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids], dtype=torch.long)\n",
    "    \n",
    "    # Move tensors back to device for final computation\n",
    "    cite_embeds = cite_embeds.to(device)\n",
    "    ref_embeds_unique = ref_embeds_unique.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    logits = torch.matmul(cite_embeds, ref_embeds_unique.t()) / model.config.temperature\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    accuracy = (predictions == labels).float().mean().item()\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    retrieval_metrics = compute_retrieval_metrics(logits.cpu(), labels.cpu(), ks=k_values)\n",
    "    \n",
    "    # Prepare results dictionary\n",
    "    results = {\n",
    "        'loss': loss.item(),\n",
    "        'accuracy': accuracy,\n",
    "        'num_citations': len(cited_art_ids),\n",
    "        'num_unique_targets': len(target_art_ids_unique),\n",
    "        'mrr': retrieval_metrics['mrr']\n",
    "    }\n",
    "    \n",
    "    # Add top-k accuracies to results\n",
    "    for k in k_values:\n",
    "        if f'top_{k}_accuracy' in retrieval_metrics:\n",
    "            results[f'top_{k}_accuracy'] = retrieval_metrics[f'top_{k}_accuracy']\n",
    "    \n",
    "    if return_embeddings:\n",
    "        results.update({\n",
    "            'cite_embeds': cite_embeds.cpu(),\n",
    "            'ref_embeds': ref_embeds_unique.cpu(),\n",
    "            'cited_art_ids': cited_art_ids,\n",
    "            'target_art_ids': target_art_ids_unique,\n",
    "            'logits': logits.cpu(),\n",
    "            'labels': labels.cpu()\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_citation_model(\n",
    "    model,\n",
    "    results,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    train_ratio: float = 0.8,\n",
    "    num_epochs: int = 5,\n",
    "    learning_rate: float = 1.5e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_steps: int = 0,\n",
    "    device: str = None,\n",
    "    save_path: str = \"citation_model.pt\",\n",
    "    batch_size: int = 128,\n",
    "    temperatures = [],\n",
    "    k_values = [1, 5, 10, 50, 100, 1000]\n",
    "):\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable memory efficient training\n",
    "    model.transformer.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_metrics = {'loss': float('inf')}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        if epoch < len(temperatures):\n",
    "            model.config.temperature = temperatures[epoch]\n",
    "            print(f\"temperature changed to {temperatures[epoch]}\")\n",
    "        \n",
    "        # Create new collated data for this epoch\n",
    "        print(\"Collating training data with new random masks...\")\n",
    "        collated = collate(results, tokenizer, config)\n",
    "        dataset = CitationDataset(collated)\n",
    "        train_size = int(len(dataset) * train_ratio)\n",
    "        train_dataset = dataset[:train_size]\n",
    "        val_dataset = dataset[train_size:]\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size * 2, # use larger batch for validation (no gradients) \n",
    "            shuffle=False,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"\\nRunning validation...\")\n",
    "        model.eval()\n",
    "        \n",
    "        val_metrics = validate_citation_model(\n",
    "            model=model,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            k_values=k_values\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nValidation metrics:\")\n",
    "        print(f\"  Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"  Accuracy (top-1): {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Mean Reciprocal Rank: {val_metrics['mrr']:.4f}\")\n",
    "        print(f\"  Number of citations: {val_metrics['num_citations']}\")\n",
    "        print(f\"  Number of unique targets: {val_metrics['num_unique_targets']}\")\n",
    "        print(\"\\nTop-k accuracy:\")\n",
    "        for k in k_values:\n",
    "            if f'top_{k}_accuracy' in val_metrics:\n",
    "                print(f\"  k={k}: {val_metrics[f'top_{k}_accuracy']:.4f}\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if val_metrics['loss'] < best_val_metrics['loss']:\n",
    "            best_val_metrics = val_metrics\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'validation_metrics': val_metrics,\n",
    "                'temperature': model.config.temperature\n",
    "            }, save_path)\n",
    "            print(f\"\\nSaved new best model to {save_path}\")\n",
    "            print(f\"Best validation metrics so far:\")\n",
    "            print(f\"  Loss: {best_val_metrics['loss']:.4f}\")\n",
    "            print(f\"  Accuracy (top-1): {best_val_metrics['accuracy']:.4f}\")\n",
    "            print(f\"  MRR: {best_val_metrics['mrr']:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3612835/3229273527.py:614: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "temperature changed to 1\n",
      "\n",
      "Epoch 1/15\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████                                                                                                              | 8543/237381 [00:17<07:39, 497.57it/s]\n",
      "Training:  35%|███████████████████████████████████▎                                                                | 18/51 [01:02<01:54,  3.47s/it, loss=6.04]"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration   the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 128\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    collate_sample_size: int = None,\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Load articles\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "config = ExperimentConfig(collate_sample_size=50000,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "\n",
    "\n",
    "# results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "\n",
    "# This will now use caching directly \n",
    "# results = tokenize_sources(cache_path='./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt')\n",
    "\n",
    "\n",
    "# Create model config\n",
    "model_config = CitationConfig(\n",
    "    base_model_name=config.model_name,\n",
    "    vocab_size=len(tokenizer),\n",
    "    cite_token_id=tokenizer.convert_tokens_to_ids(config.cite_token),\n",
    "    ref_token_id=tokenizer.convert_tokens_to_ids(config.ref_token),\n",
    "    temperature=config.temperature,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = CitationModel(model_config)\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Replace the previous training code with:\n",
    "trained_model = train_citation_model(\n",
    "    model=model,\n",
    "    results=results,  # Pass raw results instead of dataloader\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    "    num_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1.5e-4,\n",
    "    batch_size=200,\n",
    "    train_ratio = 0.2,\n",
    "    save_path=\"./experiments/best_citation_model.pt\",\n",
    "    temperatures = [1, .5,.2, .1, 0.07, 0.06, 0.05],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_citations(\n",
    "    model,\n",
    "    query_text: str,\n",
    "    target_texts: List[str],\n",
    "    target_titles: List[str],\n",
    "    tokenizer,\n",
    "    device=None,\n",
    "    top_k: int = 5,\n",
    "    temperature: float = 0.07,\n",
    "    show_scores: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Matches citations in a query document against a set of target documents.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained citation model\n",
    "        query_text: Source text containing citations to match\n",
    "        target_texts: List of target document texts to match against\n",
    "        target_titles: List of target document titles (for display)\n",
    "        tokenizer: Tokenizer to use for processing texts\n",
    "        device: Device to run inference on\n",
    "        top_k: Number of top matches to return per citation\n",
    "        temperature: Temperature parameter for similarity scaling\n",
    "        show_scores: Whether to show similarity scores in output\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping citation indices to lists of top-k matches\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Process query text\n",
    "    query_encoded = tokenizer(\n",
    "        query_text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Find citation token positions\n",
    "    cite_mask = model.get_citation_masks(query_encoded['input_ids'].to(device))\n",
    "    if not cite_mask.any():\n",
    "        print(\"No citations found in query text.\")\n",
    "        return {}\n",
    "    \n",
    "    # Get citation embeddings\n",
    "    with torch.no_grad():\n",
    "        source_outputs = model.transformer(\n",
    "            input_ids=query_encoded['input_ids'].to(device),\n",
    "            attention_mask=query_encoded['attention_mask'].to(device),\n",
    "            return_dict=True\n",
    "        )\n",
    "    cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "    cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "    \n",
    "    # Get citation text positions\n",
    "    offset_mapping = query_encoded['offset_mapping'][0].numpy()\n",
    "    cite_positions = torch.where(cite_mask[0])[0].cpu().numpy()\n",
    "    citation_spans = [offset_mapping[pos] for pos in cite_positions]\n",
    "    \n",
    "    # Process target texts\n",
    "    all_ref_embeds = []\n",
    "    \n",
    "    # Process targets in batches\n",
    "    batch_size = 32\n",
    "    for i in tqdm.tqdm(range(0, len(target_texts), batch_size), desc=\"Processing targets\"):\n",
    "        batch_texts = target_texts[i:i + batch_size]\n",
    "        \n",
    "        # Encode batch\n",
    "        target_encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            add_special_tokens=False,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get reference embeddings\n",
    "        with torch.no_grad():\n",
    "            target_outputs = model.transformer(\n",
    "                input_ids=target_encoded['input_ids'].to(device),\n",
    "                attention_mask=target_encoded['attention_mask'].to(device),\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        # Get reference token positions and embeddings\n",
    "        ref_mask = model.get_reference_masks(target_encoded['input_ids'].to(device))\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "        \n",
    "        all_ref_embeds.append(ref_embeds.cpu())\n",
    "    \n",
    "    # Concatenate all reference embeddings\n",
    "    ref_embeds = torch.cat(all_ref_embeds).to(device)\n",
    "    \n",
    "    # Compute similarities for all citations\n",
    "    similarities = torch.matmul(cite_embeds, ref_embeds.t()) / temperature\n",
    "    \n",
    "    # Get top-k matches for each citation\n",
    "    top_k_values, top_k_indices = torch.topk(similarities, min(top_k, len(target_texts)), dim=1)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\nCitation Matches:\")\n",
    "    for i, (citation_span, top_values, top_indices) in enumerate(zip(citation_spans, \n",
    "                                                                    top_k_values.cpu().numpy(),\n",
    "                                                                    top_k_indices.cpu().numpy())):\n",
    "        start, end = citation_span\n",
    "        citation_text = query_text[start:end]\n",
    "        print(f\"\\nCitation {i+1}: '{citation_text}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        matches = []\n",
    "        for rank, (score, idx) in enumerate(zip(top_values, top_indices), 1):\n",
    "            title = target_titles[idx]\n",
    "            if show_scores:\n",
    "                print(f\"{rank}. {title} (score: {score:.3f})\")\n",
    "            else:\n",
    "                print(f\"{rank}. {title}\")\n",
    "            matches.append({\n",
    "                'title': title,\n",
    "                'index': idx,\n",
    "                'score': float(score)\n",
    "            })\n",
    "        \n",
    "        results[i] = matches\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_citation_matches(results: Dict, target_texts: List[str], max_preview_len: int = 100):\n",
    "    \"\"\"\n",
    "    Prints detailed information about citation matches including text previews.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from match_citations\n",
    "        target_texts: List of target document texts\n",
    "        max_preview_len: Maximum length of text preview to show\n",
    "    \"\"\"\n",
    "    for citation_idx, matches in results.items():\n",
    "        print(f\"\\nCitation {citation_idx + 1} - Detailed Matches:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for rank, match in enumerate(matches, 1):\n",
    "            title = match['title']\n",
    "            score = match['score']\n",
    "            idx = match['index']\n",
    "            \n",
    "            print(f\"\\n{rank}. {title}\")\n",
    "            print(f\"Score: {score:.3f}\")\n",
    "            \n",
    "            # Show text preview\n",
    "            preview = target_texts[idx][:max_preview_len]\n",
    "            if len(target_texts[idx]) > max_preview_len:\n",
    "                preview += \"...\"\n",
    "            print(f\"Preview: {preview}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "# Example query with citations to match\n",
    "query_text = '''\n",
    "The [[transformer architecture]] has revolutionized NLP. \n",
    "Recent work in <cite>large language models</cite> has shown impressive results.\n",
    "'''\n",
    "\n",
    "# List of target documents and their titles\n",
    "target_texts = [...]\n",
    "target_titles = [...]\n",
    "\n",
    "# Match citations\n",
    "results = match_citations(\n",
    "    model=model,\n",
    "    query_text=query_text,\n",
    "    target_texts=target_texts,\n",
    "    target_titles=target_titles,\n",
    "    tokenizer=tokenizer,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Print detailed matches\n",
    "print_citation_matches(results, target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(799)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([80, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break\n",
    "\n",
    "labels = [torch.where(batch['cited_art_ids']==id)[0][0].item() for id in batch['target_art_ids']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useless =  0.1\n",
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "' ' ' april ' ' ' ( apr. ) is the fourth month of the year in the julian calendar | julian and gregorian calendar s, and comes between march and may. it is one of four months to have 30 <CITE> s. april always begins on the same day of the week as july, and additionally, january in leap years. april always ends on the same day of the week as <CITE>. = = the month = = april comes between march and may, making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as june, <CITE> and november are later in the year. april begins on the same day of the week as july every year and on the same day of the week as january in <CITE> s. april ends on the same day of the week as <CITE> every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in common year s, april starts on the same day of the week as <CITE> of the previous year, and in <CITE> s, may of the previous year. in common years, april finishes on the same day of the week as july of the previous year, and in leap years, february and <CITE> of the previous year. in common years immediately after other common years, april starts on the same day of the week as january of the previous year, and in leap years and years immediately after that, april finishes on the same day of the week as january of the previous year. in years immediately before common years, april starts on the same day of the week as <CITE> and <CITE> of the following year, and in years immediately before leap years, june of the following year. in years immediately before common years, april finishes on the same day of the week as september of the following year, and in years immediately before leap years, march and june of the following year. april [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Source attention mask: 126, \n",
      "Target: id=357:\n",
      "'''September''' (Sep.) is the ninth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[August]] and [[October]]. It has 30 [[day]]s. Its name comes from the [[Latin]] word ''sep...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' september ' ' ' ( sep. ) is the ninth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ august ] ] and [ [ october ] ]. it has 30 [ [ day ] ] s. its name comes from the [ [ latin ] ] word ' ' sept ' ' for \" seven \" ( it was the seventh month of the year, before [ [ january ] ] and [ [ february <REF>\n",
      "\n",
      "\n",
      "Target: id=832:\n",
      "'''day''' is the time it takes the [[Earth]] to spin around once. It is day time on the side of the Earth that is facing the [[Sun]]. When it is [[wikt:night|night]] time, that side of the Earth is fa...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' day ' ' ' is the time it takes the [ [ earth ] ] to spin around once. it is day time on the side of the earth that is facing the [ [ sun ] ]. when it is [ [ wikt : night | night ] ] time, that side of the earth is facing away from the sun. it takes 24 [ [ hour ] ] s for the earth to spin once, so that is one day, including the day time and night time <REF>\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[November]] (of the current year) and [[January]] (of the following year). It has...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' december ' ' ' ( dec. ) is the twelfth and last [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ november ] ] ( of the current year ) and [ [ january ] ] ( of the following year ). it has 31 days. with the name of the month coming from the [ [ latin ] ] ' ' decem ' ' for \" ten \", it was the tenth month of <REF>\n",
      "\n",
      "\n",
      "Target: id=297:\n",
      "'''October''' (Oct.) is the tenth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[September]] and [[November]]. It has 31 [[day]]s. The name comes from the [[Latin]] ''octo''...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' october ' ' ' ( oct. ) is the tenth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ september ] ] and [ [ november ] ]. it has 31 [ [ day ] ] s. the name comes from the [ [ latin ] ] ' ' octo ' ' for \" eight \". it was the eighth month of the year before [ [ january ] ] and [ [ february ] <REF>\n",
      "\n",
      "\n",
      "Target: id=228:\n",
      "'''leap year''' is a [[calendar year]] in which an extra [[day]] is added to the [[Gregorian calendar]], which is used by most of the world. A [[common year]] has 365 [[day]]s, but a leap year has 366...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' leap year ' ' ' is a [ [ calendar year ] ] in which an extra [ [ day ] ] is added to the [ [ gregorian calendar ] ], which is used by most of the world. a [ [ common year ] ] has 365 [ [ day ] ] s, but a leap year has 366 days. the extra day, february 29, is added to the [ [ month ] ] of [ [ february ] ]. in a common year, february <REF>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "useless_chars = np.sum([c==']' for c in source_text])*2/len(source_text)\n",
    "print('useless = ', useless_chars)\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "print(f\"Source attention mask: {(sample['attention_mask']==0).sum()}, \")\n",
    "\n",
    "for i, target_art_id in enumerate(sample['target_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['target_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:200]}...\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
