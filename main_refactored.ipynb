{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 150/237381 [00:00<05:33, 710.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([79, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Iterator, Union, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tqdm \n",
    "import hashlib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch.cuda.amp  # For automatic mixed precision\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if len(collated_data) > 1000:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source\n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "            \n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "                'target_count': len(present_citations),\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    target_art_ids = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_ids = torch.cat([item['target_ids'][:item['target_count']] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'][:item['target_count']] for item in batch])\n",
    "    target_counts = torch.tensor([item['target_count'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'target_counts': target_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration   the citation matching model.\"\"\"\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 100\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    temperature: float = 0.07\n",
    "    device: Optional[torch.device] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # # Load articles\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "# config = ExperimentConfig()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# tokenizer.add_special_tokens({\n",
    "#     'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "# })\n",
    "\n",
    "\n",
    "# results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "\n",
    "# # # # This will now use caching directly \n",
    "# # results = tokenize_sources(cache_path='./cache/tokenized_1caf5def_895012ad817559b15b42e1d366769a67.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Collate the data\n",
    "collated_data = collate(results, tokenizer, config)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CitationDataset(collated_data)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=citation_collate_fn\n",
    ")\n",
    "\n",
    "# Example of resulting tensor shapes for a batch\n",
    "for batch in dataloader:\n",
    "    print(\"Source shape:\", batch['source_ids'].shape)  # [batch_size, source_len]\n",
    "    print(\"Target shape:\", batch['target_ids'].shape)  # [total_targets, target_len]\n",
    "    print(\"Target counts:\", batch['target_counts'])    # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 150/237381 [00:00<05:42, 692.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([16, 512])\n",
      "Target shape: torch.Size([77, 100])\n",
      "Target counts: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237381, 237381, 237381, 1056)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources), len(citation_data), len(results), len(collated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading articles from JSONL file...\n",
      "INFO:root:Loaded 237381 articles.\n"
     ]
    }
   ],
   "source": [
    "sources = tokenizer.batch_decode(batch['source_ids'])\n",
    "# cited_articles = [preprocessor.id2ref[id] for id in batch['cited_art_ids']]\n",
    "# target_articles = [preprocessor.id2ref[id] for id in batch['target_art_ids']]\n",
    "targets = tokenizer.batch_decode(batch['target_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512]), torch.Size([77, 100]), tensor(81), 81, 77)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "\n",
    "batch['source_ids'].shape, batch['target_ids'].shape, (batch['source_ids']==cite_token).sum(), len(batch['cited_art_ids']), len(batch['target_art_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "is a [ [ spring ] ] month in the [ [ northern hemisphere ] ] and an [ [ autumn | autumn / fall ] ] month in the [ [ southern hemisphere ] ]. in each [ [ hemisphere ] ], it is the [ [ season ] ] al equivalent of [ [ october ] ] in the other. it is unclear as to where april got its name. a common theory is that it comes from the [ [ latin ] ] word \" aperire \", meaning \" to open \", referring to [ [ flower ] ] s opening in [ [ spring ] ]. another theory is that the name could come from [ [ aphrodite ] ], the greek goddess of [ [ love ] ]. it was originally the second month in the old roman <CITE>, before the start of the new year was put to [ [ january 1 ] ]. quite a few festivals are held in this month. in many [ [ southeast asia ] ] n cultures, new year is celebrated in this month ( including [ [ songkran ] ] ). in western <CITE>, [ [ easter ] ] can be celebrated on a [ [ sunday ] ] between [ [ march 22 ] ] and <CITE>. in [ [ eastern orthodox church | orthodox ] ] christianity, it can fall between [ [ april 4 ] ] and [ [ may 8 ] ]. at the end of the month, central and northern [ [ europe ] ] an [ [ culture ] ] s celebrate [ [ walpurgis night ] ] on [ [ april 30 ] ], marking the transition from [ [ winter ] ] into [ [ summer ] ]. = = april in poetry = = [ [ poetry | poets ] ] use ' ' april ' ' to mean the end of winter. for example : ' ' april showers bring <CITE> flowers. ' ' = = events in april = = = = = fixed events = = = * [ [ april 1 ] ] - [ [ april fools ' day ] ] * [ [ april 1 ] ] - islamic republic day ( [ [ iran ] ] ) * [ [ april 2 ] ] - international children ' s book day * [ [ april 2 ] ] - [ [ thailand | thai ] ] heritage and [ [ wikt : conservation | conservation ] ] day * [ [ april 2 ] ] - world [ [ autism ] ] awareness day * [ [ april 2 ] ] - malvinas day ( <CITE> ) [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Target: id=2268:\n",
      "'''calendar''' is a tool for organizing days. People use calendars to say when something happened, a\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "2009\n",
      "\n",
      "\n",
      "Target: id=56495:\n",
      "'''Christianity''' is the largest world religion by number of followers (around 2.4 billion). Member\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "\n",
      "\n",
      "\n",
      "Target: id=3189:\n",
      "{{about|the day|the [[North Korea|North Korean]] football team|April 25 SC}}\n",
      "{{calendar}}\n",
      "{{day}}\n",
      "==\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "report\n",
      "\n",
      "\n",
      "Target: id=244:\n",
      "'''May''' is the fifth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian c\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused239]\n",
      "\n",
      "\n",
      "Target: id=31:\n",
      "'''Argentina''' (officially the '''Argentine Republic''')  is a country in [[South America]]. Argent\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "[unused30]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "for i, target_art_id in enumerate(sample['cited_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['cited_art_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:100]}\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:100]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title = april\n",
      "(7734,) (7734,) (7734,)\n",
      "original text =  ' ' ' april ' ' ' ( apr. ) is the fourth [ [ month ] ] of the [ [ year ] ] in the [ [ julian calendar | julian ] ] and [ [ gregorian calendar ] ] s, and comes between [ [ march ] ] and [ [ may ] ]. it is one of four months to have 30 [ [ day ] ] s. april always begins on the same day of the week as [ [ july ] ], and additionally, [ [ january ] ] in leap years. april always ends on the same day of the week as [ [ december ] ]. = = the month = = [ [ file : colorful spring garden. jpg | thumb | 180px | right | [ [ spring ] ] flowers in april in the [ [ northern hemisphere ] ]. ] ] april comes between [ [ march ] ] and [ [ may ] ], making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as [ [ june ] ], [ [ september ] ] and [ [ november ] ] are later in the year. april begins on the same day of the week as [ [ july ] ] every year and on the same day of the week as [ [ january ] ] in [ [ leap year ] ] s. april ends on the same day o\n",
      "##################################################\n",
      "masked text =  ' ' ' april ' ' ' ( apr. ) is the fourth of the in the and s, and comes between and. it is one of four months to have 30 s. april always begins on the same day of the week as, and additionally, in leap years. april always ends on the same day of the week as. = = the month = = flowers in april in the. ] ] april comes between and, making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as, and are later in the year. april begins on the same day of the week as every year and on the same day of the week as in s. april ends on the same day of the week as every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in s, april starts on the same day of the week as of the previous year, and in s, of the previous year. in common years, april finishes on the same day of the week as of the previous year, and in leap years, and of the previous year. in common years immediately after other common years, april starts on t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title = august\n",
      "(4777,) (4777,) (4777,)\n",
      "original text =  ' ' ' august ' ' ' ( aug. ) is the eighth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ july ] ] and [ [ september ] ]. it has 31 [ [ day ] ] s. it was named after the roman emperor [ [ augustus caesar ] ]. august does not begin on the same day of the week as any other month in [ [ common year ] ] s, but begins on the same day of the week as [ [ february ] ] in [ [ leap year ] ] s. august always ends on the same day of the week as [ [ november ] ]. = = the month = = [ [ file : hw - augustus. jpg | thumb | 120px | right | roman emperor [ [ augustus caesar ] ], after whom august is named ] ] this month was first called ' ' sextilis ' ' in [ [ latin ] ], because it was the sixth month in the old [ [ roman calendar ] ]. the roman calendar began in march about 735 & nbsp ; bc with [ [ romulus and remus | romulus ] ]. [ [ october ] ] was the eighth month. august was the eighth month when january or february were added to the start of the year by king\n",
      "##################################################\n",
      "masked text =  ' ' ' august ' ' ' ( aug. ) is the eighth of the in the, coming between and. it has 31 s. it was named after the roman emperor [ [ augustus caesar ] ]. august does not begin on the same day of the week as any other month in s, but begins on the same day of the week as in s. august always ends on the same day of the week as. = = the month = =, after whom august is named ] ] this month was first called ' ' sextilis ' ' in, because it was the sixth month in the old. the roman calendar began in march about 735 & nbsp ; bc with. was the eighth month. august was the eighth month when january or february were added to the start of the year by king about 700 & nbsp ; bc. or, when those two months were moved from the end to the beginning of the year by the decemvirs about 450 & nbsp ; bc ( roman writers disagree ). in [ [ 153 bc ] ] was determined as the beginning of the year. august is named for [ [ augustus caesar ] ] who became in this month. < ref name = \" infoplease \" > { { citation | titl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title = art\n",
      "(2137,) (2137,) (2137,)\n",
      "original text =  ' ' ' art ' ' ' is a creative activity and technical [ [ skill ] ]. it produces a product, an object. ' ' ' art ' ' ' is a diverse range of human activities in creating visual, performing subjects, and expressing the author ' s thoughts. the product of art is called a ' ' ' work of art ' ' ', for others to experience. < ref > various definitions in : wilson, simon & lack, jennifer 2008. ' ' the tate guide to modern art terms ' '. tate publishing. isbn 978 - 1 - 85437 - 750 - 0 < / ref > < ref > e. h. gombrich 1995. ' ' the story of art ' '. london : phaidon. isbn 978 - 0714832470 < / ref > < ref > kleiner, gardner, mamiya and tansey. 2004. ' ' art through the ages ' '. 12th ed. 2 volumes, wadsworth. isbn 0 - 534 - 64095 - 8 ( vol 1 ) and isbn 0 - 534 - 64091 - 5 ( vol 2 ) < / ref > some art is useful in a practical sense, such as a sculptured clay [ [ bowl ] ] that can be used. that kind of art is sometimes called a ' ' [ [ craft ] ] ' '. those who make art are called [ [ artist ] ] s.\n",
      "##################################################\n",
      "masked text =  ' ' ' art ' ' ' is a creative activity and technical. it produces a product, an object. ' ' ' art ' ' ' is a diverse range of human activities in creating visual, performing subjects, and expressing the author ' s thoughts. the product of art is called a ' ' ' work of art ' ' ', for others to experience. < ref > various definitions in : wilson, simon & lack, jennifer 2008. ' ' the tate guide to modern art terms ' '. tate publishing. isbn 978 - 1 - 85437 - 750 - 0 < / ref > < ref > e. h. gombrich 1995. ' ' the story of art ' '. london : phaidon. isbn 978 - 0714832470 < / ref > < ref > kleiner, gardner, mamiya and tansey. 2004. ' ' art through the ages ' '. 12th ed. 2 volumes, wadsworth. isbn 0 - 534 - 64095 - 8 ( vol 1 ) and isbn 0 - 534 - 64091 - 5 ( vol 2 ) < / ref > some art is useful in a practical sense, such as a sculptured clay that can be used. that kind of art is sometimes called a ' ' ' '. those who make art are called s. they hope to affect the s of people who experience it. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title = a\n",
      "(979,) (979,) (979,)\n",
      "original text =  ' ' ' a ' ' ' or ' ' ' a ' ' ' is the first letter of the [ [ english language | english ] ] [ [ alphabet ] ]. the small letter, ' ' ' a ' ' ' or ' ' ' α ' ' ', is used as a lower case [ [ vowel ] ]. < ref > { { cite book | url = https : / / www. worldcat. org / oclc / 17648714 | title = the oxford english dictionary | date = 1989 | publisher = clarendon press | isbn = 0 - 19 - 861186 - 2 | edition = 2nd | location = oxford | oclc = 17648714 } } < / ref > when it is spoken, a is said as a long ' ' ' a ' ' ', a [ [ diphthong ] ] of e and y. a is similar to alphabet of the [ [ greek alphabet ] ]. that is not surprising, because it means the same sound. \" alpha and [ [ omega ( letter ) | omega ] ] \" ( the last letter of the greek alphabet ) means from beginning to the end. in [ [ musical notation ] ], the letter a is the symbol of a note in the scale, below [ [ b ] ] and above [ [ g ] ]. a is the letter that was used to represent a team in an old [ [ tv ] ] show, [ [ the a - team ] ]. a c\n",
      "##################################################\n",
      "masked text =  ' ' ' a ' ' ' or ' ' ' a ' ' ' is the first letter of the. the small letter, ' ' ' a ' ' ' or ' ' ' α ' ' ', is used as a lower case. < ref > { { cite book | url = https : / / www. worldcat. org / oclc / 17648714 | title = the oxford english dictionary | date = 1989 | publisher = clarendon press | isbn = 0 - 19 - 861186 - 2 | edition = 2nd | location = oxford | oclc = 17648714 } } < / ref > when it is spoken, a is said as a long ' ' ' a ' ' ', a of e and y. a is similar to alphabet of the. that is not surprising, because it means the same sound. \" alpha and \" ( the last letter of the greek alphabet ) means from beginning to the end. in, the letter a is the symbol of a note in the scale, below and above. a is the letter that was used to represent a team in an old [ [ tv ] ] show,. a capital a is written \" a \". use a capital a at the start of a sentence if writing. a is also a musical note, sometimes referred to as \" la \". = = where it came from = = the letter ' a ' was in the phoenician\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "title = air\n",
      "(1249,) (1249,) (1249,)\n",
      "original text =  ' ' ' air ' ' ' is the [ [ earth ] ] ' s [ [ atmosphere ] ]. air is a [ [ mixture ] ] of many gases and tiny dust particles. it is the clear [ [ gas ] ] in which living things live and [ [ breathe ] ]. it has an indefinite shape and [ [ volume ] ]. it has [ [ mass ] ] and [ [ weight ] ], because it is [ [ matter ] ]. the weight of air creates [ [ atmosphere pressure | atmospheric pressure ] ]. there is no air in [ [ outer space ] ]. atmosphere is a [ [ mixture ] ] of about 78 % [ [ nitrogen ] ], 21 % of [ [ oxygen ] ], and 1 % other gases, such as carbon dioxide. animals live and need to breathe the oxygen in the atmosphere. in [ [ breathing ] ], the [ [ lung ] ] s put oxygen into the [ [ blood ] ], and send back [ [ carbon dioxide ] ] to the air. plants need the carbon dioxide in the air to live. they give off the oxygen that we breathe. without it we die of [ [ asphyxia ] ]. air can be [ [ air pollution | polluted ] ] by some gases ( such as [ [ carbon monoxide ] ], hydrocarbons, and\n",
      "##################################################\n",
      "masked text =  ' ' ' air ' ' ' is the ' s [ [ atmosphere ] ]. air is a of many gases and tiny dust particles. it is the clear in which living things live and [ [ breathe ] ]. it has an indefinite shape and. it has and, because it is. the weight of air creates. there is no air in. atmosphere is a of about 78 %, 21 % of, and 1 % other gases, such as carbon dioxide. animals live and need to breathe the oxygen in the atmosphere. in, the s put oxygen into the, and send back to the air. plants need the carbon dioxide in the air to live. they give off the oxygen that we breathe. without it we die of. air can be by some gases ( such as, hydrocarbons, and nitrogen oxides ),, and ash. this causes various problems including, and. it can damage people ' s and the environment. there are debates about whether or not to act upon climate change, but soon enough the earth will heat up to much, causing our home to become too hot and not support life! some say fewer people would die of cold weather, and that is true bu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_tokenize(batch_sources, batch_citations):\n",
    "    # batch_sources = sources[batch_start:batch_end]\n",
    "    # batch_citations = citation_data[batch_start:batch_end]\n",
    "\n",
    "    # Ba tch encode\n",
    "    batch_encoded = tokenizer.batch_encode_plus(\n",
    "        batch_sources,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    all_results = []\n",
    "    # Process each item in the batch\n",
    "    for idx in range(len(batch_sources)):\n",
    "        offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "        input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "        \n",
    "        # Create offset to index mapping\n",
    "        off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "        off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "        \n",
    "        # Create citation tokens array\n",
    "        mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "        cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "        \n",
    "        # Fill in citations\n",
    "        for i, j, art_id in batch_citations[idx]:\n",
    "            s, e = off2i[i], off2i[j]\n",
    "            cite_tokens[s] = art_id\n",
    "            mask_tokens[s:e] = art_id\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'input_ids': np.array(input_ids),\n",
    "            'cite_tokens': cite_tokens,\n",
    "            'mask_tokens': mask_tokens,\n",
    "            'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "        })\n",
    "    return all_results\n",
    "\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "for i in range(5):\n",
    "    ref = preprocessor.id2ref[i+1]\n",
    "    print(f\"title = {ref}\")\n",
    "    result = batch_tokenize(sources[i:i+1], citation_data[i:i+1])[0]\n",
    "    print(result['input_ids'].shape, result['cite_tokens'].shape, result['mask_tokens'].shape)\n",
    "    input_ids = result['input_ids']\n",
    "    cite_tokens = result['cite_tokens']\n",
    "    mask_tokens = result['mask_tokens']\n",
    "    \n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print('original text = ', decoded_text[:1000])\n",
    "    input_ids = input_ids[mask_tokens==0]\n",
    "    print('#'*50)\n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print('masked text = ', decoded_text[:1000])\n",
    "    print('\\n'*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      " [[Category:something something]]'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "[[File:Colorful spring garden.jpg|thumb|180px|right|[[Spring]] flowers in April in the [[Northern Hemisphere]].]]\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 3\n",
      "\n",
      "\n",
      "Cleaned text: \n",
      " '''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(33, 42, 'month'),\n",
       " (50, 58, 'year'),\n",
       " (66, 92, 'julian calendar'),\n",
       " (97, 119, 'gregorian calendar'),\n",
       " (140, 149, 'march'),\n",
       " (154, 161, 'may'),\n",
       " (199, 206, 'day'),\n",
       " (260, 268, 'july'),\n",
       " (288, 299, 'january'),\n",
       " (364, 376, 'december'),\n",
       " (414, 423, 'march'),\n",
       " (428, 435, 'may'),\n",
       " (554, 562, 'june'),\n",
       " (564, 577, 'september'),\n",
       " (582, 594, 'november'),\n",
       " (662, 670, 'july'),\n",
       " (717, 728, 'january'),\n",
       " (732, 745, 'leap year'),\n",
       " (790, 802, 'december'),\n",
       " (883, 898, 'common year'),\n",
       " (945, 956, 'october'),\n",
       " (986, 999, 'leap year')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def find_citations(text, articles_dict):\n",
    "    citations = []\n",
    "    for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "        match_text = match.group(1)\n",
    "        citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "        citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "        ref = None\n",
    "        for cit in citation:\n",
    "            if cit.lower() in articles_dict:\n",
    "                ref = cit.lower()\n",
    "                break\n",
    "        if ref:\n",
    "            citations.append((match.start(), match.end(), ref))\n",
    "    return citations\n",
    "\n",
    "\n",
    "def clean_wiki_text(text: str) -> str:\n",
    "    \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "    # Find main content starting from first bold title\n",
    "    match = re.search(r\"'''([^']+?)'''\", text)\n",
    "    if match:\n",
    "        text = text[match.start():]\n",
    "\n",
    "    # Remove wiki elements and clean up\n",
    "    text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "    # text = re.sub(r'\\[\\[Category:.*\\]\\]', '', text)\n",
    "    # text = re.sub(r'\\[\\[File:.*\\]\\]', '', text)\n",
    "    # text = re.sub(r'\\[\\[Category:.*\\]\\]', '', text)\n",
    "    # text = re.sub(r'\\{\\{stub.*\\}\\}', '', text)\n",
    "    return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "text = preprocessor.articles_dict['april'] \n",
    "text = \"[[Category:something something]]\" + text\n",
    "print(f\"Original text: \\n {text[:1000]}\\n\\n\")\n",
    "text =clean_wiki_text(text)\n",
    "\n",
    "print(f\"Cleaned text: \\n {text[:1000]}\\n\\n\")\n",
    "\n",
    "find_citations(text[:1000], preprocessor.articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[[File:Colorful spring garden.jpg|thumb|180px|right|[[Spring]] flowers in April in the [[Northern Hemisphere]].]]',\n",
       " \"[[File:Aprilsnar 2001.png|thumb|200px|right|An [[April Fools' Day]] hoax for [[April 1]] in [[Copenhagen]].]]\",\n",
       " '[[File:Songkran in Wat Kungthapao 03.jpg|thumb|180px|right|[[Songkran]] celebration in [[Thailand]] around [[April 14]].]]',\n",
       " '[[File:Earth flag PD.jpg|thumb|200px|right|Proposed [[flag]] for [[Earth Day]] on [[April 22]].]]',\n",
       " \"[[File:St George's Day 2010 - 14.jpg|thumb|200px|right|[[Saint George]]'s Day on [[April 23]] in [[London]]'s [[Trafalgar Square]].]]\",\n",
       " '[[File:Anzac1.JPG|thumb|180px|right|[[ANZAC Day]] commemoration in [[Australia]] on [[April 25]].]]',\n",
       " \"[[File:Koninginnedag2007.jpg|thumb|180px|right|Queen's Day, [[April 30]], celebration in the [[Netherlands]]. It changed to King's Day, [[April 27]], in [[2014]].]]\",\n",
       " '[[File:Valborgsbrasa-1.jpg|thumb|210px|right|[[Walpurgis Night]] bonfire on [[April 30]] in [[Sweden]].]]',\n",
       " '[[File:Vajicka1.jpg|thumb|200px|right|Eggs celebrating [[Easter]], which often falls in April, but sometimes falls in [[March]].]]',\n",
       " '[[File:Aprilregen - Lithografie.jpg|thumb|200px|right|Image traditionally showing it as [[rain]]ing in April in the [[Northern Hemisphere]].]]',\n",
       " '[[File:Nunavut-Feierlichkeit (01-04-99).jpg|thumb|180px|right|[[Inauguration]] celebration for [[Nunavut]] on [[April 1]], [[1999]].]]',\n",
       " '[[File:Moai Rano raraku.jpg|thumb|160px|right|A statue on [[Easter Island]] - Jacob Roggeveen became the first [[Europe]]an to land there on [[April 5]], [[1722]].]]',\n",
       " '[[File:Titanic-New York Herald front page.jpeg|thumb|150px|right|[[Newspaper]] report on the sinking of the [[RMS Titanic]] on [[April 15]], [[1912]].]]',\n",
       " '[[File:San Francisco Fire Sacramento Street 1906-04-18.jpg|thumb|180px|right|[[Fire]]s after the [[San Francisco]] [[earthquake]] on [[April 18]], [[1906]].]]',\n",
       " '[[File:Anzac Beach 4th Bn landing 8am April 25 1915.jpg|thumb|180px|right|[[Australia]]n and [[New Zealand]] forces landing at Anzac Cove, [[April 25]], [[1915]].]]',\n",
       " '[[File:HMS Bounty.jpg|thumb|200px|right|Painting showing the Mutiny on the Bounty on [[April 28]], [[1789]].]]',\n",
       " '[[File:Juliana 1959.jpg|thumb|150px|right|Queen [[Juliana of the Netherlands]], who abdicated the throne on her 71st [[birthday]], [[April 30]], [[1980]].]]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\[\\[File:.*\\]\\]'\n",
    "text = preprocessor.articles_dict['april'] \n",
    "matches = re.finditer(pattern, text, re.X)\n",
    "[m.group() for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
