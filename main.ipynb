{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading cached tokenized results from ./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirjoudaki\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/paperGPT/wandb/run-20241125_152151-r2zmgw0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amirjoudaki/citation-matching/runs/r2zmgw0x' target=\"_blank\">stilted-flower-9</a></strong> to <a href='https://wandb.ai/amirjoudaki/citation-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amirjoudaki/citation-matching' target=\"_blank\">https://wandb.ai/amirjoudaki/citation-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amirjoudaki/citation-matching/runs/r2zmgw0x' target=\"_blank\">https://wandb.ai/amirjoudaki/citation-matching/runs/r2zmgw0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment config: ExperimentConfig(model_name='bert-base-uncased', vocab_size=30524, initial_logit_scale=0.6931471805599453, cite_token='<CITE>', ref_token='<REF>', cite_token_id=30522, ref_token_id=30523, max_length=512, source_len=512, target_len=128, max_targets=5, overlap=0.5, num_epochs=15, learning_rate=0.00015, Adam_eps=1e-08, weight_decay=0.01, warmup_steps=0, batch_size=200, train_ratio=0.8, collate_sample_size=5000, k_values=[1, 5, 10, 50, 100, 1000], project_name='citation-matching', run_name=None, save_path='./experiments/best_citation_model.pt', cache_dir='cache', device=device(type='cuda'))\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2302454/340569076.py:785: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "Current logit scale: 2.0000\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<07:26, 529.68it/s]\n",
      "Training:   0%|                                                                                                                        | 0/21 [00:00<?, ?it/s]/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:10<00:00,  3.37s/it, loss=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 5.5125\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.09s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 33.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 6.5471\n",
      "  Accuracy (top-1): 0.0552\n",
      "  Mean Reciprocal Rank: 0.1160\n",
      "  Number of citations: 5888\n",
      "  Number of unique targets: 2498\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.0552\n",
      "  k=5: 0.1632\n",
      "  k=10: 0.2351\n",
      "  k=50: 0.4484\n",
      "  k=100: 0.6223\n",
      "  k=1000: 0.9614\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 6.5471\n",
      "  Accuracy (top-1): 0.0552\n",
      "   MRR: 0.1160\n",
      "\n",
      "Epoch 2/15\n",
      "Current logit scale: 2.7514\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<07:52, 500.59it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:11<00:00,  3.39s/it, loss=1.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 4.7028\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.11s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 33.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 5.8334\n",
      "  Accuracy (top-1): 0.0727\n",
      "  Mean Reciprocal Rank: 0.1456\n",
      "  Number of citations: 5903\n",
      "  Number of unique targets: 2455\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.0727\n",
      "  k=5: 0.2092\n",
      "  k=10: 0.2909\n",
      "  k=50: 0.5296\n",
      "  k=100: 0.7484\n",
      "  k=1000: 0.9787\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 5.8334\n",
      "  Accuracy (top-1): 0.0727\n",
      "   MRR: 0.1456\n",
      "\n",
      "Epoch 3/15\n",
      "Current logit scale: 3.9575\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:02<13:37, 289.70it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:11<00:00,  3.39s/it, loss=1.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 3.8944\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.11s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 5.2661\n",
      "  Accuracy (top-1): 0.0901\n",
      "  Mean Reciprocal Rank: 0.1672\n",
      "  Number of citations: 5958\n",
      "  Number of unique targets: 2470\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.0901\n",
      "  k=5: 0.2333\n",
      "  k=10: 0.3145\n",
      "  k=50: 0.5606\n",
      "  k=100: 0.8157\n",
      "  k=1000: 0.9844\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 5.2661\n",
      "  Accuracy (top-1): 0.0901\n",
      "   MRR: 0.1672\n",
      "\n",
      "Epoch 4/15\n",
      "Current logit scale: 5.7838\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<08:04, 488.81it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:10<00:00,  3.37s/it, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 3.0907\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.12s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 4.7646\n",
      "  Accuracy (top-1): 0.1147\n",
      "  Mean Reciprocal Rank: 0.2019\n",
      "  Number of citations: 5928\n",
      "  Number of unique targets: 2487\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.1147\n",
      "  k=5: 0.2807\n",
      "  k=10: 0.3731\n",
      "  k=50: 0.6253\n",
      "  k=100: 0.7952\n",
      "  k=1000: 0.9921\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 4.7646\n",
      "  Accuracy (top-1): 0.1147\n",
      "   MRR: 0.2019\n",
      "\n",
      "Epoch 5/15\n",
      "Current logit scale: 8.2439\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<08:05, 487.95it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:10<00:00,  3.38s/it, loss=0.821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 2.4251\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.16s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 38.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 4.5541\n",
      "  Accuracy (top-1): 0.1305\n",
      "  Mean Reciprocal Rank: 0.2211\n",
      "  Number of citations: 5907\n",
      "  Number of unique targets: 2470\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.1305\n",
      "  k=5: 0.3066\n",
      "  k=10: 0.3990\n",
      "  k=50: 0.6326\n",
      "  k=100: 0.8354\n",
      "  k=1000: 0.9927\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 4.5541\n",
      "  Accuracy (top-1): 0.1305\n",
      "   MRR: 0.2211\n",
      "\n",
      "Epoch 6/15\n",
      "Current logit scale: 10.9001\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<07:51, 502.10it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:10<00:00,  3.37s/it, loss=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.9685\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.02s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 34.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 4.3011\n",
      "  Accuracy (top-1): 0.1448\n",
      "  Mean Reciprocal Rank: 0.2430\n",
      "  Number of citations: 5884\n",
      "  Number of unique targets: 2493\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.1448\n",
      "  k=5: 0.3351\n",
      "  k=10: 0.4368\n",
      "  k=50: 0.6835\n",
      "  k=100: 0.8666\n",
      "  k=1000: 0.9932\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 4.3011\n",
      "  Accuracy (top-1): 0.1448\n",
      "   MRR: 0.2430\n",
      "\n",
      "Epoch 7/15\n",
      "Current logit scale: 13.1487\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<08:06, 486.11it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:10<00:00,  3.37s/it, loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.6358\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.04s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 4.4709\n",
      "  Accuracy (top-1): 0.1571\n",
      "  Mean Reciprocal Rank: 0.2517\n",
      "  Number of citations: 5939\n",
      "  Number of unique targets: 2472\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.1571\n",
      "  k=5: 0.3474\n",
      "  k=10: 0.4395\n",
      "  k=50: 0.6488\n",
      "  k=100: 0.7786\n",
      "  k=1000: 0.9929\n",
      "\n",
      "Epoch 8/15\n",
      "Current logit scale: 15.1575\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                  | 664/237381 [00:01<08:24, 469.48it/s]\n",
      "Training:  14%|██████████████▍                                                                                      | 3/21 [00:11<01:06,  3.68s/it, loss=1.48]"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import bz2\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import tqdm \n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class WikiArticle:\n",
    "    \"\"\"Represents a Wikipedia article with its metadata.\"\"\"\n",
    "    title: str\n",
    "    text: str\n",
    "    timestamp: str\n",
    "    is_redirect: bool\n",
    "\n",
    "class WikiDumpProcessor:\n",
    "    \"\"\"Processes Wikipedia XML dumps and extracts articles.\"\"\"\n",
    "    \n",
    "    def __init__(self, dump_path: str):\n",
    "        self.dump_path = dump_path\n",
    "        self._ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "        self._skip_prefixes = {\n",
    "            'Wikipedia:', 'Template:', 'Category:', 'Portal:', 'File:', \n",
    "            'MediaWiki:', 'Help:', 'Book:', 'Draft:', 'TimedText:', \n",
    "            'Module:', 'Special:'\n",
    "        }\n",
    "\n",
    "    def iter_articles(self, skip_redirects: bool = True) -> Iterator[WikiArticle]:\n",
    "        \"\"\"Iterates through valid articles in the dump.\"\"\"\n",
    "        dump_file = bz2.BZ2File(self.dump_path) if self.dump_path.endswith('.bz2') else open(self.dump_path, 'rb')\n",
    "        \n",
    "        for _, elem in ET.iterparse(dump_file, events=('end',)):\n",
    "            if not elem.tag.endswith('page'):\n",
    "                continue\n",
    "\n",
    "            # Extract basic article data\n",
    "            title = elem.find('.//mw:title', self._ns).text\n",
    "            if any(title.startswith(prefix) for prefix in self._skip_prefixes):\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            # Get revision data\n",
    "            rev = elem.find('.//mw:revision', self._ns)\n",
    "            text = rev.find('mw:text', self._ns).text if rev is not None else ''\n",
    "            timestamp = rev.find('mw:timestamp', self._ns).text if rev is not None else ''\n",
    "            is_redirect = bool(re.match(r'#REDIRECT', text or '', re.IGNORECASE))\n",
    "\n",
    "            if skip_redirects and is_redirect:\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            yield WikiArticle(title=title, text=text, timestamp=timestamp, is_redirect=is_redirect)\n",
    "            elem.clear()\n",
    "\n",
    "class ArticleStorage:\n",
    "    \"\"\"Handles storage and retrieval of Wikipedia articles.\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: WikiDumpProcessor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def save_to_jsonl(self, output_path: Union[str, Path], sample_size: Optional[int] = None) -> int:\n",
    "        \"\"\"Saves articles to a JSONL file.\"\"\"\n",
    "        count = 0\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for i, article in enumerate(self.processor.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                json.dump(article.__dict__, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def save_to_sqlite(self, db_path: Union[str, Path], sample_size: Optional[int] = None,\n",
    "                      batch_size: int = 1000) -> int:\n",
    "        \"\"\"Saves articles to a SQLite database.\"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                    (title TEXT PRIMARY KEY, text TEXT, timestamp TEXT, is_redirect INTEGER)''')\n",
    "        c.execute('CREATE INDEX IF NOT EXISTS idx_title ON articles(title)')\n",
    "        \n",
    "        count = 0\n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for i, article in enumerate(self.processor.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                batch.append((article.title, article.text, article.timestamp, \n",
    "                            1 if article.is_redirect else 0))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    c.executemany('INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)', batch)\n",
    "                    conn.commit()\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "            \n",
    "            if batch:\n",
    "                c.executemany('INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)', batch)\n",
    "                conn.commit()\n",
    "                count += len(batch)\n",
    "                \n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "        return count\n",
    "\n",
    "\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "\n",
    "# experiment related \n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Unified configuration for the entire citation matching experiment.\"\"\"\n",
    "    # Model configuration\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    vocab_size: Optional[int] = None  # Will be set after tokenizer initialization\n",
    "    initial_logit_scale: float = np.log(1/0.07)  # Same initial value as CLIP\n",
    "    \n",
    "    # Token configuration\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    cite_token_id: Optional[int] = None  # Will be set after tokenizer initialization\n",
    "    ref_token_id: Optional[int] = None   # Will be set after tokenizer initialization\n",
    "    \n",
    "    # Text processing configuration\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 128\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    \n",
    "    # Training configuration\n",
    "    num_epochs: int = 15\n",
    "    learning_rate: float = 1.5e-4\n",
    "    logits_learning_rate: float = 1.5e-2,\n",
    "    Adam_eps: float = 1e-8\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 0\n",
    "    batch_size: int = 200\n",
    "    train_ratio: float = 0.8\n",
    "    collate_sample_size: Optional[int] = None\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    k_values: List[int] = field(default_factory=lambda: [1, 5, 10, 50, 100, 1000])\n",
    "    \n",
    "    # Logging and saving configuration\n",
    "    project_name: str = \"citation-matching\"\n",
    "    run_name: Optional[str] = None\n",
    "    save_path: str = \"./experiments/best_citation_model.pt\"\n",
    "    cache_dir: str = \"cache\"\n",
    "    \n",
    "    # Hardware configuration\n",
    "    device: Optional[torch.device] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    bracket_tokens = tokenizer.convert_tokens_to_ids(['[',']'])\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if config.collate_sample_size and len(collated_data)>config.collate_sample_size:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source: \n",
    "            # only keep citation tokens that are sampled to be masked \n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            # don't mask citations that are not sampled \n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            # remove brackets from the rest of the text \n",
    "            mask_tokens = np.where(np.isin(input_ids,bracket_tokens),1, mask_tokens)\n",
    "            # don't mask the citation tokens \n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            # set the citation tokens (first token of a citation range) as special token <CITE> \n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            # mask all tokens in a citation, except for the first (special) token \n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "\n",
    "            # keep the cited article ids in the text in the order they appear (with repeats)\n",
    "            # & keep the unique cited artile ids \n",
    "            # this will enable us to link each special cite token to a target via the article id\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "\n",
    "\n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_art_ids_all = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    target_ids = torch.cat([item['target_ids'] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'] for item in batch])\n",
    "\n",
    "    # Get unique indices and inverse indices\n",
    "    target_art_ids, unique_indices = np.unique(target_art_ids_all.numpy(), return_index=True)\n",
    "    target_art_ids = torch.tensor(target_art_ids)\n",
    "    unique_indices = torch.tensor(unique_indices)\n",
    "    \n",
    "    # Use unique indices to get corresponding targets\n",
    "    target_ids = target_ids[unique_indices]\n",
    "    target_attention_mask = target_attention_mask[unique_indices]\n",
    "\n",
    "    id2i = {id.item():i for i,id in enumerate(target_art_ids)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids],dtype=torch.long)\n",
    "\n",
    "      \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CitationModelOutput:\n",
    "    \"\"\"Custom output class for the citation model.\"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    cite_embeds: Optional[torch.FloatTensor] = None\n",
    "    ref_embeds: Optional[torch.FloatTensor] = None\n",
    "\n",
    "class CitationModel(nn.Module):\n",
    "    \"\"\"Custom model for citation matching using transformer embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base model configuration\n",
    "        base_config = AutoConfig.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # Load base transformer model\n",
    "        self.transformer = AutoModel.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Resize token embeddings if needed\n",
    "        if config.vocab_size != self.transformer.config.vocab_size:\n",
    "            self.transformer.resize_token_embeddings(config.vocab_size)\n",
    "\n",
    "        # Add learnable logit scale parameter\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * config.initial_logit_scale)\n",
    "\n",
    "    \n",
    "    def get_citation_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for citation token positions.\"\"\"\n",
    "        return input_ids == self.config.cite_token_id\n",
    "    \n",
    "    def get_reference_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for reference token positions.\"\"\"\n",
    "        return input_ids == self.config.ref_token_id\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        source_ids: torch.Tensor,\n",
    "        target_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        target_attention_mask: Optional[torch.Tensor] = None,\n",
    "        cited_art_ids: Optional[torch.Tensor] = None,\n",
    "        target_art_ids: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[Tuple, CitationModelOutput]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        \n",
    "        # Process source text\n",
    "        source_outputs = self.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Process target text\n",
    "        target_outputs = self.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get citation mask and extract citation embeddings\n",
    "        cite_mask = self.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        # Get reference mask and extract reference embeddings\n",
    "        ref_mask = self.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "        \n",
    "        # Clamp logit scale to prevent numerical instability\n",
    "        logit_scale = torch.clamp(self.logit_scale, 0, torch.log(torch.tensor(100.0)))\n",
    "        \n",
    "        # Compute similarity scores with learned scale\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) * logit_scale.exp()\n",
    "\n",
    "        # compute the loss \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        if return_dict:\n",
    "            return CitationModelOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                cite_embeds=cite_embeds,\n",
    "                ref_embeds=ref_embeds\n",
    "            )\n",
    "        \n",
    "        return (loss, logits, cite_embeds, ref_embeds)\n",
    "\n",
    "\n",
    "def compute_retrieval_metrics(logits, labels, ks=[1, 5, 10, 50, 100, 1000]):\n",
    "    # Get rankings of correct targets\n",
    "    correct_scores = logits[torch.arange(logits.size(0)), labels]\n",
    "    rankings = (logits >= correct_scores.unsqueeze(1)).sum(1)\n",
    "    \n",
    "    # Compute MRR\n",
    "    mrr = (1.0 / rankings).mean().item()\n",
    "    \n",
    "    # Compute top-k accuracy for different k values\n",
    "    metrics = {'mrr': mrr}\n",
    "    for k in ks:\n",
    "        if k <= logits.size(1):  # Only compute if k is not larger than number of targets\n",
    "            top_k_acc = (rankings <= k).float().mean().item()\n",
    "            metrics[f'top_{k}_accuracy'] = top_k_acc\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate_citation_model(\n",
    "    model,\n",
    "    val_dataloader,\n",
    "    device: str = None,\n",
    "    return_embeddings: bool = False,\n",
    "    k_values: List[int] = [1, 5, 10, 50, 100, 1000],\n",
    "    similarity_batch_size: int = 4096\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store accumulated embeddings and IDs\n",
    "    all_cite_embeds = []\n",
    "    all_ref_embeds = []\n",
    "    all_cited_art_ids = []\n",
    "    all_target_art_ids = []\n",
    "    \n",
    "    # Accumulate embeddings and IDs\n",
    "    with torch.no_grad():\n",
    "        # Get and clamp logit scale\n",
    "        for batch in tqdm.tqdm(val_dataloader, desc=\"Computing embeddings\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Process source text\n",
    "            source_outputs = model.transformer(\n",
    "                input_ids=batch['source_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Process target text\n",
    "            target_outputs = model.transformer(\n",
    "                input_ids=batch['target_ids'],\n",
    "                attention_mask=batch['target_attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Get citation mask and extract citation embeddings\n",
    "            cite_mask = model.get_citation_masks(batch['source_ids'])\n",
    "            cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "            \n",
    "            # Get reference mask and extract reference embeddings\n",
    "            ref_mask = model.get_reference_masks(batch['target_ids'])\n",
    "            ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "            ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "            \n",
    "            # Store embeddings and IDs\n",
    "            all_cite_embeds.append(cite_embeds.cpu())\n",
    "            all_ref_embeds.append(ref_embeds.cpu())\n",
    "            all_cited_art_ids.append(batch['cited_art_ids'].cpu())\n",
    "            all_target_art_ids.append(batch['target_art_ids'].cpu())\n",
    "    \n",
    "    # Concatenate all accumulated tensors\n",
    "    cite_embeds = torch.cat(all_cite_embeds)\n",
    "    ref_embeds = torch.cat(all_ref_embeds)\n",
    "    cited_art_ids = torch.cat(all_cited_art_ids)\n",
    "    target_art_ids = torch.cat(all_target_art_ids)\n",
    "    \n",
    "    # Get unique target art IDs and create mapping\n",
    "    target_art_ids_unique, unique_indices = np.unique(target_art_ids.numpy(), return_index=True)\n",
    "    target_art_ids_unique = torch.tensor(target_art_ids_unique)\n",
    "    ref_embeds_unique = ref_embeds[torch.tensor(unique_indices)]\n",
    "    \n",
    "    # Create ID to index mapping\n",
    "    id2i = {id.item(): i for i, id in enumerate(target_art_ids_unique)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids], dtype=torch.long)\n",
    "    \n",
    "    # Move reference embeddings to device once\n",
    "    ref_embeds_unique = ref_embeds_unique.to(device)\n",
    "    \n",
    "    # Initialize arrays to store results\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    all_predictions = []\n",
    "    all_logits_list = []\n",
    "    all_labels_list = []\n",
    "    \n",
    "    # Process citation embeddings in batches\n",
    "    num_batches = (len(cite_embeds) + similarity_batch_size - 1) // similarity_batch_size\n",
    "\n",
    "    # Get and clamp logit scale\n",
    "    logit_scale = torch.clamp(model.logit_scale, 0, torch.log(torch.tensor(100.0)))\n",
    "    \n",
    "    for i in tqdm.tqdm(range(num_batches), desc=\"Computing similarities\"):\n",
    "        start_idx = i * similarity_batch_size\n",
    "        end_idx = min((i + 1) * similarity_batch_size, len(cite_embeds))\n",
    "        \n",
    "        # Move batch of citation embeddings to device\n",
    "        cite_embeds_batch = cite_embeds[start_idx:end_idx].to(device)\n",
    "        labels_batch = labels[start_idx:end_idx].to(device)\n",
    "        \n",
    "        # Compute similarity scores for this batch\n",
    "        logits_batch = torch.matmul(cite_embeds_batch, ref_embeds_unique.t()) * logit_scale.exp()\n",
    "        \n",
    "        # Compute loss for this batch\n",
    "        loss_batch = F.cross_entropy(logits_batch, labels_batch)\n",
    "        total_loss += loss_batch.item() * len(labels_batch)\n",
    "        \n",
    "        # Compute predictions and accuracy for this batch\n",
    "        predictions_batch = torch.argmax(logits_batch, dim=-1)\n",
    "        total_correct += (predictions_batch == labels_batch).sum().item()\n",
    "        \n",
    "        # Store predictions and logits for later computation\n",
    "        all_predictions.append(predictions_batch.cpu())\n",
    "        all_logits_list.append(logits_batch.cpu())\n",
    "        all_labels_list.append(labels_batch.cpu())\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del logits_batch, cite_embeds_batch, labels_batch, predictions_batch\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all predictions and compute accuracy\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    num_citations = len(cite_embeds)\n",
    "    accuracy = total_correct / num_citations\n",
    "    avg_loss = total_loss / num_citations\n",
    "    \n",
    "    # Compute retrieval metrics using concatenated logits\n",
    "    all_logits = torch.cat(all_logits_list)\n",
    "    all_labels = torch.cat(all_labels_list)\n",
    "    retrieval_metrics = compute_retrieval_metrics(all_logits, all_labels, ks=k_values)\n",
    "    \n",
    "    # Prepare results dictionary (matching original function's output)\n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'num_citations': num_citations,\n",
    "        'num_unique_targets': len(target_art_ids_unique),\n",
    "        'mrr': retrieval_metrics['mrr']\n",
    "    }\n",
    "    \n",
    "    # Add top-k accuracies to results\n",
    "    for k in k_values:\n",
    "        if f'top_{k}_accuracy' in retrieval_metrics:\n",
    "            results[f'top_{k}_accuracy'] = retrieval_metrics[f'top_{k}_accuracy']\n",
    "    \n",
    "    if return_embeddings:\n",
    "        results.update({\n",
    "            'cite_embeds': cite_embeds.cpu(),\n",
    "            'ref_embeds': ref_embeds_unique.cpu(),\n",
    "            'cited_art_ids': cited_art_ids,\n",
    "            'target_art_ids': target_art_ids_unique,\n",
    "            'logits': torch.cat(all_logits_list),  # Full logits matrix\n",
    "            'labels': labels.cpu()\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "    \n",
    "def train_citation_model(\n",
    "    model: CitationModel,\n",
    "    results: List[dict],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ExperimentConfig,\n",
    "    device: Optional[str] = None,\n",
    "    project_name: Optional[str] = None,\n",
    "    run_name: Optional[str] = None\n",
    ") -> CitationModel:\n",
    "    \"\"\"\n",
    "    Training function with corrected wandb logging.\n",
    "    \"\"\"\n",
    "    import wandb\n",
    "    \n",
    "    # Initialize wandb with config parameters\n",
    "    project_name = project_name or config.project_name\n",
    "    run_name = run_name or config.run_name\n",
    "\n",
    "    \n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Set device\n",
    "    print(f\"Experiment config: {config}\")\n",
    "    device = device or config.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "    global_step = 0  # Add global step counter\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable memory efficient training\n",
    "    model.transformer.gradient_checkpointing_enable()\n",
    "    \n",
    "    # # Initialize optimizer with config parameters\n",
    "    # optimizer = AdamW(\n",
    "    #     model.parameters(), \n",
    "    #     lr=config.learning_rate, \n",
    "    #     weight_decay=config.weight_decay,\n",
    "    #     eps=config.Adam_eps\n",
    "    # )\n",
    "    # Initialize optimizer with separate parameter groups\n",
    "    optimizer = AdamW([\n",
    "        {\n",
    "            'params': [param for name, param in model.named_parameters() if name != 'logit_scale'],\n",
    "            'lr': config.learning_rate,\n",
    "            'weight_decay': config.weight_decay,\n",
    "            'eps': config.Adam_eps\n",
    "        },\n",
    "        {\n",
    "            'params': [model.logit_scale],  # separate group for logit_scale\n",
    "            'lr': config.logits_learning_rate,  # higher learning rate for logit_scale\n",
    "            'weight_decay': 0  # typically don't need weight decay for scale parameter\n",
    "        }\n",
    "    ], lr=config.learning_rate)\n",
    "    \n",
    "    best_val_metrics = {'loss': float('inf')}\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "\n",
    "        # Log current logit scale instead of temperature\n",
    "        current_scale = model.logit_scale.exp().item()\n",
    "        print(f\"Current logit scale: {current_scale:.4f}\")\n",
    "        wandb.log({\"logit_scale\": current_scale}, step=global_step)\n",
    "        \n",
    "        # Create new collated data for this epoch\n",
    "        print(\"Collating training data with new random masks...\")\n",
    "        collated = collate(results, tokenizer, config)\n",
    "        dataset = CitationDataset(collated)\n",
    "        train_size = int(len(dataset) * config.train_ratio)\n",
    "        train_dataset = dataset[:train_size]\n",
    "        val_dataset = dataset[train_size:]\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Log training metrics with global step\n",
    "            wandb.log({\n",
    "                \"train/batch_loss\": loss.item(),\n",
    "                \"train/learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "            }, step=global_step)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            global_step += 1  # Increment global step after each batch\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Log epoch-level training metrics\n",
    "        wandb.log({\n",
    "            \"train/epoch_loss\": avg_train_loss,\n",
    "            \"epoch\": epoch\n",
    "        }, step=global_step)\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"\\nRunning validation...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        \n",
    "        val_metrics = validate_citation_model(\n",
    "            model=model,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device,\n",
    "            k_values=config.k_values\n",
    "        )\n",
    "        \n",
    "        # Log validation metrics\n",
    "        wandb_val_metrics = {\n",
    "            \"val/loss\": val_metrics['loss'],\n",
    "            \"val/accuracy\": val_metrics['accuracy'],\n",
    "            \"val/mrr\": val_metrics['mrr']\n",
    "        }\n",
    "        \n",
    "        # Add top-k accuracies to wandb metrics\n",
    "        for k in config.k_values:\n",
    "            if f'top_{k}_accuracy' in val_metrics:\n",
    "                wandb_val_metrics[f\"val/top_{k}_accuracy\"] = val_metrics[f'top_{k}_accuracy']\n",
    "        \n",
    "        wandb.log(wandb_val_metrics, step=global_step)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"\\nValidation metrics:\")\n",
    "        print(f\"  Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"  Accuracy (top-1): {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Mean Reciprocal Rank: {val_metrics['mrr']:.4f}\")\n",
    "        print(f\"  Number of citations: {val_metrics['num_citations']}\")\n",
    "        print(f\"  Number of unique targets: {val_metrics['num_unique_targets']}\")\n",
    "        print(\"\\nTop-k accuracy:\")\n",
    "        for k in config.k_values:\n",
    "            if f'top_{k}_accuracy' in val_metrics:\n",
    "                print(f\"  k={k}: {val_metrics[f'top_{k}_accuracy']:.4f}\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if val_metrics['loss'] < best_val_metrics['loss']:\n",
    "            best_val_metrics = val_metrics\n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'validation_metrics': val_metrics,\n",
    "                'logit_scale': model.logit_scale.item(),\n",
    "                'global_step': global_step\n",
    "            }\n",
    "            torch.save(save_dict, config.save_path)\n",
    "            \n",
    "            # Log best metrics to wandb\n",
    "            wandb.run.summary.update({\n",
    "                \"best_val_loss\": best_val_metrics['loss'],\n",
    "                \"best_val_accuracy\": best_val_metrics['accuracy'],\n",
    "                \"best_val_mrr\": best_val_metrics['mrr'],\n",
    "                \"best_model_epoch\": epoch,\n",
    "                \"best_model_step\": global_step\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nSaved new best model to {config.save_path}\")\n",
    "            print(f\"Best validation metrics so far:\")\n",
    "            print(f\"  Loss: {best_val_metrics['loss']:.4f}\")\n",
    "            print(f\"  Accuracy (top-1): {best_val_metrics['accuracy']:.4f}\")\n",
    "            print(f\"   MRR: {best_val_metrics['mrr']:.4f}\")\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self,config: ExperimentConfig):\n",
    "        # get tokenizer \n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        tokenizer.add_special_tokens({\n",
    "            'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "        })\n",
    "        \"\"\"Update config with tokenizer-dependent values.\"\"\"\n",
    "        config.cite_token_id = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "        config.ref_token_id = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "        config.vocab_size = len(tokenizer)\n",
    "        model = CitationModel(config)\n",
    "        \n",
    "        self.config = config \n",
    "        self.model = model \n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "\n",
    "    def get_model(self, load_path = None):\n",
    "        if load_path:\n",
    "            ckpoint = torch.load('./experiments/best_citation_model.pt', weights_only=True)\n",
    "            self.model.load_state_dict(ckpoint['model_state_dict'])\n",
    "            self.model.config.temperature = checkpoint['temperature']\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "    def get_results(self, cache_path=None, tokenizer=None):\n",
    "        if cache_path:\n",
    "            results = tokenize_sources(cache_path=cache_path)\n",
    "        else:\n",
    "            # Load articles\n",
    "            preprocessor = WikiProcessor()\n",
    "            sources, citation_data = preprocessor.find_source_citations()\n",
    "            \n",
    "            results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "        return results\n",
    "            \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "config = ExperimentConfig(collate_sample_size=5000,initial_logit_scale=np.log(1.0/0.5))\n",
    "\n",
    "experiment = Experiment(config)\n",
    "\n",
    "tokenizer = experiment.get_tokenizer()\n",
    "\n",
    "model = experiment.get_model()\n",
    "\n",
    "# results = experiment.get_results(cache_path='./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt')\n",
    "\n",
    "# train model using config \n",
    "trained_model = train_citation_model(\n",
    "    model=model,\n",
    "    results=results,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useless =  0.1\n",
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "' ' ' april ' ' ' ( apr. ) is the fourth month of the year in the julian calendar | julian and gregorian calendar s, and comes between march and may. it is one of four months to have 30 <CITE> s. april always begins on the same day of the week as july, and additionally, january in leap years. april always ends on the same day of the week as <CITE>. = = the month = = april comes between march and may, making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as june, <CITE> and november are later in the year. april begins on the same day of the week as july every year and on the same day of the week as january in <CITE> s. april ends on the same day of the week as <CITE> every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in common year s, april starts on the same day of the week as <CITE> of the previous year, and in <CITE> s, may of the previous year. in common years, april finishes on the same day of the week as july of the previous year, and in leap years, february and <CITE> of the previous year. in common years immediately after other common years, april starts on the same day of the week as january of the previous year, and in leap years and years immediately after that, april finishes on the same day of the week as january of the previous year. in years immediately before common years, april starts on the same day of the week as <CITE> and <CITE> of the following year, and in years immediately before leap years, june of the following year. in years immediately before common years, april finishes on the same day of the week as september of the following year, and in years immediately before leap years, march and june of the following year. april [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Source attention mask: 126, \n",
      "Target: id=357:\n",
      "'''September''' (Sep.) is the ninth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[August]] and [[October]]. It has 30 [[day]]s. Its name comes from the [[Latin]] word ''sep...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' september ' ' ' ( sep. ) is the ninth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ august ] ] and [ [ october ] ]. it has 30 [ [ day ] ] s. its name comes from the [ [ latin ] ] word ' ' sept ' ' for \" seven \" ( it was the seventh month of the year, before [ [ january ] ] and [ [ february <REF>\n",
      "\n",
      "\n",
      "Target: id=832:\n",
      "'''day''' is the time it takes the [[Earth]] to spin around once. It is day time on the side of the Earth that is facing the [[Sun]]. When it is [[wikt:night|night]] time, that side of the Earth is fa...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' day ' ' ' is the time it takes the [ [ earth ] ] to spin around once. it is day time on the side of the earth that is facing the [ [ sun ] ]. when it is [ [ wikt : night | night ] ] time, that side of the earth is facing away from the sun. it takes 24 [ [ hour ] ] s for the earth to spin once, so that is one day, including the day time and night time <REF>\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[November]] (of the current year) and [[January]] (of the following year). It has...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' december ' ' ' ( dec. ) is the twelfth and last [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ november ] ] ( of the current year ) and [ [ january ] ] ( of the following year ). it has 31 days. with the name of the month coming from the [ [ latin ] ] ' ' decem ' ' for \" ten \", it was the tenth month of <REF>\n",
      "\n",
      "\n",
      "Target: id=297:\n",
      "'''October''' (Oct.) is the tenth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[September]] and [[November]]. It has 31 [[day]]s. The name comes from the [[Latin]] ''octo''...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' october ' ' ' ( oct. ) is the tenth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ september ] ] and [ [ november ] ]. it has 31 [ [ day ] ] s. the name comes from the [ [ latin ] ] ' ' octo ' ' for \" eight \". it was the eighth month of the year before [ [ january ] ] and [ [ february ] <REF>\n",
      "\n",
      "\n",
      "Target: id=228:\n",
      "'''leap year''' is a [[calendar year]] in which an extra [[day]] is added to the [[Gregorian calendar]], which is used by most of the world. A [[common year]] has 365 [[day]]s, but a leap year has 366...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' leap year ' ' ' is a [ [ calendar year ] ] in which an extra [ [ day ] ] is added to the [ [ gregorian calendar ] ], which is used by most of the world. a [ [ common year ] ] has 365 [ [ day ] ] s, but a leap year has 366 days. the extra day, february 29, is added to the [ [ month ] ] of [ [ february ] ]. in a common year, february <REF>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "useless_chars = np.sum([c==']' for c in source_text])*2/len(source_text)\n",
    "print('useless = ', useless_chars)\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "print(f\"Source attention mask: {(sample['attention_mask']==0).sum()}, \")\n",
    "\n",
    "for i, target_art_id in enumerate(sample['target_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['target_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:200]}...\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
