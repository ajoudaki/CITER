{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e45919-7df5-473e-991b-8c2f6adfcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 21:10:53,544 - INFO - Loading articles from JSONL file...\n",
      "2024-11-09 21:11:02,441 - INFO - Loaded 237381 articles\n",
      "2024-11-09 21:11:02,442 - INFO - Preparing training data...\n",
      "2024-11-09 21:11:04,229 - INFO - Preparing validation data...\n",
      "2024-11-09 21:11:04,725 - INFO - Using device: cuda\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Processing samples: 100%|██████████| 77/77 [01:05<00:00,  1.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Statistics:\n",
      "Total samples: 9798\n",
      "Processed: 6676\n",
      "Skipped (no citation): 3122\n",
      "Skipped (errors): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 3/3 [00:01<00:00,  2.43batch/s]\n",
      "2024-11-09 21:12:11,947 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Statistics:\n",
      "Total samples: 296\n",
      "Processed: 194\n",
      "Skipped (no citation): 102\n",
      "Skipped (errors): 0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 35/209 [00:45<03:46,  1.30s/it, loss=0.5454, avg_loss=1.0640]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from data_processing import (\n",
    "    ArticleContentProcessor,\n",
    "    CitationDataPreprocessor\n",
    ")\n",
    "from modeling import (\n",
    "    ModelConfig,\n",
    "    CitationMatcher,\n",
    "    CitationDataset,\n",
    "    create_dataloader\n",
    ")\n",
    "from training import (\n",
    "    TrainingConfig,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "def setup_logging(output_dir: Path) -> None:\n",
    "    \"\"\"Configure logging for the training process.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(output_dir / 'training.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def setup_environment() -> None:\n",
    "    \"\"\"Configure training environment.\"\"\"\n",
    "    # Set environment variables\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "def create_output_directory() -> Path:\n",
    "    \"\"\"Create and return output directory for this training run.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(f\"training_runs/run_{timestamp}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def load_and_prepare_data(\n",
    "    jsonl_path: str,\n",
    "    train_sample_size: int,\n",
    "    val_sample_size: int\n",
    ") -> tuple:\n",
    "    \"\"\"Load and prepare training and validation data.\"\"\"\n",
    "    logging.info(\"Loading articles from JSONL file...\")\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        articles_dict = {}\n",
    "        for line in f:\n",
    "            article = json.loads(line)\n",
    "            content = ArticleContentProcessor.clean_wiki_content(article['text'])\n",
    "            if content:\n",
    "                articles_dict[article['title'].lower()] = content\n",
    "    \n",
    "    logging.info(f\"Loaded {len(articles_dict)} articles\")\n",
    "    \n",
    "    # Prepare citation data\n",
    "    preprocessor = CitationDataPreprocessor(articles_dict)\n",
    "    \n",
    "    logging.info(\"Preparing training data...\")\n",
    "    train_sources, train_targets = preprocessor.create_citation_pairs(\n",
    "        sample_size=train_sample_size,\n",
    "        cite_samples_per_article=1\n",
    "    )\n",
    "\n",
    "    S = set(train_sources)\n",
    "    T = set(train_targets)\n",
    "    \n",
    "    logging.info(\"Preparing validation data...\")\n",
    "    val_sources, val_targets = preprocessor.create_citation_pairs(\n",
    "        sample_size=val_sample_size,\n",
    "        cite_samples_per_article=10\n",
    "    )\n",
    "\n",
    "    # Remove any validation samples that are also in the training set\n",
    "    val_sources, val_targets = zip(*[\n",
    "        (source, target) for source, target in zip(val_sources, val_targets)\n",
    "        if source not in S and target not in T\n",
    "    ])\n",
    "    \n",
    "    return train_sources, train_targets, val_sources, val_targets\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    output_dir = create_output_directory()\n",
    "    setup_logging(output_dir)\n",
    "    setup_environment()\n",
    "    \n",
    "    # Configuration\n",
    "    model_config = ModelConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        max_length=512,\n",
    "        cite_token=\"<CITE>\",\n",
    "        ref_token=\"<REF>\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        learning_rate=1.5e-4,\n",
    "        temperature=0.1,\n",
    "        num_workers=4,\n",
    "        gradient_clip_value=1.0,\n",
    "        scheduler_patience=2,\n",
    "        scheduler_factor=0.5,\n",
    "        eval_k_values=[1, 3, 5, 10, 50]\n",
    "    )\n",
    "    \n",
    "    # Data preparation\n",
    "    train_sources, train_targets, val_sources, val_targets = load_and_prepare_data(\n",
    "        jsonl_path='./data/wiki_articles.jsonl',\n",
    "        train_sample_size=10000,\n",
    "        val_sample_size=100\n",
    "    )\n",
    "    \n",
    "    # Model initialization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    model = CitationMatcher(model_config).to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = CitationDataset(\n",
    "        sources=train_sources,\n",
    "        targets=train_targets,\n",
    "        tokenizer=model.tokenizer,\n",
    "        config=model_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = CitationDataset(\n",
    "        sources=val_sources,\n",
    "        targets=val_targets,\n",
    "        tokenizer=model.tokenizer,\n",
    "        config=model_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_loader = create_dataloader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=training_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=training_config.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = create_dataloader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=training_config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=training_config.num_workers\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    logging.info(\"Starting training...\")\n",
    "    metrics_history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=training_config,\n",
    "        save_dir=output_dir / 'checkpoints',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Save training history\n",
    "    torch.save(\n",
    "        {\n",
    "            'metrics_history': [metric.__dict__ for metric in metrics_history],\n",
    "            'model_config': model_config.__dict__,\n",
    "            'training_config': training_config.__dict__\n",
    "        },\n",
    "        output_dir / 'training_history.pt'\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Training completed. Results saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during training:\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ba1cc-d1d3-4cf5-9fbb-e82f0e4186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb302bc-5802-4f5d-959c-4bdf4c9acdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f27a2-64ab-4277-ad92-4dc3b610b132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
