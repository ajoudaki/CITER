{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb9402b-e160-4eb9-9ba2-e67bda839fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_341561/3868594122.py:889: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=self.config.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming wandb run: bxwycp0q\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bxwycp0q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆▆▆███████</td></tr><tr><td>logit_scale</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████████</td></tr><tr><td>train/batch_in_epoch</td><td>▁▁▂▃▃▄▄▅▆▆▇██▁▁▂▃▃▄▄▅▆▆▆▇▁▂▂▃▃▅▆▆▆▇█▁▁▃▄</td></tr><tr><td>train/batch_loss</td><td>█▇▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch_loss</td><td>█▃▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▆█</td></tr><tr><td>val/loss</td><td>█▃▁</td></tr><tr><td>val/mrr</td><td>▁▆█</td></tr><tr><td>val/top_1000_accuracy</td><td>▁▇█</td></tr><tr><td>val/top_100_accuracy</td><td>▁▇█</td></tr><tr><td>val/top_10_accuracy</td><td>▁▆█</td></tr><tr><td>val/top_1_accuracy</td><td>▁▆█</td></tr><tr><td>val/top_50_accuracy</td><td>▁▇█</td></tr><tr><td>val/top_5_accuracy</td><td>▁▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_model_epoch</td><td>2</td></tr><tr><td>best_model_step</td><td>54</td></tr><tr><td>best_val_accuracy</td><td>0.22978</td></tr><tr><td>best_val_loss</td><td>4.13941</td></tr><tr><td>best_val_mrr</td><td>0.33235</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>logit_scale</td><td>3.06166</td></tr><tr><td>train/batch_in_epoch</td><td>8</td></tr><tr><td>train/batch_loss</td><td>1.44116</td></tr><tr><td>train/epoch_loss</td><td>1.80679</td></tr><tr><td>train/learning_rate</td><td>0.00015</td></tr><tr><td>val/accuracy</td><td>0.22978</td></tr><tr><td>val/loss</td><td>4.13941</td></tr><tr><td>val/mrr</td><td>0.33235</td></tr><tr><td>val/top_1000_accuracy</td><td>0.97078</td></tr><tr><td>val/top_100_accuracy</td><td>0.8206</td></tr><tr><td>val/top_10_accuracy</td><td>0.53849</td></tr><tr><td>val/top_1_accuracy</td><td>0.22895</td></tr><tr><td>val/top_50_accuracy</td><td>0.74545</td></tr><tr><td>val/top_5_accuracy</td><td>0.44596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-pine-64</strong> at: <a href='https://wandb.ai/amirjoudaki/citation-matching/runs/bxwycp0q' target=\"_blank\">https://wandb.ai/amirjoudaki/citation-matching/runs/bxwycp0q</a><br/> View project at: <a href='https://wandb.ai/amirjoudaki/citation-matching' target=\"_blank\">https://wandb.ai/amirjoudaki/citation-matching</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241127_180613-bxwycp0q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bxwycp0q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/paperGPT/wandb/run-20241127_181416-bxwycp0q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/amirjoudaki/citation-matching/runs/bxwycp0q' target=\"_blank\">feasible-pine-64</a></strong> to <a href='https://wandb.ai/amirjoudaki/citation-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amirjoudaki/citation-matching' target=\"_blank\">https://wandb.ai/amirjoudaki/citation-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amirjoudaki/citation-matching/runs/bxwycp0q' target=\"_blank\">https://wandb.ai/amirjoudaki/citation-matching/runs/bxwycp0q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341561/3868594122.py:987: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from checkpoint at epoch 3, batch 6, step 60\n",
      "\n",
      "Epoch 4/100\n",
      "Current logit scale: 21.3173\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                            | 1351/237381 [00:04<12:26, 316.16it/s]\n",
      "Training:   6%|████▏                                                                      | 1/18 [00:01<00:33,  1.97s/it]/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 63. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Training:  39%|████████████████████████▉                                       | 7/18 [00:09<00:14,  1.31s/it, loss=1.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint at step 60 to checkpoints/citation-matching/feasible-pine-64/checkpoint-step-60.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 63. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Training:  56%|███████████████████████████████████                            | 10/18 [00:23<00:23,  2.89s/it, loss=1.42]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 61 that is less than the current step 63. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 62 that is less than the current step 63. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "Training:  94%|███████████████████████████████████████████████████████████▌   | 17/18 [00:57<00:04,  4.93s/it, loss=1.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint at step 70 to checkpoints/citation-matching/feasible-pine-64/checkpoint-step-70.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████| 18/18 [01:03<00:00,  3.51s/it, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.5000\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.00s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 147.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  loss: 4.0877\n",
      "  accuracy: 0.2333\n",
      "  num_citations: 24072.0000\n",
      "  num_unique_targets: 9672.0000\n",
      "  mrr: 0.3365\n",
      "  top_1_accuracy: 0.2322\n",
      "  top_5_accuracy: 0.4479\n",
      "  top_10_accuracy: 0.5452\n",
      "  top_50_accuracy: 0.7488\n",
      "  top_100_accuracy: 0.8265\n",
      "  top_1000_accuracy: 0.9761\n",
      "\n",
      "Saved new best model to checkpoints/citation-matching/feasible-pine-64/best_model.pt\n",
      "Best validation metrics:\n",
      "  loss: 4.0877\n",
      "  accuracy: 0.2333\n",
      "  mrr: 0.3365\n",
      "\n",
      "Epoch 5/100\n",
      "Current logit scale: 21.4952\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                            | 1351/237381 [00:02<07:46, 506.12it/s]\n",
      "Training:  50%|████████████████████████████████                                | 9/18 [00:45<00:47,  5.22s/it, loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint at step 80 to checkpoints/citation-matching/feasible-pine-64/checkpoint-step-80.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████| 18/18 [01:28<00:00,  4.89s/it, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.3010\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.96s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 246.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  loss: 4.0048\n",
      "  accuracy: 0.2430\n",
      "  num_citations: 24119.0000\n",
      "  num_unique_targets: 9572.0000\n",
      "  mrr: 0.3465\n",
      "  top_1_accuracy: 0.2423\n",
      "  top_5_accuracy: 0.4591\n",
      "  top_10_accuracy: 0.5547\n",
      "  top_50_accuracy: 0.7604\n",
      "  top_100_accuracy: 0.8341\n",
      "  top_1000_accuracy: 0.9780\n",
      "\n",
      "Saved new best model to checkpoints/citation-matching/feasible-pine-64/best_model.pt\n",
      "Best validation metrics:\n",
      "  loss: 4.0048\n",
      "  accuracy: 0.2430\n",
      "  mrr: 0.3465\n",
      "\n",
      "Epoch 6/100\n",
      "Current logit scale: 21.5611\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                            | 1351/237381 [00:02<07:59, 491.73it/s]\n",
      "Training:   6%|███▌                                                            | 1/18 [00:08<02:27,  8.66s/it, loss=1.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint at step 90 to checkpoints/citation-matching/feasible-pine-64/checkpoint-step-90.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████████████████████████████████████▌                        | 11/18 [00:56<00:36,  5.21s/it, loss=1.21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint at step 100 to checkpoints/citation-matching/feasible-pine-64/checkpoint-step-100.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████| 18/18 [01:29<00:00,  4.99s/it, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 1.1478\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.96s/it]\n",
      "Computing similarities: 100%|███████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 238.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  loss: 4.0234\n",
      "  accuracy: 0.2454\n",
      "  num_citations: 24050.0000\n",
      "  num_unique_targets: 9595.0000\n",
      "  mrr: 0.3482\n",
      "  top_1_accuracy: 0.2448\n",
      "  top_5_accuracy: 0.4567\n",
      "  top_10_accuracy: 0.5517\n",
      "  top_50_accuracy: 0.7651\n",
      "  top_100_accuracy: 0.8402\n",
      "  top_1000_accuracy: 0.9783\n",
      "\n",
      "Epoch 7/100\n",
      "Current logit scale: 21.5721\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                            | 1351/237381 [00:02<07:32, 521.12it/s]\n",
      "Training:  17%|██████████▌                                                    | 3/18 [00:17<01:28,  5.92s/it, loss=0.901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved checkpoint at step 110 to checkpoints/citation-matching/feasible-pine-64/checkpoint-step-110.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████████████████████████████████████▌                        | 11/18 [01:00<00:38,  5.49s/it, loss=1.17]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1249\u001b[0m\n\u001b[1;32m   1247\u001b[0m config\u001b[38;5;241m.\u001b[39mresume_from \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/citation-matching/feasible-pine-64/checkpoint-step-60.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1248\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(config)\n\u001b[0;32m-> 1249\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_citation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 1099\u001b[0m, in \u001b[0;36mtrain_citation_model\u001b[0;34m(experiment, results)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m   1097\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[0;32m-> 1099\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# Update tracking\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/amp/grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import random \n",
    "import bz2\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import tqdm \n",
    "import yaml\n",
    "\n",
    "@dataclass\n",
    "class WikiArticle:\n",
    "    \"\"\"Represents a Wikipedia article with its metadata.\"\"\"\n",
    "    title: str\n",
    "    text: str\n",
    "    timestamp: str\n",
    "    is_redirect: bool\n",
    "\n",
    "class WikiDumpProcessor:\n",
    "    \"\"\"Processes Wikipedia XML dumps and extracts articles.\"\"\"\n",
    "    \n",
    "    def __init__(self, dump_path: str):\n",
    "        self.dump_path = dump_path\n",
    "        self._ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "        self._skip_prefixes = {\n",
    "            'Wikipedia:', 'Template:', 'Category:', 'Portal:', 'File:', \n",
    "            'MediaWiki:', 'Help:', 'Book:', 'Draft:', 'TimedText:', \n",
    "            'Module:', 'Special:'\n",
    "        }\n",
    "\n",
    "    def iter_articles(self, skip_redirects: bool = True) -> Iterator[WikiArticle]:\n",
    "        \"\"\"Iterates through valid articles in the dump.\"\"\"\n",
    "        dump_file = bz2.BZ2File(self.dump_path) if self.dump_path.endswith('.bz2') else open(self.dump_path, 'rb')\n",
    "        \n",
    "        for _, elem in ET.iterparse(dump_file, events=('end',)):\n",
    "            if not elem.tag.endswith('page'):\n",
    "                continue\n",
    "\n",
    "            # Extract basic article data\n",
    "            title = elem.find('.//mw:title', self._ns).text\n",
    "            if any(title.startswith(prefix) for prefix in self._skip_prefixes):\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            # Get revision data\n",
    "            rev = elem.find('.//mw:revision', self._ns)\n",
    "            text = rev.find('mw:text', self._ns).text if rev is not None else ''\n",
    "            timestamp = rev.find('mw:timestamp', self._ns).text if rev is not None else ''\n",
    "            is_redirect = bool(re.match(r'#REDIRECT', text or '', re.IGNORECASE))\n",
    "\n",
    "            if skip_redirects and is_redirect:\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            yield WikiArticle(title=title, text=text, timestamp=timestamp, is_redirect=is_redirect)\n",
    "            elem.clear()\n",
    "\n",
    "class ArticleStorage:\n",
    "    \"\"\"Handles storage and retrieval of Wikipedia articles.\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: WikiDumpProcessor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def save_to_jsonl(self, output_path: Union[str, Path], sample_size: Optional[int] = None) -> int:\n",
    "        \"\"\"Saves articles to a JSONL file.\"\"\"\n",
    "        count = 0\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for i, article in enumerate(self.processor.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                json.dump(article.__dict__, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def save_to_sqlite(self, db_path: Union[str, Path], sample_size: Optional[int] = None,\n",
    "                      batch_size: int = 1000) -> int:\n",
    "        \"\"\"Saves articles to a SQLite database.\"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        \n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS articles\n",
    "                    (title TEXT PRIMARY KEY, text TEXT, timestamp TEXT, is_redirect INTEGER)''')\n",
    "        c.execute('CREATE INDEX IF NOT EXISTS idx_title ON articles(title)')\n",
    "        \n",
    "        count = 0\n",
    "        batch = []\n",
    "        \n",
    "        try:\n",
    "            for i, article in enumerate(self.processor.iter_articles()):\n",
    "                if sample_size is not None and i >= sample_size:\n",
    "                    break\n",
    "                    \n",
    "                batch.append((article.title, article.text, article.timestamp, \n",
    "                            1 if article.is_redirect else 0))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    c.executemany('INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)', batch)\n",
    "                    conn.commit()\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "            \n",
    "            if batch:\n",
    "                c.executemany('INSERT OR REPLACE INTO articles VALUES (?, ?, ?, ?)', batch)\n",
    "                conn.commit()\n",
    "                count += len(batch)\n",
    "                \n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "        return count\n",
    "\n",
    "\n",
    "\n",
    "class WikiProcessor:\n",
    "    \"\"\"Prepares citation data for model training.\"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str = \"data/wiki_articles.jsonl\"):\n",
    "        \n",
    "        # Load articles\n",
    "        logging.info(\"Loading articles from JSONL file...\")\n",
    "        self.articles_dict = {}\n",
    "        self.id2ref = {}\n",
    "        self.ref2id = {}\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                ref = article['title'].lower()\n",
    "                id = len(self.articles_dict) + 1\n",
    "                self.articles_dict[ref] = self.clean_wiki_text(article['text'])\n",
    "                self.ref2id[ref] = id \n",
    "                self.id2ref[id] = ref\n",
    "        logging.info(f\"Loaded {len(self.articles_dict)} articles.\")\n",
    "\n",
    "    def _find_citations(self,text):\n",
    "        citations = []\n",
    "        for match in re.finditer(r'\\[\\[(.*?)\\]\\]', text):\n",
    "            match_text = match.group(1)\n",
    "            citation = match_text.split('|') if '|' in match_text else [match_text]\n",
    "            citation = [(c.split('#')[0] if '#' in c else c) for c in citation]\n",
    "            ref = None\n",
    "            for cit in citation:\n",
    "                if cit.lower() in self.articles_dict:\n",
    "                    ref = cit.lower()\n",
    "                    break\n",
    "            if ref:\n",
    "                citations.append((match.start(), match.end(), self.ref2id[ref]))\n",
    "        return citations\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_wiki_text(text: str) -> str:\n",
    "        \"\"\"Cleans wiki content by removing metadata and formatting.\"\"\"\n",
    "        # Find main content starting from first bold title\n",
    "        match = re.search(r\"'''([^']+?)'''\", text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "\n",
    "        # Remove wiki elements and clean up\n",
    "        text = re.sub(r'\\[\\[File:.*\\]\\]|\\[\\[Category:.*\\]\\]|\\{\\{stub.*\\}\\}', '', text)\n",
    "        return '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
    "\n",
    "    def find_source_citations(self) -> Tuple[List[str], List[Tuple[List[str], int, int]]]:\n",
    "        \"\"\"Creates source-target pairs for citation matching.\"\"\"\n",
    "\n",
    "        articles = list(self.articles_dict.keys())\n",
    "        sources = []\n",
    "        citation_data = []\n",
    "\n",
    "        for title in articles:\n",
    "            text = self.articles_dict[title]\n",
    "            source_text = self.clean_wiki_text(text)\n",
    "            citations = self._find_citations(source_text)            \n",
    "            sources.append(source_text)\n",
    "            citation_data.append(citations)\n",
    "\n",
    "        return sources, citation_data\n",
    "\n",
    "\n",
    "# experiment related \n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_cache_path(sources, model_name: str, cache_dir: str) -> str:\n",
    "    \"\"\"Generate a unique cache path based on input data and model name.\"\"\"\n",
    "    # Create a hash of the sources and model name\n",
    "    content_hash = hashlib.md5(str(sources).encode()).hexdigest()\n",
    "    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]\n",
    "    return os.path.join(cache_dir, f\"tokenized_{model_hash}_{content_hash}.pt\")\n",
    "\n",
    "def tokenize_sources(sources=None, citation_data=None, tokenizer=None, batch_size=1000, cache_dir=\"cache\", cache_path=None):\n",
    "    # Generate cache path\n",
    "    if cache_path is None:\n",
    "        cache_path = get_cache_path(sources, tokenizer.name_or_path, cache_dir)\n",
    "    \n",
    "    # Check if cached results exist\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached tokenized results from {cache_path}\")\n",
    "        return torch.load(cache_path, weights_only=False)\n",
    "    \n",
    "    logging.info(\"Tokenizing sources...\")\n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in tqdm.tqdm(range(0, len(sources), batch_size), total=len(sources)//batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sources))\n",
    "        batch_sources = sources[batch_start:batch_end]\n",
    "        batch_citations = citation_data[batch_start:batch_end]\n",
    "        \n",
    "        # Batch encode\n",
    "        batch_encoded = tokenizer.batch_encode_plus(\n",
    "            batch_sources,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for idx in range(len(batch_sources)):\n",
    "            offset_mapping = batch_encoded[\"offset_mapping\"][idx]\n",
    "            input_ids = batch_encoded[\"input_ids\"][idx]\n",
    "            \n",
    "            # Create offset to index mapping\n",
    "            off2i = {s:i for i, (s,_) in enumerate(offset_mapping)}\n",
    "            off2i.update({e:i+1 for i, (_,e) in enumerate(offset_mapping)})\n",
    "            \n",
    "            # Create citation tokens array\n",
    "            mask_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            cite_tokens = np.zeros(len(input_ids), dtype=int)\n",
    "            \n",
    "            # Fill in citations\n",
    "            for i, j, art_id in batch_citations[idx]:\n",
    "                s, e = off2i[i], off2i[j]\n",
    "                cite_tokens[s] = art_id\n",
    "                mask_tokens[s:e] = art_id\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'input_ids': np.array(input_ids),\n",
    "                'cite_tokens': cite_tokens,\n",
    "                'mask_tokens': mask_tokens,\n",
    "                'attention_mask': batch_encoded[\"attention_mask\"][idx] if \"attention_mask\" in batch_encoded else None\n",
    "            })\n",
    "\n",
    "    # Cache the results\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    torch.save(all_results, cache_path)\n",
    "    logging.info(f\"Cached tokenized results to {cache_path}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def collate(results, tokenizer, config):\n",
    "    cite_token = tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "    ref_token = tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "    bracket_tokens = tokenizer.convert_tokens_to_ids(['[',']'])\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "\n",
    "    collated_data = []\n",
    "    # id_to_tokenized = {i: result for i, result in enumerate(results)}\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(results))):\n",
    "        result = results[i]\n",
    "        if config.collate_sample_size and len(collated_data)>config.collate_sample_size:\n",
    "            break\n",
    "        \n",
    "        # Process each source segment\n",
    "        for s in range(0, len(result['input_ids']), int((1-config.overlap)*config.source_len)):\n",
    "            e = s + config.source_len\n",
    "            \n",
    "            # Get source segment\n",
    "            input_ids = result['input_ids'][s:e].copy()\n",
    "            cite_tokens = result['cite_tokens'][s:e]\n",
    "            mask_tokens = result['mask_tokens'][s:e]\n",
    "            \n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "                \n",
    "            # Get all citations from this segment\n",
    "            present_citations = np.unique(cite_tokens[cite_tokens > 0])\n",
    "            if len(present_citations) > config.max_targets:\n",
    "                present_citations = np.random.choice(present_citations, config.max_targets, replace=False)\n",
    "            max_targets = min(config.max_targets, len(present_citations))\n",
    "\n",
    "            # Skip if segment is too short\n",
    "            if len(input_ids) < config.source_len // 2:\n",
    "                continue\n",
    "            # Skip if no citations\n",
    "            if max_targets == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize target arrays\n",
    "            target_ids = np.full((max_targets, config.target_len), pad_token, dtype=np.int64)\n",
    "            target_attention_mask = np.zeros((max_targets, config.target_len), dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            # Prepare source: \n",
    "            # only keep citation tokens that are sampled to be masked \n",
    "            cite_tokens_mask = np.isin(cite_tokens, present_citations)\n",
    "            # don't mask citations that are not sampled \n",
    "            mask_tokens = np.where(np.isin(mask_tokens, present_citations), mask_tokens, 0)\n",
    "            # remove brackets from the rest of the text \n",
    "            mask_tokens = np.where(np.isin(input_ids,bracket_tokens),1, mask_tokens)\n",
    "            # don't mask the citation tokens \n",
    "            mask_tokens[cite_tokens_mask] = 0\n",
    "            # set the citation tokens (first token of a citation range) as special token <CITE> \n",
    "            input_ids[cite_tokens_mask] = cite_token\n",
    "            # mask all tokens in a citation, except for the first (special) token \n",
    "            source_ids = input_ids[mask_tokens == 0]\n",
    "\n",
    "            # keep the cited article ids in the text in the order they appear (with repeats)\n",
    "            # & keep the unique cited artile ids \n",
    "            # this will enable us to link each special cite token to a target via the article id\n",
    "            target_art_ids = present_citations\n",
    "            cited_art_ids = cite_tokens[cite_tokens_mask]\n",
    "            \n",
    "            # Pad or truncate source\n",
    "            if len(source_ids) > config.source_len:\n",
    "                source_ids = source_ids[:config.source_len]\n",
    "            elif len(source_ids) < config.source_len:\n",
    "                source_ids = np.pad(source_ids, \n",
    "                                  (0, config.source_len - len(source_ids)),\n",
    "                                  'constant', \n",
    "                                  constant_values=pad_token)\n",
    "            \n",
    "            # Create source attention mask\n",
    "            attention_mask = (source_ids != pad_token).astype(np.int64)\n",
    "            \n",
    "            # Process each target\n",
    "            for idx, citation_id in enumerate(present_citations):\n",
    "                # Get pre-tokenized target content\n",
    "                # ids are 1-indexed \n",
    "                target_data = results[citation_id - 1]\n",
    "                target_tokens = target_data['input_ids']\n",
    "                \n",
    "                # Truncate if needed and add ref_token\n",
    "                if len(target_tokens) >= config.target_len - 1:\n",
    "                    target_tokens = target_tokens[:config.target_len-1]\n",
    "                target_tokens = np.append(target_tokens, ref_token)\n",
    "                \n",
    "                # Pad to target_len\n",
    "                if len(target_tokens) < config.target_len:\n",
    "                    target_tokens = np.pad(target_tokens,\n",
    "                                         (0, config.target_len - len(target_tokens)),\n",
    "                                         'constant',\n",
    "                                         constant_values=pad_token)\n",
    "                \n",
    "                # Store in target arrays\n",
    "                target_ids[idx] = target_tokens\n",
    "                target_attention_mask[idx] = (target_tokens != pad_token)\n",
    "                # citation_ids[idx] = citation_id\n",
    "\n",
    "\n",
    "            # Store the collected data\n",
    "            collated_data.append({\n",
    "                'source_art_id': i+1,\n",
    "                'source_ids': torch.tensor(source_ids, dtype=torch.long),\n",
    "                'cited_art_ids': torch.tensor(cited_art_ids, dtype=torch.long),\n",
    "                'target_art_ids': torch.tensor(target_art_ids, dtype=torch.long),\n",
    "                'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'target_attention_mask': torch.tensor(target_attention_mask, dtype=torch.long),\n",
    "            })\n",
    "    \n",
    "    return collated_data\n",
    "\n",
    "class CitationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for citation data with stacked targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, collated_data):\n",
    "        self.data = collated_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def citation_collate_fn(batch):\n",
    "    # Stack sources normally\n",
    "    source_ids = torch.stack([item['source_ids'] for item in batch])\n",
    "    cited_art_ids = torch.cat([item['cited_art_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # Concatenate targets\n",
    "    target_art_ids_all = torch.cat([item['target_art_ids'] for item in batch])\n",
    "    target_ids = torch.cat([item['target_ids'] for item in batch])\n",
    "    target_attention_mask = torch.cat([item['target_attention_mask'] for item in batch])\n",
    "\n",
    "    # Get unique indices and inverse indices\n",
    "    target_art_ids, unique_indices = np.unique(target_art_ids_all.numpy(), return_index=True)\n",
    "    target_art_ids = torch.tensor(target_art_ids)\n",
    "    unique_indices = torch.tensor(unique_indices)\n",
    "    \n",
    "    # Use unique indices to get corresponding targets\n",
    "    target_ids = target_ids[unique_indices]\n",
    "    target_attention_mask = target_attention_mask[unique_indices]\n",
    "\n",
    "    id2i = {id.item():i for i,id in enumerate(target_art_ids)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids],dtype=torch.long)\n",
    "\n",
    "      \n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'cited_art_ids': cited_art_ids,\n",
    "        'target_art_ids': target_art_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'target_attention_mask': target_attention_mask,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CitationModelOutput:\n",
    "    \"\"\"Custom output class for the citation model.\"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    cite_embeds: Optional[torch.FloatTensor] = None\n",
    "    ref_embeds: Optional[torch.FloatTensor] = None\n",
    "\n",
    "class CitationModel(nn.Module):\n",
    "    \"\"\"Custom model for citation matching using transformer embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load base model configuration\n",
    "        base_config = AutoConfig.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # Load base transformer model\n",
    "        self.transformer = AutoModel.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Resize token embeddings if needed\n",
    "        if config.vocab_size != self.transformer.config.vocab_size:\n",
    "            self.transformer.resize_token_embeddings(config.vocab_size)\n",
    "\n",
    "        # Add learnable logit scale parameter\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * config.initial_logit_scale)\n",
    "\n",
    "    \n",
    "    def get_citation_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for citation token positions.\"\"\"\n",
    "        return input_ids == self.config.cite_token_id\n",
    "    \n",
    "    def get_reference_masks(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for reference token positions.\"\"\"\n",
    "        return input_ids == self.config.ref_token_id\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        source_ids: torch.Tensor,\n",
    "        target_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        target_attention_mask: Optional[torch.Tensor] = None,\n",
    "        cited_art_ids: Optional[torch.Tensor] = None,\n",
    "        target_art_ids: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[Tuple, CitationModelOutput]:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        \n",
    "        # Process source text\n",
    "        source_outputs = self.transformer(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Process target text\n",
    "        target_outputs = self.transformer(\n",
    "            input_ids=target_ids,\n",
    "            attention_mask=target_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get citation mask and extract citation embeddings\n",
    "        cite_mask = self.get_citation_masks(source_ids)\n",
    "        cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "        \n",
    "        # Get reference mask and extract reference embeddings\n",
    "        ref_mask = self.get_reference_masks(target_ids)\n",
    "        ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        cite_embeds = F.normalize(cite_embeds, p=2, dim=-1)\n",
    "        ref_embeds = F.normalize(ref_embeds, p=2, dim=-1)\n",
    "        \n",
    "        # Clamp logit scale to prevent numerical instability\n",
    "        logit_scale = torch.clamp(self.logit_scale, 0, torch.log(torch.tensor(20.0)))\n",
    "        \n",
    "        # Compute similarity scores with learned scale\n",
    "        logits = torch.matmul(cite_embeds, ref_embeds.t()) * logit_scale.exp()\n",
    "\n",
    "        # compute the loss \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        if return_dict:\n",
    "            return CitationModelOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                cite_embeds=cite_embeds,\n",
    "                ref_embeds=ref_embeds\n",
    "            )\n",
    "        \n",
    "        return (loss, logits, cite_embeds, ref_embeds)\n",
    "\n",
    "\n",
    "def compute_retrieval_metrics(logits, labels, ks=[1, 5, 10, 50, 100, 1000]):\n",
    "    # Get rankings of correct targets\n",
    "    correct_scores = logits[torch.arange(logits.size(0)), labels]\n",
    "    rankings = (logits >= correct_scores.unsqueeze(1)).sum(1)\n",
    "    \n",
    "    # Compute MRR\n",
    "    mrr = (1.0 / rankings).mean().item()\n",
    "    \n",
    "    # Compute top-k accuracy for different k values\n",
    "    metrics = {'mrr': mrr}\n",
    "    for k in ks:\n",
    "        if k <= logits.size(1):  # Only compute if k is not larger than number of targets\n",
    "            top_k_acc = (rankings <= k).float().mean().item()\n",
    "            metrics[f'top_{k}_accuracy'] = top_k_acc\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def validate_citation_model(\n",
    "    model,\n",
    "    val_dataloader,\n",
    "    device: str = None,\n",
    "    return_embeddings: bool = False,\n",
    "    k_values: List[int] = [1, 5, 10, 50, 100, 1000],\n",
    "    similarity_batch_size: int = 512\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store accumulated embeddings and IDs\n",
    "    all_cite_embeds = []\n",
    "    all_ref_embeds = []\n",
    "    all_cited_art_ids = []\n",
    "    all_target_art_ids = []\n",
    "    \n",
    "    # Accumulate embeddings and IDs\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(val_dataloader, desc=\"Computing embeddings\"):\n",
    "            # Move batch to device and convert to FP16\n",
    "            batch = {k: (v.to(device, dtype=torch.float16) if isinstance(v, torch.FloatTensor) \n",
    "                        else v.to(device)) for k, v in batch.items()}\n",
    "            \n",
    "            # Process source text\n",
    "            source_outputs = model.transformer(\n",
    "                input_ids=batch['source_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Process target text\n",
    "            target_outputs = model.transformer(\n",
    "                input_ids=batch['target_ids'],\n",
    "                attention_mask=batch['target_attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Extract embeddings with masks\n",
    "            cite_mask = model.get_citation_masks(batch['source_ids'])\n",
    "            cite_embeds = source_outputs.last_hidden_state[cite_mask]\n",
    "            ref_mask = model.get_reference_masks(batch['target_ids'])\n",
    "            ref_embeds = target_outputs.last_hidden_state[ref_mask]\n",
    "            \n",
    "            # Normalize and move to CPU immediately\n",
    "            cite_embeds = F.normalize(cite_embeds, p=2, dim=-1).cpu()\n",
    "            ref_embeds = F.normalize(ref_embeds, p=2, dim=-1).cpu()\n",
    "            \n",
    "            # Store embeddings and IDs on CPU\n",
    "            all_cite_embeds.append(cite_embeds)\n",
    "            all_ref_embeds.append(ref_embeds)\n",
    "            all_cited_art_ids.append(batch['cited_art_ids'].cpu())\n",
    "            all_target_art_ids.append(batch['target_art_ids'].cpu())\n",
    "            \n",
    "            # Clear GPU cache after each batch\n",
    "            del source_outputs, target_outputs, cite_embeds, ref_embeds\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all accumulated tensors\n",
    "    cite_embeds = torch.cat(all_cite_embeds)\n",
    "    ref_embeds = torch.cat(all_ref_embeds)\n",
    "    cited_art_ids = torch.cat(all_cited_art_ids)\n",
    "    target_art_ids = torch.cat(all_target_art_ids)\n",
    "    \n",
    "    # Get unique target art IDs and create mapping\n",
    "    target_art_ids_unique, unique_indices = np.unique(target_art_ids.numpy(), return_index=True)\n",
    "    target_art_ids_unique = torch.tensor(target_art_ids_unique)\n",
    "    ref_embeds_unique = ref_embeds[torch.tensor(unique_indices)]\n",
    "    \n",
    "    # Create ID to index mapping\n",
    "    id2i = {id.item(): i for i, id in enumerate(target_art_ids_unique)}\n",
    "    labels = torch.tensor([id2i[id.item()] for id in cited_art_ids], dtype=torch.long)\n",
    "    \n",
    "    # Process in smaller batches for similarity computation\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    all_predictions = []\n",
    "    logits_list = []  # Store logits temporarily for metrics computation\n",
    "    labels_list = []  # Store labels temporarily for metrics computation\n",
    "    \n",
    "    num_batches = (len(cite_embeds) + similarity_batch_size - 1) // similarity_batch_size\n",
    "    logit_scale = torch.clamp(model.logit_scale, 0, torch.log(torch.tensor(20.0)))\n",
    "    \n",
    "    # Move ref_embeds to GPU once\n",
    "    ref_embeds_unique = ref_embeds_unique.to(device)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(num_batches), desc=\"Computing similarities\"):\n",
    "        start_idx = i * similarity_batch_size\n",
    "        end_idx = min((i + 1) * similarity_batch_size, len(cite_embeds))\n",
    "        \n",
    "        # Process batch\n",
    "        cite_embeds_batch = cite_embeds[start_idx:end_idx].to(device)\n",
    "        labels_batch = labels[start_idx:end_idx].to(device)\n",
    "        \n",
    "        # Compute similarities and loss\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits_batch = torch.matmul(cite_embeds_batch, ref_embeds_unique.t()) * logit_scale.exp()\n",
    "            loss_batch = F.cross_entropy(logits_batch, labels_batch)\n",
    "        \n",
    "        total_loss += loss_batch.item() * len(labels_batch)\n",
    "        predictions_batch = torch.argmax(logits_batch, dim=-1)\n",
    "        total_correct += (predictions_batch == labels_batch).sum().item()\n",
    "        \n",
    "        # Store predictions and move to CPU\n",
    "        all_predictions.append(predictions_batch.cpu())\n",
    "        logits_list.append(logits_batch.cpu())\n",
    "        labels_list.append(labels_batch.cpu())\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del logits_batch, cite_embeds_batch, labels_batch, predictions_batch\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute final metrics\n",
    "    num_citations = len(cite_embeds)\n",
    "    accuracy = total_correct / num_citations\n",
    "    avg_loss = total_loss / num_citations\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    all_logits = torch.cat(logits_list)\n",
    "    all_labels = torch.cat(labels_list)\n",
    "    retrieval_metrics = compute_retrieval_metrics(all_logits, all_labels, ks=k_values)\n",
    "    \n",
    "    # Clear temporary lists\n",
    "    del logits_list, labels_list\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'num_citations': num_citations,\n",
    "        'num_unique_targets': len(target_art_ids_unique),\n",
    "        'mrr': retrieval_metrics['mrr']\n",
    "    }\n",
    "    \n",
    "    # Add top-k accuracies\n",
    "    for k in k_values:\n",
    "        if f'top_{k}_accuracy' in retrieval_metrics:\n",
    "            results[f'top_{k}_accuracy'] = retrieval_metrics[f'top_{k}_accuracy']\n",
    "    \n",
    "    if return_embeddings:\n",
    "        results.update({\n",
    "            'cite_embeds': cite_embeds,\n",
    "            'ref_embeds': ref_embeds_unique.cpu(),\n",
    "            'cited_art_ids': cited_art_ids,\n",
    "            'target_art_ids': target_art_ids_unique,\n",
    "            'logits': all_logits,\n",
    "            'labels': labels\n",
    "        })\n",
    "    \n",
    "    # Final cleanup\n",
    "    del cite_embeds, ref_embeds, ref_embeds_unique\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # Model configuration\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    vocab_size: Optional[int] = None\n",
    "    initial_logit_scale: float = np.log(1/0.07)\n",
    "    \n",
    "    # Random seed configuration\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Token configuration\n",
    "    cite_token: str = \"<CITE>\"\n",
    "    ref_token: str = \"<REF>\"\n",
    "    cite_token_id: Optional[int] = None\n",
    "    ref_token_id: Optional[int] = None\n",
    "    \n",
    "    # Text processing configuration\n",
    "    max_length: int = 512\n",
    "    source_len: int = 512\n",
    "    target_len: int = 128\n",
    "    max_targets: int = 5\n",
    "    overlap: float = 0.5\n",
    "    \n",
    "    # Training configuration\n",
    "    num_epochs: int = 100\n",
    "    learning_rate: float = 1.5e-4\n",
    "    logits_learning_rate: float = 1.5e-2\n",
    "    max_grad_norm: float = 1.0\n",
    "    Adam_eps: float = 1e-8\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 0\n",
    "    batch_size: int = 200\n",
    "    train_ratio: float = 0.5\n",
    "    collate_sample_size: Optional[int] = None\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    k_values: List[int] = field(default_factory=lambda: [1, 5, 10, 50, 100, 1000])\n",
    "    \n",
    "    # Checkpoint configuration\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    checkpoint_every: int = 1000\n",
    "    project_name: str = \"citation-matching\"\n",
    "    run_name: Optional[str] = None\n",
    "    resume_from: Optional[str] = None\n",
    "    \n",
    "    # Hardware configuration\n",
    "    device: Optional[torch.device] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def get_checkpoint_dir(self) -> Path:\n",
    "        if self.project_name and self.run_name:\n",
    "            checkpoint_path = Path(self.checkpoint_dir) / self.project_name / self.run_name\n",
    "        elif self.project_name:\n",
    "            checkpoint_path = Path(self.checkpoint_dir) / self.project_name\n",
    "        else:\n",
    "            checkpoint_path = Path(self.checkpoint_dir)\n",
    "        checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        with open(path / \"config.yaml\", 'w') as f:\n",
    "            yaml.dump(asdict(self), f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> 'ExperimentConfig':\n",
    "        with open(path / \"config.yaml\", 'r') as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "        return cls(**config_dict)\n",
    "    \n",
    "    def set_seed(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.tokenizer.add_special_tokens({\n",
    "            'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "        })\n",
    "        \n",
    "        # Update config with tokenizer-dependent values\n",
    "        config.cite_token_id = self.tokenizer.convert_tokens_to_ids(config.cite_token)\n",
    "        config.ref_token_id = self.tokenizer.convert_tokens_to_ids(config.ref_token)\n",
    "        config.vocab_size = len(self.tokenizer)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = CitationModel(config)\n",
    "        \n",
    "        # Load checkpoint if specified\n",
    "        if config.resume_from:\n",
    "            self.load_checkpoint(config.resume_from)\n",
    "    \n",
    "    def get_checkpoint_path(self, step: Optional[int] = None, epoch: Optional[int] = None, is_best: bool = False) -> Path:\n",
    "        checkpoint_dir = self.config.get_checkpoint_dir()\n",
    "        \n",
    "        if is_best:\n",
    "            return checkpoint_dir / \"best_model.pt\"\n",
    "        elif step is not None:\n",
    "            return checkpoint_dir / f\"checkpoint-step-{step}.pt\"\n",
    "        elif epoch is not None:\n",
    "            return checkpoint_dir / f\"checkpoint-epoch-{epoch}.pt\"\n",
    "        else:\n",
    "            raise ValueError(\"Must specify either step, epoch, or is_best=True\")\n",
    "    \n",
    "    def save_checkpoint(self, \n",
    "                       path: Path, \n",
    "                       optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "                       scaler: Optional[GradScaler] = None,\n",
    "                       epoch: Optional[int] = None,\n",
    "                       batch_in_epoch: Optional[int] = None,\n",
    "                       global_step: Optional[int] = None,\n",
    "                       val_metrics: Optional[dict] = None,\n",
    "                       best_val_metrics: Optional[dict] = None,\n",
    "                       wandb_run_id: Optional[str] = None,\n",
    "                       is_best: bool = False):\n",
    "        \n",
    "        # Save RNG states as numpy arrays\n",
    "        rng_state = {\n",
    "            'python': random.getstate(),\n",
    "            'numpy': np.random.get_state(),\n",
    "            'torch': torch.get_rng_state().cpu().numpy(),\n",
    "            'cuda': torch.cuda.get_rng_state().cpu().numpy() if torch.cuda.is_available() else None\n",
    "        }\n",
    "        \n",
    "        # Prepare save dictionary\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'config': self.config,\n",
    "            'rng_state': rng_state,\n",
    "        }\n",
    "        \n",
    "        # Add optional states\n",
    "        if optimizer is not None:\n",
    "            save_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        if scaler is not None:\n",
    "            save_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "        if epoch is not None:\n",
    "            save_dict['epoch'] = epoch\n",
    "        if batch_in_epoch is not None:\n",
    "            save_dict['batch_in_epoch'] = batch_in_epoch\n",
    "        if global_step is not None:\n",
    "            save_dict['global_step'] = global_step\n",
    "        if val_metrics is not None:\n",
    "            save_dict['validation_metrics'] = val_metrics\n",
    "        if best_val_metrics is not None:\n",
    "            save_dict['best_val_metrics'] = best_val_metrics\n",
    "        if wandb_run_id is not None:\n",
    "            save_dict['wandb_run_id'] = wandb_run_id\n",
    "        \n",
    "        # Save checkpoint and config\n",
    "        torch.save(save_dict, path)\n",
    "        self.config.save(path.parent)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"\\nSaved new best model to {path}\")\n",
    "            if val_metrics:\n",
    "                print(\"Best validation metrics:\")\n",
    "                for metric in ['loss', 'accuracy', 'mrr']:\n",
    "                    if metric in val_metrics:\n",
    "                        print(f\"  {metric}: {val_metrics[metric]:.4f}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: Union[str, Path]) -> dict:\n",
    "        checkpoint_path = Path(checkpoint_path)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.config.device)\n",
    "        \n",
    "        # Load model state\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Load config if present\n",
    "        if 'config' in checkpoint:\n",
    "            resume_from = self.config.resume_from\n",
    "            self.config = checkpoint['config']\n",
    "            self.config.resume_from = resume_from\n",
    "        \n",
    "        # Restore RNG states\n",
    "        if 'rng_state' in checkpoint:\n",
    "            random.setstate(checkpoint['rng_state']['python'])\n",
    "            np.random.set_state(checkpoint['rng_state']['numpy'])\n",
    "            torch.set_rng_state(torch.tensor(checkpoint['rng_state']['torch'], dtype=torch.uint8))\n",
    "            if torch.cuda.is_available() and checkpoint['rng_state']['cuda'] is not None:\n",
    "                torch.cuda.set_rng_state(torch.tensor(checkpoint['rng_state']['cuda'], dtype=torch.uint8))\n",
    "\n",
    "        # Initialize missing fields with defaults if not present\n",
    "        default_fields = {\n",
    "            'optimizer_state_dict': None,\n",
    "            'scaler_state_dict': None,\n",
    "            'epoch': 0,\n",
    "            'batch_in_epoch': 0,\n",
    "            'global_step': 0,\n",
    "            'validation_metrics': None,\n",
    "            'best_val_metrics': {'loss': float('inf')},\n",
    "            'wandb_run_id': None\n",
    "        }\n",
    "        \n",
    "        for field, default_value in default_fields.items():\n",
    "            if field not in checkpoint:\n",
    "                checkpoint[field] = default_value\n",
    "        \n",
    "        return checkpoint\n",
    "    \n",
    "    def get_model(self) -> CitationModel:\n",
    "        return self.model\n",
    "    \n",
    "    def get_tokenizer(self) -> AutoTokenizer:\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def get_results(self, cache_path=None):\n",
    "        if cache_path:\n",
    "            results = tokenize_sources(cache_path=cache_path)\n",
    "        else:\n",
    "            preprocessor = WikiProcessor()\n",
    "            sources, citation_data = preprocessor.find_source_citations()\n",
    "            results = tokenize_sources(sources, citation_data, self.tokenizer, cache_dir=\"cache\")\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "def train_citation_model(\n",
    "    experiment: Experiment,\n",
    "    results: List[dict],\n",
    ") -> CitationModel:\n",
    "    \"\"\"\n",
    "    Memory-optimized training function with enhanced checkpoint management.\n",
    "    \"\"\"\n",
    "    import wandb\n",
    "    import gc\n",
    "    \n",
    "    config = experiment.config\n",
    "    model = experiment.model\n",
    "    tokenizer = experiment.tokenizer\n",
    "    \n",
    "    # Set random seeds\n",
    "    config.set_seed()\n",
    "    \n",
    "    # Initialize or resume wandb run\n",
    "    if config.resume_from:\n",
    "        checkpoint = experiment.load_checkpoint(config.resume_from)\n",
    "        wandb_run_id = checkpoint['wandb_run_id']\n",
    "        print(f\"Resuming wandb run: {wandb_run_id}\")\n",
    "        wandb.init(\n",
    "            project=config.project_name,\n",
    "            name=config.run_name,\n",
    "            id=wandb_run_id,\n",
    "            resume=\"must\"\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=config.project_name,\n",
    "            name=config.run_name,\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        # Update run name in config if not set\n",
    "        if not config.run_name:\n",
    "            config.run_name = wandb.run.name\n",
    "    \n",
    "    # Initialize training state\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    batch_in_epoch = 0\n",
    "    best_val_metrics = {'loss': float('inf')}\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Move model to device and enable memory efficient training\n",
    "    model = model.to(config.device)\n",
    "    model.transformer.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW([\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if n != 'logit_scale'],\n",
    "            'lr': config.learning_rate,\n",
    "            'weight_decay': config.weight_decay,\n",
    "            'eps': config.Adam_eps\n",
    "        },\n",
    "        {\n",
    "            'params': [model.logit_scale],\n",
    "            'lr': config.logits_learning_rate,\n",
    "            'weight_decay': 0\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # Load checkpoint state if resuming\n",
    "    if config.resume_from:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        global_step = checkpoint['global_step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        batch_in_epoch = checkpoint['batch_in_epoch']\n",
    "        best_val_metrics = checkpoint['best_val_metrics']\n",
    "        print(f\"Resumed from checkpoint at epoch {start_epoch}, batch {batch_in_epoch}, step {global_step}\")\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        \n",
    "        # Log current scale\n",
    "        current_scale = model.logit_scale.exp().item()\n",
    "        print(f\"Current logit scale: {current_scale:.4f}\")\n",
    "        wandb.log({\"logit_scale\": current_scale}, step=global_step)\n",
    "        \n",
    "        # Training data preparation\n",
    "        print(\"Collating training data with new random masks...\")\n",
    "        collated = collate(results, tokenizer, config)\n",
    "        dataset = CitationDataset(collated)\n",
    "        \n",
    "        # Create train/val split\n",
    "        indices = np.arange(len(dataset))\n",
    "        train_size = int(len(dataset) * config.train_ratio)\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "\n",
    "        from torch.utils.data import Subset\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(config.seed + epoch)\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=citation_collate_fn,\n",
    "            generator=generator\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config.batch_size * 1.8),\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            collate_fn=citation_collate_fn\n",
    "        )\n",
    "        \n",
    "        # Clear memory\n",
    "        del collated, dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Skip previously processed batches if resuming\n",
    "            if epoch == start_epoch and batch_idx < batch_in_epoch:\n",
    "                continue\n",
    "            \n",
    "            batch = {k: v.to(config.device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if config.max_grad_norm:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update tracking\n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Log metrics\n",
    "            wandb.log({\n",
    "                \"train/batch_loss\": loss.item(),\n",
    "                'logit_scale': model.logit_scale.item(),\n",
    "                \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"train/batch_in_epoch\": batch_idx,\n",
    "                \"epoch\": epoch\n",
    "            }, step=global_step)\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if global_step > 0 and global_step % config.checkpoint_every == 0:\n",
    "                checkpoint_path = experiment.get_checkpoint_path(step=global_step)\n",
    "                experiment.save_checkpoint(\n",
    "                    checkpoint_path,\n",
    "                    optimizer=optimizer,\n",
    "                    scaler=scaler,\n",
    "                    epoch=epoch,\n",
    "                    batch_in_epoch=batch_idx,\n",
    "                    global_step=global_step,\n",
    "                    wandb_run_id=wandb.run.id\n",
    "                )\n",
    "                print(f\"\\nSaved checkpoint at step {global_step} to {checkpoint_path}\")\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Clear memory\n",
    "            del outputs, loss, batch\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Log epoch-level training metrics\n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        wandb.log({\n",
    "            \"train/epoch_loss\": avg_train_loss,\n",
    "            \"epoch\": epoch\n",
    "        }, step=global_step)\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"\\nRunning validation...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            val_metrics = validate_citation_model(\n",
    "                model=model,\n",
    "                val_dataloader=val_dataloader,\n",
    "                device=config.device,\n",
    "                k_values=config.k_values\n",
    "            )\n",
    "        \n",
    "        # Log validation metrics\n",
    "        wandb_val_metrics = {\n",
    "            \"val/loss\": val_metrics['loss'],\n",
    "            \"val/accuracy\": val_metrics['accuracy'],\n",
    "            \"val/mrr\": val_metrics['mrr']\n",
    "        }\n",
    "        \n",
    "        for k in config.k_values:\n",
    "            if f'top_{k}_accuracy' in val_metrics:\n",
    "                wandb_val_metrics[f\"val/top_{k}_accuracy\"] = val_metrics[f'top_{k}_accuracy']\n",
    "        \n",
    "        wandb.log(wandb_val_metrics, step=global_step)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"\\nValidation metrics:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save best model if validation loss improved\n",
    "        if val_metrics['loss'] < best_val_metrics['loss']:\n",
    "            best_val_metrics = val_metrics\n",
    "            best_model_path = experiment.get_checkpoint_path(is_best=True)\n",
    "            experiment.save_checkpoint(\n",
    "                best_model_path,\n",
    "                optimizer=optimizer,\n",
    "                scaler=scaler,\n",
    "                epoch=epoch,\n",
    "                batch_in_epoch=batch_idx,\n",
    "                global_step=global_step,\n",
    "                val_metrics=val_metrics,\n",
    "                best_val_metrics=best_val_metrics,\n",
    "                wandb_run_id=wandb.run.id,\n",
    "                is_best=True\n",
    "            )\n",
    "            \n",
    "            # Update wandb summary with best metrics\n",
    "            wandb.run.summary.update({\n",
    "                \"best_val_loss\": val_metrics['loss'],\n",
    "                \"best_val_accuracy\": val_metrics['accuracy'],\n",
    "                \"best_val_mrr\": val_metrics['mrr'],\n",
    "                \"best_model_epoch\": epoch,\n",
    "                \"best_model_step\": global_step\n",
    "            })\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        epoch_checkpoint_path = experiment.get_checkpoint_path(epoch=epoch)\n",
    "        experiment.save_checkpoint(\n",
    "            epoch_checkpoint_path,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            batch_in_epoch=batch_idx,\n",
    "            global_step=global_step,\n",
    "            val_metrics=val_metrics,\n",
    "            best_val_metrics=best_val_metrics,\n",
    "            wandb_run_id=wandb.run.id\n",
    "        )\n",
    "        \n",
    "        # Clear memory after each epoch\n",
    "        del val_metrics, train_dataloader, val_dataloader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    wandb.finish()\n",
    "    return model\n",
    "\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    project_name=\"citation-matching\",\n",
    "    run_name=None,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    checkpoint_every=100,\n",
    "    seed=42,\n",
    "    collate_sample_size=100000,\n",
    "    batch_size=270,\n",
    "    initial_logit_scale=np.log(1/0.07),\n",
    "    train_ratio=.5,\n",
    "    logits_learning_rate=1e-2,\n",
    "    max_grad_norm=0.5\n",
    ")\n",
    "\n",
    "experiment = Experiment(config)\n",
    "results = experiment.get_results(cache_path='./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt')\n",
    "\n",
    "# Train from scratch\n",
    "# trained_model = train_citation_model(experiment, results)\n",
    "\n",
    "# Or resume from checkpoint\n",
    "config.resume_from = \"checkpoints/citation-matching/feasible-pine-64/checkpoint-step-60.pt\"\n",
    "experiment = Experiment(config)\n",
    "trained_model = train_citation_model(experiment, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5cd4d7-49b8-4c0f-b735-a34eb61912a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
