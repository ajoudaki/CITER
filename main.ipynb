{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e45919-7df5-473e-991b-8c2f6adfcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-10 13:13:49,247 - INFO - Loading articles from JSONL file...\n",
      "2024-11-10 13:13:53,468 - INFO - Loaded 237381 articles\n",
      "2024-11-10 13:13:53,469 - INFO - Preparing training data...\n",
      "2024-11-10 13:13:54,196 - INFO - Preparing validation data...\n",
      "2024-11-10 13:13:55,080 - INFO - Using device: cuda\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Processing samples: 100%|██████████| 2/2 [00:11<00:00,  5.87s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1213 samples\n",
      "Skipped 757 samples without citation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 3/3 [00:11<00:00,  3.72s/batch]\n",
      "2024-11-10 13:14:19,150 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1360 samples\n",
      "Skipped 1194 samples without citation\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:49<00:00,  1.31s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Training Loss: 0.9453\n",
      "Validation Loss: 1.5942\n",
      "Best Top-1 Accuracy: 0.2846\n",
      "Mean Reciprocal Rank: 0.4319\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.285\n",
      "Top-5 Accuracy: 0.612\n",
      "Top-10 Accuracy: 0.726\n",
      "Top-50 Accuracy: 0.900\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:49<00:00,  1.30s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Training Loss: 0.2869\n",
      "Validation Loss: 1.6976\n",
      "Best Top-1 Accuracy: 0.2691\n",
      "Mean Reciprocal Rank: 0.4126\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.269\n",
      "Top-5 Accuracy: 0.577\n",
      "Top-10 Accuracy: 0.701\n",
      "Top-50 Accuracy: 0.892\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:49<00:00,  1.31s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Training Loss: 0.1870\n",
      "Validation Loss: 1.7977\n",
      "Best Top-1 Accuracy: 0.2478\n",
      "Mean Reciprocal Rank: 0.3928\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.248\n",
      "Top-5 Accuracy: 0.569\n",
      "Top-10 Accuracy: 0.673\n",
      "Top-50 Accuracy: 0.881\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Training Loss: 0.1536\n",
      "Validation Loss: 1.8233\n",
      "Best Top-1 Accuracy: 0.2529\n",
      "Mean Reciprocal Rank: 0.3970\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.253\n",
      "Top-5 Accuracy: 0.571\n",
      "Top-10 Accuracy: 0.686\n",
      "Top-50 Accuracy: 0.888\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Training Loss: 0.1209\n",
      "Validation Loss: 1.7661\n",
      "Best Top-1 Accuracy: 0.2676\n",
      "Mean Reciprocal Rank: 0.4162\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.268\n",
      "Top-5 Accuracy: 0.601\n",
      "Top-10 Accuracy: 0.704\n",
      "Top-50 Accuracy: 0.897\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Training Loss: 0.0823\n",
      "Validation Loss: 1.7657\n",
      "Best Top-1 Accuracy: 0.2551\n",
      "Mean Reciprocal Rank: 0.4033\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.255\n",
      "Top-5 Accuracy: 0.594\n",
      "Top-10 Accuracy: 0.696\n",
      "Top-50 Accuracy: 0.893\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Training Loss: 0.0735\n",
      "Validation Loss: 1.7819\n",
      "Best Top-1 Accuracy: 0.2669\n",
      "Mean Reciprocal Rank: 0.4148\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.267\n",
      "Top-5 Accuracy: 0.595\n",
      "Top-10 Accuracy: 0.714\n",
      "Top-50 Accuracy: 0.896\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Training Loss: 0.0619\n",
      "Validation Loss: 1.7203\n",
      "Best Top-1 Accuracy: 0.2699\n",
      "Mean Reciprocal Rank: 0.4208\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.270\n",
      "Top-5 Accuracy: 0.618\n",
      "Top-10 Accuracy: 0.726\n",
      "Top-50 Accuracy: 0.900\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Training Loss: 0.0558\n",
      "Validation Loss: 1.7035\n",
      "Best Top-1 Accuracy: 0.2713\n",
      "Mean Reciprocal Rank: 0.4231\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.271\n",
      "Top-5 Accuracy: 0.618\n",
      "Top-10 Accuracy: 0.732\n",
      "Top-50 Accuracy: 0.901\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38/38 [00:50<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n",
      "2024-11-10 13:25:55,518 - INFO - Training completed. Results saved to training_runs/run_20241110_131348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "Training Loss: 0.0488\n",
      "Validation Loss: 1.7250\n",
      "Best Top-1 Accuracy: 0.2654\n",
      "Mean Reciprocal Rank: 0.4211\n",
      "Validation Size: 1360\n",
      "Top-1 Accuracy: 0.265\n",
      "Top-5 Accuracy: 0.618\n",
      "Top-10 Accuracy: 0.726\n",
      "Top-50 Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from wiki_citations import CitationDataPreprocessor\n",
    "\n",
    "from modeling import (\n",
    "    ModelConfig,\n",
    "    CitationMatcher,\n",
    "    CitationDataset,\n",
    "    create_dataloader\n",
    ")\n",
    "from training import (\n",
    "    TrainingConfig,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "def setup_logging(output_dir: Path) -> None:\n",
    "    \"\"\"Configure logging for the training process.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(output_dir / 'training.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def setup_environment() -> None:\n",
    "    \"\"\"Configure training environment.\"\"\"\n",
    "    # Set environment variables\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "def create_output_directory() -> Path:\n",
    "    \"\"\"Create and return output directory for this training run.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(f\"training_runs/run_{timestamp}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def load_and_prepare_data(\n",
    "    jsonl_path: str,\n",
    "    train_sample_size: int,\n",
    "    val_sample_size: int\n",
    ") -> tuple:\n",
    "    \"\"\"Load and prepare training and validation data.\"\"\"\n",
    "    logging.info(\"Loading articles from JSONL file...\")\n",
    "    articles_dict = {}\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            article = json.loads(line)\n",
    "            articles_dict[article['title'].lower()] = article['text']\n",
    "    \n",
    "    logging.info(f\"Loaded {len(articles_dict)} articles\")\n",
    "    \n",
    "    # Prepare citation data\n",
    "    preprocessor = CitationDataPreprocessor(articles_dict)\n",
    "    \n",
    "    logging.info(\"Preparing training data...\")\n",
    "    train_sources, train_targets = preprocessor.create_citation_pairs(\n",
    "        sample_size=train_sample_size,\n",
    "        cite_samples_per_article=1\n",
    "    )\n",
    "\n",
    "    S = set(train_sources)\n",
    "    T = set(train_targets)\n",
    "    \n",
    "    logging.info(\"Preparing validation data...\")\n",
    "    val_sources, val_targets = preprocessor.create_citation_pairs(\n",
    "        sample_size=val_sample_size,\n",
    "        cite_samples_per_article=10\n",
    "    )\n",
    "\n",
    "    # Remove any validation samples that are also in the training set\n",
    "    val_sources, val_targets = zip(*[\n",
    "        (source, target) for source, target in zip(val_sources, val_targets)\n",
    "        if source not in S and target not in T\n",
    "    ])\n",
    "    \n",
    "    return train_sources, train_targets, val_sources, val_targets\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    output_dir = create_output_directory()\n",
    "    setup_logging(output_dir)\n",
    "    setup_environment()\n",
    "    \n",
    "    # Configuration\n",
    "    model_config = ModelConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        max_length=512,\n",
    "        cite_token=\"<CITE>\",\n",
    "        ref_token=\"<REF>\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        learning_rate=1.5e-4,\n",
    "        temperature=0.1,\n",
    "        num_workers=4,\n",
    "        gradient_clip_value=1.0,\n",
    "        scheduler_patience=2,\n",
    "        scheduler_factor=0.5,\n",
    "        eval_k_values=[1, 5, 10, 50]\n",
    "    )\n",
    "    \n",
    "    # Data preparation\n",
    "    train_sources, train_targets, val_sources, val_targets = load_and_prepare_data(\n",
    "        jsonl_path='./data/wiki_articles.jsonl',\n",
    "        train_sample_size=1000,\n",
    "        val_sample_size=100\n",
    "    )\n",
    "    \n",
    "    # Model initialization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    model = CitationMatcher(model_config).to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = CitationDataset(\n",
    "        sources=train_sources,\n",
    "        targets=train_targets,\n",
    "        tokenizer=model.tokenizer,\n",
    "        config=model_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = CitationDataset(\n",
    "        sources=val_sources,\n",
    "        targets=val_targets,\n",
    "        tokenizer=model.tokenizer,\n",
    "        config=model_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_loader = create_dataloader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=training_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=training_config.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = create_dataloader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=training_config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=training_config.num_workers\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    logging.info(\"Starting training...\")\n",
    "    metrics_history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=training_config,\n",
    "        save_dir=output_dir / 'checkpoints',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Save training history\n",
    "    torch.save(\n",
    "        {\n",
    "            'metrics_history': [metric.__dict__ for metric in metrics_history],\n",
    "            'model_config': model_config.__dict__,\n",
    "            'training_config': training_config.__dict__\n",
    "        },\n",
    "        output_dir / 'training_history.pt'\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Training completed. Results saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during training:\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998296a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ba1cc-d1d3-4cf5-9fbb-e82f0e4186a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb302bc-5802-4f5d-959c-4bdf4c9acdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f27a2-64ab-4277-ad92-4dc3b610b132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
