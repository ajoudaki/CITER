{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:root:Loading cached tokenized results from ./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt\n",
      "/tmp/ipykernel_2203108/1310817513.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load('./experiments/best_citation_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2203108/615699325.py:732: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "temperature changed to 1\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████                                                                                                              | 8543/237381 [00:13<06:10, 617.39it/s]\n",
      "Training:   0%|                                                                                                                       | 0/126 [00:00<?, ?it/s]/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 126/126 [07:12<00:00,  3.43s/it, loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 5.8528\n",
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [04:22<00:00,  4.17s/it]\n",
      "Computing similarities: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "  Loss: 9.8985\n",
      "  Accuracy (top-1): 0.0224\n",
      "  Mean Reciprocal Rank: 0.0271\n",
      "  Number of citations: 120513\n",
      "  Number of unique targets: 30269\n",
      "\n",
      "Top-k accuracy:\n",
      "  k=1: 0.0063\n",
      "  k=5: 0.0332\n",
      "  k=10: 0.0657\n",
      "  k=50: 0.1975\n",
      "  k=100: 0.2637\n",
      "  k=1000: 0.4346\n",
      "\n",
      "Saved new best model to ./experiments/best_citation_model.pt\n",
      "Best validation metrics so far:\n",
      "  Loss: 9.8985\n",
      "  Accuracy (top-1): 0.0224\n",
      "  MRR: 0.0271\n",
      "\n",
      "Epoch 2/15\n",
      "temperature changed to 0.5\n",
      "Collating training data with new random masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████                                                                                                              | 8543/237381 [00:13<06:09, 619.19it/s]\n",
      "Training:  20%|███████████████████▋                                                                               | 25/126 [01:30<06:05,  3.62s/it, loss=5.11]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacity of 23.59 GiB of which 572.19 MiB is free. Including non-PyTorch memory, this process has 20.03 GiB memory in use. Process 2204102 has 2.97 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 5.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(loaded_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Usage example:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Replace the previous training code with:\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_citation_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass raw results instead of dataloader\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./experiments/best_citation_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.07\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.06\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 803\u001b[0m, in \u001b[0;36mtrain_citation_model\u001b[0;34m(model, results, tokenizer, config, train_ratio, num_epochs, learning_rate, weight_decay, warmup_steps, device, save_path, batch_size, temperatures, k_values)\u001b[0m\n\u001b[1;32m    800\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Backward pass with gradient scaling\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m    805\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/function.py:306\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:313\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    315\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 0 has a total capacity of 23.59 GiB of which 572.19 MiB is free. Including non-PyTorch memory, this process has 20.03 GiB memory in use. Process 2204102 has 2.97 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 5.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "            \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # Load articles\n",
    "# preprocessor = WikiProcessor()\n",
    "# sources, citation_data = preprocessor.find_source_citations()\n",
    "\n",
    "config = ExperimentConfig(collate_sample_size=50000,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': [config.cite_token, config.ref_token]\n",
    "})\n",
    "\n",
    "\n",
    "# results = tokenize_sources(sources, citation_data, tokenizer, cache_dir=\"cache\",)\n",
    "\n",
    "# This will now use caching directly \n",
    "results = tokenize_sources(cache_path='./cache/tokenized_1caf5def_eb27a5477eaa3d549aebc4886f3717d1.pt')\n",
    "\n",
    "\n",
    "# Create model config\n",
    "model_config = CitationConfig(\n",
    "    base_model_name=config.model_name,\n",
    "    vocab_size=len(tokenizer),\n",
    "    cite_token_id=tokenizer.convert_tokens_to_ids(config.cite_token),\n",
    "    ref_token_id=tokenizer.convert_tokens_to_ids(config.ref_token),\n",
    "    temperature=config.temperature,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = CitationModel(model_config)\n",
    "\n",
    "loaded_model = torch.load('./experiments/best_citation_model.pt', weights_only=True)\n",
    "\n",
    "model.load_state_dict(loaded_model['model_state_dict'])\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Replace the previous training code with:\n",
    "trained_model = train_citation_model(\n",
    "    model=model,\n",
    "    results=results,  # Pass raw results instead of dataloader\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    "    num_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1.5e-4,\n",
    "    batch_size=200,\n",
    "    train_ratio = 0.5,\n",
    "    save_path=\"./experiments/best_citation_model.pt\",\n",
    "    temperatures = [1, .5,.2, .1, 0.07, 0.06, 0.05],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1688960/73700011.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load('./experiments/best_citation_model.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useless =  0.1\n",
      "Source original: 1:\n",
      "'''April''' (Apr.) is the fourth [[month]] of the [[year]] in the [[Julian calendar|Julian]] and [[Gregorian calendar]]s, and comes between [[March]] and [[May]]. It is one of four months to have 30 [[day]]s.\n",
      "April always begins on the same day of the week as [[July]], and additionally, [[January]] in leap years. April always ends on the same day of the week as [[December]].\n",
      "== The Month ==\n",
      "April comes between [[March]] and [[May]], making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as [[June]], [[September]] and [[November]] are later in the year.\n",
      "April begins on the same day of the week as [[July]] every year and on the same day of the week as [[January]] in [[leap year]]s. April ends on the same day of the week as [[December]] every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
      "In [[common year]]s, April starts on the same day of the week as [[October]] of the previous year, and in [[leap year]]s\n",
      "\n",
      "\n",
      "##################################################\n",
      "Source tokens decoded:\n",
      "' ' ' april ' ' ' ( apr. ) is the fourth month of the year in the julian calendar | julian and gregorian calendar s, and comes between march and may. it is one of four months to have 30 <CITE> s. april always begins on the same day of the week as july, and additionally, january in leap years. april always ends on the same day of the week as <CITE>. = = the month = = april comes between march and may, making it the fourth month of the year. it also comes first in the year out of the four months that have 30 days, as june, <CITE> and november are later in the year. april begins on the same day of the week as july every year and on the same day of the week as january in <CITE> s. april ends on the same day of the week as <CITE> every year, as each other ' s last days are exactly 35 weeks ( 245 days ) apart. in common year s, april starts on the same day of the week as <CITE> of the previous year, and in <CITE> s, may of the previous year. in common years, april finishes on the same day of the week as july of the previous year, and in leap years, february and <CITE> of the previous year. in common years immediately after other common years, april starts on the same day of the week as january of the previous year, and in leap years and years immediately after that, april finishes on the same day of the week as january of the previous year. in years immediately before common years, april starts on the same day of the week as <CITE> and <CITE> of the following year, and in years immediately before leap years, june of the following year. in years immediately before common years, april finishes on the same day of the week as september of the following year, and in years immediately before leap years, march and june of the following year. april [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n",
      "Source attention mask: 126, \n",
      "Target: id=357:\n",
      "'''September''' (Sep.) is the ninth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[August]] and [[October]]. It has 30 [[day]]s. Its name comes from the [[Latin]] word ''sep...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' september ' ' ' ( sep. ) is the ninth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ august ] ] and [ [ october ] ]. it has 30 [ [ day ] ] s. its name comes from the [ [ latin ] ] word ' ' sept ' ' for \" seven \" ( it was the seventh month of the year, before [ [ january ] ] and [ [ february <REF>\n",
      "\n",
      "\n",
      "Target: id=832:\n",
      "'''day''' is the time it takes the [[Earth]] to spin around once. It is day time on the side of the Earth that is facing the [[Sun]]. When it is [[wikt:night|night]] time, that side of the Earth is fa...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' day ' ' ' is the time it takes the [ [ earth ] ] to spin around once. it is day time on the side of the earth that is facing the [ [ sun ] ]. when it is [ [ wikt : night | night ] ] time, that side of the earth is facing away from the sun. it takes 24 [ [ hour ] ] s for the earth to spin once, so that is one day, including the day time and night time <REF>\n",
      "\n",
      "\n",
      "Target: id=100:\n",
      "'''December''' (Dec.) is the twelfth and last [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[November]] (of the current year) and [[January]] (of the following year). It has...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' december ' ' ' ( dec. ) is the twelfth and last [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ november ] ] ( of the current year ) and [ [ january ] ] ( of the following year ). it has 31 days. with the name of the month coming from the [ [ latin ] ] ' ' decem ' ' for \" ten \", it was the tenth month of <REF>\n",
      "\n",
      "\n",
      "Target: id=297:\n",
      "'''October''' (Oct.) is the tenth [[month]] of the [[year]] in the [[Gregorian calendar]], coming between [[September]] and [[November]]. It has 31 [[day]]s. The name comes from the [[Latin]] ''octo''...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' october ' ' ' ( oct. ) is the tenth [ [ month ] ] of the [ [ year ] ] in the [ [ gregorian calendar ] ], coming between [ [ september ] ] and [ [ november ] ]. it has 31 [ [ day ] ] s. the name comes from the [ [ latin ] ] ' ' octo ' ' for \" eight \". it was the eighth month of the year before [ [ january ] ] and [ [ february ] <REF>\n",
      "\n",
      "\n",
      "Target: id=228:\n",
      "'''leap year''' is a [[calendar year]] in which an extra [[day]] is added to the [[Gregorian calendar]], which is used by most of the world. A [[common year]] has 365 [[day]]s, but a leap year has 366...\n",
      "\n",
      "\n",
      "Target tokens:\n",
      "' ' ' leap year ' ' ' is a [ [ calendar year ] ] in which an extra [ [ day ] ] is added to the [ [ gregorian calendar ] ], which is used by most of the world. a [ [ common year ] ] has 365 [ [ day ] ] s, but a leap year has 366 days. the extra day, february 29, is added to the [ [ month ] ] of [ [ february ] ]. in a common year, february <REF>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "source_art_id = sample['source_art_id']\n",
    "original_source = sources[source_art_id-1]\n",
    "source_text = tokenizer.decode(sample['source_ids'], )\n",
    "cited_art_ids = sample['cited_art_ids']\n",
    "\n",
    "useless_chars = np.sum([c==']' for c in source_text])*2/len(source_text)\n",
    "print('useless = ', useless_chars)\n",
    "print(f\"Source original: {source_art_id}:\\n{original_source[:1000]}\\n\\n\")\n",
    "print('#'*50)\n",
    "print(f\"Source tokens decoded:\\n{source_text[:]}\\n\\n\")\n",
    "print(f\"Source attention mask: {(sample['attention_mask']==0).sum()}, \")\n",
    "\n",
    "for i, target_art_id in enumerate(sample['target_art_ids']):\n",
    "    target_art_ref = preprocessor.id2ref[target_art_id.item()]\n",
    "    target_original = sources[target_art_id-1]\n",
    "    target_text = tokenizer.decode(sample['target_ids'][i], )\n",
    "    print(f\"Target: id={target_art_id}:\\n{target_original[:200]}...\\n\\n\")\n",
    "    print(f\"Target tokens:\\n{target_text[:]}\\n\\n\")\n",
    "target_art_ids = sample['target_art_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
