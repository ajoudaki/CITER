{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e45919-7df5-473e-991b-8c2f6adfcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-09 19:24:50,307 - INFO - Loading articles from JSONL file...\n",
      "2024-11-09 19:24:59,222 - INFO - Loaded 237381 articles\n",
      "2024-11-09 19:24:59,223 - INFO - Preparing training data...\n",
      "2024-11-09 19:25:02,239 - INFO - Preparing validation data...\n",
      "2024-11-09 19:25:08,821 - INFO - Using device: cuda\n",
      "/home/amir/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 samples...\n",
      "Preprocessing stats: {'skipped_no_cite': 6229, 'skipped_errors': 0, 'processed': 13349}\n",
      "Processed 10000 samples...\n",
      "Processed 20000 samples...\n",
      "Processed 30000 samples...\n",
      "Processed 40000 samples...\n",
      "Processed 50000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 19:38:17,760 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing stats: {'skipped_no_cite': 21500, 'skipped_errors': 0, 'processed': 37531}\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 418/418 [09:12<00:00,  1.32s/it, loss=0.0201, avg_loss=0.5933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing validation embeddings: 100%|████████| 587/587 [08:36<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing rankings for 37531 samples...\n",
      "\n",
      "Validation Metrics:\n",
      "top_k_accuracy: {1: 0.07204710772428126, 3: 0.15291359143108363, 5: 0.20159334949774851, 10: 0.2820601635980922, 50: 0.52745730196371}\n",
      "mrr: 0.1430\n",
      "median_rank: 43.0000\n",
      "mean_rank: 466.2326\n",
      "val_size: 37531\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Training Loss: 0.5933\n",
      "Validation Loss: 1.4167\n",
      "Best Top-1 Accuracy: 0.0720\n",
      "Mean Reciprocal Rank: 0.1430\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 418/418 [09:15<00:00,  1.33s/it, loss=0.0368, avg_loss=0.3432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing validation embeddings: 100%|████████| 587/587 [08:37<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing rankings for 37531 samples...\n",
      "\n",
      "Validation Metrics:\n",
      "top_k_accuracy: {1: 0.07122112387093336, 3: 0.14513335642535505, 5: 0.19386640377288109, 10: 0.27667794623111563, 50: 0.5266046734699316}\n",
      "mrr: 0.1397\n",
      "median_rank: 43.0000\n",
      "mean_rank: 521.0886\n",
      "val_size: 37531\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Training Loss: 0.3432\n",
      "Validation Loss: 1.4627\n",
      "Best Top-1 Accuracy: 0.0712\n",
      "Mean Reciprocal Rank: 0.1397\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|  | 21/418 [00:27<08:48,  1.33s/it, loss=0.2330, avg_loss=0.2555]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "from data_processing import (\n",
    "    ArticleContentProcessor,\n",
    "    CitationDataPreprocessor\n",
    ")\n",
    "from model_architecture import (\n",
    "    ModelConfig,\n",
    "    CitationMatcher,\n",
    "    CitationDataset,\n",
    "    create_dataloader\n",
    ")\n",
    "from training_module import (\n",
    "    TrainingConfig,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "def setup_logging(output_dir: Path) -> None:\n",
    "    \"\"\"Configure logging for the training process.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(output_dir / 'training.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def setup_environment() -> None:\n",
    "    \"\"\"Configure training environment.\"\"\"\n",
    "    # Set environment variables\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "def create_output_directory() -> Path:\n",
    "    \"\"\"Create and return output directory for this training run.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(f\"training_runs/run_{timestamp}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def load_and_prepare_data(\n",
    "    jsonl_path: str,\n",
    "    train_sample_size: int,\n",
    "    val_sample_size: int\n",
    ") -> tuple:\n",
    "    \"\"\"Load and prepare training and validation data.\"\"\"\n",
    "    logging.info(\"Loading articles from JSONL file...\")\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        articles_dict = {}\n",
    "        for line in f:\n",
    "            article = json.loads(line)\n",
    "            content = ArticleContentProcessor.clean_wiki_content(article['text'])\n",
    "            if content:\n",
    "                articles_dict[article['title'].lower()] = content\n",
    "    \n",
    "    logging.info(f\"Loaded {len(articles_dict)} articles\")\n",
    "    \n",
    "    # Prepare citation data\n",
    "    preprocessor = CitationDataPreprocessor(articles_dict)\n",
    "    \n",
    "    logging.info(\"Preparing training data...\")\n",
    "    train_sources, train_targets = preprocessor.create_citation_pairs(\n",
    "        sample_size=train_sample_size,\n",
    "        cite_samples_per_article=1\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Preparing validation data...\")\n",
    "    val_sources, val_targets = preprocessor.create_citation_pairs(\n",
    "        sample_size=val_sample_size,\n",
    "        cite_samples_per_article=10\n",
    "    )\n",
    "    \n",
    "    return train_sources, train_targets, val_sources, val_targets\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    output_dir = create_output_directory()\n",
    "    setup_logging(output_dir)\n",
    "    setup_environment()\n",
    "    \n",
    "    # Configuration\n",
    "    model_config = ModelConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        max_length=512,\n",
    "        cite_token=\"<CITE>\",\n",
    "        ref_token=\"<REF>\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        learning_rate=1.5e-4,\n",
    "        temperature=0.1,\n",
    "        num_workers=4,\n",
    "        gradient_clip_value=1.0,\n",
    "        scheduler_patience=2,\n",
    "        scheduler_factor=0.5,\n",
    "        eval_k_values=[1, 3, 5, 10, 50]\n",
    "    )\n",
    "    \n",
    "    # Data preparation\n",
    "    train_sources, train_targets, val_sources, val_targets = load_and_prepare_data(\n",
    "        jsonl_path='./data/wiki_articles.jsonl',\n",
    "        train_sample_size=50000,\n",
    "        val_sample_size=1000\n",
    "    )\n",
    "    \n",
    "    # Model initialization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    model = CitationMatcher(model_config).to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = CitationDataset(\n",
    "        sources=train_sources,\n",
    "        targets=train_targets,\n",
    "        tokenizer=model.tokenizer,\n",
    "        config=model_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = CitationDataset(\n",
    "        sources=val_sources,\n",
    "        targets=val_targets,\n",
    "        tokenizer=model.tokenizer,\n",
    "        config=model_config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_loader = create_dataloader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=training_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=training_config.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = create_dataloader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=training_config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=training_config.num_workers\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    logging.info(\"Starting training...\")\n",
    "    metrics_history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=training_config,\n",
    "        save_dir=output_dir / 'checkpoints',\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Save training history\n",
    "    torch.save(\n",
    "        {\n",
    "            'metrics_history': [metric.__dict__ for metric in metrics_history],\n",
    "            'model_config': model_config.__dict__,\n",
    "            'training_config': training_config.__dict__\n",
    "        },\n",
    "        output_dir / 'training_history.pt'\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Training completed. Results saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.exception(\"An error occurred during training:\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ba1cc-d1d3-4cf5-9fbb-e82f0e4186a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb302bc-5802-4f5d-959c-4bdf4c9acdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f27a2-64ab-4277-ad92-4dc3b610b132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
