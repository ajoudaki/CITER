{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c30ca1-6c0b-40eb-b5c2-2a662dc4b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d873dd76412d4c14bbe731e7ae095e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ced792b46f4a958d2d1684a297bb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents before preprocessing: 3236\n",
      "Number of documents after preprocessing: 474\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm.auto import tqdm \n",
    "import os\n",
    "\n",
    "\n",
    "def preprocess_latex_corpus(corpus):\n",
    "    citation_id = 1\n",
    "    citation_map = {}  # To keep track of the citation to id mapping\n",
    "\n",
    "    processed_corpus = []\n",
    "\n",
    "    # Define the regex patterns\n",
    "    citation_pattern = re.compile(r\"\\\\cite[t,p]?{(.*?)}\")\n",
    "    command_pattern = re.compile(r\"\\\\[a-zA-Z]+\")\n",
    "    comment_pattern = re.compile(r\"%.*?$\", re.MULTILINE)\n",
    "    environment_pattern = re.compile(r\"\\\\begin{(figure|table|equation).*?\\\\end{\\1}\", re.DOTALL)\n",
    "\n",
    "    for document in tqdm(corpus):\n",
    "        # Remove comments\n",
    "        document = re.sub(comment_pattern, \"\", document)\n",
    "\n",
    "        # Remove non-informative environments\n",
    "        # document = re.sub(environment_pattern, \"\", document)\n",
    "\n",
    "        # Replace citation commands\n",
    "        matches = citation_pattern.findall(document)\n",
    "        for match in matches:\n",
    "            citations = match.split(\",\")  # Handle multiple citations within one command\n",
    "            for citation in citations:\n",
    "                citation = citation.strip()\n",
    "                \n",
    "                # Assign a unique id to each citation if not already done & if not empty\n",
    "                if citation not in citation_map and citation != \"\":\n",
    "                    citation_map[citation] = f\"<CITATION_{citation_id}>\"\n",
    "                    citation_id += 1\n",
    "                \n",
    "                    # Replace the citation with the special token\n",
    "                    document = document.replace(citation, citation_map[citation])\n",
    "\n",
    "        # Remove other commands\n",
    "        # document = re.sub(command_pattern, \"\", document)\n",
    "        document = document.replace('{<','<').replace('>}','>')\n",
    "        if r'\\begin{document}' not in document:\n",
    "            continue\n",
    "        document = document.split(r'\\begin{document}')[1] # get the body\n",
    "        processed_corpus.append(document)\n",
    "\n",
    "    return processed_corpus, citation_map\n",
    "\n",
    "\n",
    "def read_tex_files(directory):\n",
    "    corpus = []\n",
    "    for root, dirs, files in tqdm(os.walk(directory)):\n",
    "        try:\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.tex'):\n",
    "                    # print(file_name)\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        corpus.append(content)\n",
    "        except:\n",
    "            pass\n",
    "    return corpus \n",
    "\n",
    "corpus = read_tex_files('./sources/')\n",
    "processed_corpus, citation_map = preprocess_latex_corpus(corpus)\n",
    "\n",
    "# pint number of corpus before & after preprocessing\n",
    "print(f'Number of documents before preprocessing: {len(corpus)}')\n",
    "print(f'Number of documents after preprocessing: {len(processed_corpus)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebbbfd-935a-4c7e-9ab2-3717e4ce2336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c95c41a3-3b29-4deb-ae55-215f19fc1feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0558cc7aca6c467db79f051b03a05766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1264 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07537f299d2744b8b5f9cc0fa92e6cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class CitationDataset(Dataset):\n",
    "    def __init__(self, text_list, tokenizer, seq_len=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in tqdm(text_list):\n",
    "            self.process_text(text)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        tokenized_text = self.tokenizer.encode(text)\n",
    "        # citation_mask = tokenized_text.ge(self.tokenizer.additional_special_tokens_ids[0])  # Shape: (batch_size, seq_len)\n",
    "        # if citation_mask.sum() == 0: # No citations in the text\n",
    "        #     return\n",
    "\n",
    "        # Create sequences of the specified length with stride half of the length\n",
    "        for i in range(0, len(tokenized_text)-self.seq_len, self.seq_len//2):\n",
    "            if i+self.seq_len+1 >= len(tokenized_text):\n",
    "                break\n",
    "            input_sequence = tokenized_text[i:i+self.seq_len]\n",
    "            target_sequence = tokenized_text[i+1:i+self.seq_len+1]  # Shifted right\n",
    "\n",
    "            self.inputs.append(torch.tensor(input_sequence))\n",
    "            self.targets.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "# Add the special tokens to the pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens_dict = {'additional_special_tokens': list(citation_map.values())}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "\n",
    "# Split the processed_corpus into training and validation sets\n",
    "train_size = int(len(processed_corpus) * 0.8)\n",
    "train_texts = processed_corpus[:train_size]\n",
    "val_texts = processed_corpus[train_size:]\n",
    "\n",
    "# Create the datasets\n",
    "seq_len = 1024\n",
    "train_dataset = CitationDataset(train_texts, tokenizer, seq_len=seq_len)\n",
    "val_dataset = CitationDataset(val_texts, tokenizer, seq_len=seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8908a1d-4eb1-444c-ba54-0eb88e908b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179e0eeef745488199fd4a1b744845a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588901bc72c342e691e6d3f74b91ed7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.797319353194464\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155468f54653449bb4c053cdb282a07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fcb4a9f23841a1a69bec78c3fdfa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.885184919266473\n",
      "Citation Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dcd03c172e4fc389014715d5b21edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fe83913d37448fb9f14e1f93ea7386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "\n",
    "import torch\n",
    "\n",
    "# Adjust the model vocabulary size\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    model.train()\n",
    "    bar = tqdm(enumerate(train_loader),total=len(train_loader),)\n",
    "    running_loss = []\n",
    "    for i, (inputs, targets) in bar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        citation_mask = targets.ge(tokenizer.additional_special_tokens_ids[0])  # Shape: (batch_size, seq_len)\n",
    "        if citation_mask.sum() < 3:\n",
    "            continue # Skip batch if it contains no citation tokens\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs,)\n",
    "        logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        targets_flattened = targets.view(-1)\n",
    "\n",
    "        # Only include the citation logits and targets in the loss calculation\n",
    "        citation_logits = logits[citation_mask.view(-1)]\n",
    "        citation_targets = targets_flattened[citation_mask.view(-1)]\n",
    "\n",
    "        loss = criterion(citation_logits, citation_targets)\n",
    "        # loss = criterion(outputs.logits.view(-1,outputs.logits.size(-1)), targets.flatten())\n",
    "        # loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        if (i+1) % 100 == 0:\n",
    "            # print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n",
    "            avg_loss = sum(running_loss) / len(running_loss)\n",
    "            bar.set_postfix(iteration=i, loss=f\"{avg_loss:.5f}\")\n",
    "            running_loss = []\n",
    "        if i > 100:\n",
    "            break\n",
    "            \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        total_citations = 0\n",
    "        bar = tqdm(val_loader, total=len(val_loader),)\n",
    "        running_loss = []\n",
    "        for i, (inputs, targets) in enumerate(bar):\n",
    "            if i > 200:\n",
    "                break\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            citation_mask = targets.ge(tokenizer.additional_special_tokens_ids[0])  # Shape: (batch_size, seq_len)\n",
    "            if citation_mask.sum() == 0:\n",
    "                continue\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            targets_flattened = targets.view(-1)\n",
    "\n",
    "            # Only include the citation logits and targets in the loss calculation\n",
    "            citation_logits = logits[citation_mask.view(-1)]\n",
    "            citation_targets = targets_flattened[citation_mask.view(-1)]\n",
    "\n",
    "            loss = criterion(citation_logits, citation_targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i+1) % 50 == 0:\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                bar.set_postfix(iteration=i, loss=f\"{avg_loss:.5f}\")\n",
    "                running_loss = []\n",
    "\n",
    "            # Compute the citation accuracy\n",
    "            citation_predictions = torch.argmax(citation_logits, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "            correct_predictions = (citation_predictions == citation_targets).sum().item()\n",
    "            total_citations += citation_targets.numel()\n",
    "            total_acc += correct_predictions\n",
    "            # print('corr pred = ', correct_predictions, ' cites = ', citation_targets.numel(), ' total acc = ', total_acc, ' total cites = ', total_citations)\n",
    "\n",
    "        print(f\"Validation Loss: {total_loss / len(val_loader)}\")\n",
    "        print(f\"Citation Accuracy: {total_acc / total_citations if total_citations > 0 else 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eae778d-46bf-41d8-abb7-417526d644c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CITATION_5825>\n",
      "Citation for, Probability: 0.029667695984244347\n",
      "<CITATION_2002>\n",
      "Citation generalization, Probability: 0.013682132586836815\n",
      "<CITATION_9241>\n",
      "Citation BERT, Probability: 0.013249002397060394\n",
      "<CITATION_2280>\n",
      "Citation transformer, Probability: 0.01189996674656868\n",
      "<CITATION_1563>\n",
      "Citation supervised, Probability: 0.011230929754674435\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the sample text\n",
    "sample_text = \"Batch normalization improves training efficiency of neural networks <CITATION_1>\"\n",
    "\n",
    "# # Preprocess the text by replacing the <cite_1> token with a <mask> token\n",
    "# mask_token = tokenizer.mask_token\n",
    "# masked_text = sample_text.replace(\"<cite_1>\", mask_token)\n",
    "\n",
    "# Tokenize the text and convert it to a tensor\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "# Send the tensor to the device\n",
    "inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the logits and compute the softmax to get probabilities\n",
    "logits = outputs.logits\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the position of the <mask> token\n",
    "mask_position = torch.where(inputs[\"input_ids\"] == tokenizer.encode('<CITATION_1>')[0])[1]\n",
    "\n",
    "# Get the probabilities of the tokens at the <mask> position\n",
    "mask_probs = probs[0, mask_position, :]\n",
    "\n",
    "# Get the top k probabilities and their indices\n",
    "top_k_probs, top_k_indices = torch.topk(mask_probs, k=5)  # Get the top 5 predictions\n",
    "\n",
    "# Print the top k citations and their probabilities\n",
    "for i in range(top_k_probs.shape[-1]):\n",
    "    try:\n",
    "        citation_id = top_k_indices[0, i].item()\n",
    "        prob = top_k_probs[0, i].item()\n",
    "        citation_key = tokenizer.decode(citation_id)\n",
    "        print(citation_key)\n",
    "        # citation_val = citation_map[citation_key]\n",
    "        citation_val = [k for k,v in citation_map.items() if v == citation_key][0]\n",
    "        print(f\"Citation {citation_val}, Probability: {prob}\")\n",
    "        # if citation_id in tokenizer.get_added_vocab():\n",
    "        #     citation_key = tokenizer.get_added_vocab()[citation_id]\n",
    "        #     print(f\"Citation: {citation_map[citation_key]}, Probability: {prob}\")\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d581996-3389-4d94-a766-544aa478b9c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'infinitewly wide neural networks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/paperGPT/cite_pred.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/paperGPT/cite_pred.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m citation_map[\u001b[39m'\u001b[39;49m\u001b[39minfinitewly wide neural networks\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'infinitewly wide neural networks'"
     ]
    }
   ],
   "source": [
    "citation_map['infi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bee06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e27925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
